[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/enhance-debugging-experience-with-jupyter-magic/2024-02-20-Enhance-debugging-experience-with-jupyter-magic.html",
    "href": "posts/enhance-debugging-experience-with-jupyter-magic/2024-02-20-Enhance-debugging-experience-with-jupyter-magic.html",
    "title": "Enhancing Debugging experience with Jupyter Magic",
    "section": "",
    "text": "The blog will cover the development of a new feature focused on enhancing the debugging experience for Kedro, a Python framework for building reproducible, maintainable, and modular data pipelines. This feature aims to streamline the debugging process by leveraging Jupyter notebooks and the inspect module to quickly restore the context of errors encountered within Kedro pipelines."
  },
  {
    "objectID": "posts/enhance-debugging-experience-with-jupyter-magic/2024-02-20-Enhance-debugging-experience-with-jupyter-magic.html#whats-in-scope-out-of-scope",
    "href": "posts/enhance-debugging-experience-with-jupyter-magic/2024-02-20-Enhance-debugging-experience-with-jupyter-magic.html#whats-in-scope-out-of-scope",
    "title": "Enhancing Debugging experience with Jupyter Magic",
    "section": "What’s in-scope & out-of-scope?",
    "text": "What’s in-scope & out-of-scope?\nIn-scope: - Integration of the feature with Jupyter notebooks. - Mapping of node inputs to function inputs using the inspect module. Out-of-scope: - Two way conversion between Notebook and source code. - Handle nested function definitions - that is a user defined function calling another user defined function which could be arbitary level of depth."
  },
  {
    "objectID": "posts/enhance-debugging-experience-with-jupyter-magic/2024-02-20-Enhance-debugging-experience-with-jupyter-magic.html#methodology",
    "href": "posts/enhance-debugging-experience-with-jupyter-magic/2024-02-20-Enhance-debugging-experience-with-jupyter-magic.html#methodology",
    "title": "Enhancing Debugging experience with Jupyter Magic",
    "section": "Methodology",
    "text": "Methodology\n\nProblem Statement\nThe challenge lies in mapping Kedro specific components to generate code that can be explored interactively in notebook to provide a seamless debugging experience. There are few key components that need to be mapped: - Generate code cell in Notebook - Loading the “Datasets” from a Kedro DataCatalog - Mapping Kedro Node’s to Python function. - A way to execute the code in the notebook - Import statements\n\nGenerate Code cell in Notebook\nOriginall\n\n\nKedro Node and Python Function\nKedro Node is a thin wrapper around Python function, with optional metadata such as name or tags to organise the node in a meaningful way. They are not too important for this particular feature, but useful for filtering pipeline. Kedro has a first party plugin kedro-viz that provide an interactive visualiation of your pipeline.\n\nfrom kedro.pipeline import node\n\ndef foo(a,b):\n    c = a + b\n    return c\n\nnode(foo, inputs=[\"transaction_data\", \"customer_data\"], outputs=[\"output_data\"], name=\"my_node\", tags=\"s\")\n\nNode(foo, ['data_a', 'data_b'], ['output_data'], 'my_node')\n\n\nYou can see how close the node resembles a typical Python function. The inputs and outputs refer to the name of a dataset of the Kedro DataCatalog. It is basically the key value of the definition of a dataset, which is ususally defined in YAML format.\nThis is an example of the definition of a dataset:\ntransaction_data:\n  type: pandas.CSVDataset\n  filepath: data/01_raw/my_raw_data.csv\nDataCatalog handles the I/O for Kedro Pipeline, the node only need to declares what data does it needs. The requirements here is that we need to map the inputs to dataset name properly. i.e. - transaction_data -&gt; a - customer_data -&gt; b\nTo run this in a notebook, we need to load the data and call the function.\na = catalog.load(\"transaction_data\")\nb = catalog.load(\"customer_data\")\n\nfoo(a, b)\nIt’s fairly easy to map this particular example, but it becomes tricker if we need to handle *args, **kwargs, optional arguments and more. This is the syntax that Kedro Node support.\ndef bar(a, b, c, *args, d=None):\n    return \"bar\"\nConsider this function, both node definitions below are valid: - node(bar, [\"transaction_data\", \"customer_data\", \"sales_data\", \"salary_data\"], [\"output_data\"]) - node(bar, [\"transaction_data\", \"customer_data\", \"sales_data\"], [\"output_data\"])\nThe solution of this is using inspect module to get the information about the function signature and node, and map it carefully with inspect.Signature.bind.\n\n\nExecuting the code in a notebook\nThere are 2 variations that we considered: 1. Code cell with the function defintion 2. Function call\nReusing the foo function mentioned earlier, with approach 1, we want to flatten it to a code cell in notebook\ndef foo(a,b):\n    c = a + b\n    return c\nNotebook cell:\na = catalog.load(\"transaction_data\")\nb = catalog.load(\"customer_data\")\n\nc = a + b\nc\nThe benefit of this is user can split the cell to inject logic or inspecting variable easily. However, it becomes increasing challenging to retrive the function body only. inspect provide method to extract the definition of foo, which is a string representation of this:\ndef foo(a,b):\n    c = a + b\n    return c\nIn order to make this runnable in a notebook cell, we need to handle a few things: 1. Remove the def line, which could be multiple lines 2. Remove the return statement, because it is not valid outside of function.\nAgain, it looks trivial at first, but if we start consideing multiple return in a function, it becomes unclear what we should do. In addition, a function could have decorator, which means removing the def isn’t always desired. At the end, we go with approach 2, which copy the function definition and make a call to it.\nThe notebook cell now look like this:\na = catalog.load(\"transaction_data\")\nb = catalog.load(\"customer_data\")\n\ndef foo(a,b): # Not necessary to copy\n    c = a + b\n    return c\n\nfoo(a, b)\n\n\nImport Statement\nWe take a fairly simple approach for this. Using inspect.getsourcefile(function), we can retrive the file that contains the function we desired. After that, we parse the file and retrive all import statements with specific keywords."
  },
  {
    "objectID": "posts/python_expert/python-expert-decorator-generator-contextmanager.html",
    "href": "posts/python_expert/python-expert-decorator-generator-contextmanager.html",
    "title": "Being Python Expert",
    "section": "",
    "text": "This presentation introduce three concepts, decorator, generator and contextmanager. The presenter explains that Python is a Protocol oriented langauge brilliantly and put these three concepts together to illustrate a story. Ultimately, being a Python expert doesn’t mean that you write advance syntax, but using these pattern wisely with simple code. These three concepts both serves its own puprose and are orthogonal to each other, yet working nicely when they are composed together."
  },
  {
    "objectID": "posts/python_expert/python-expert-decorator-generator-contextmanager.html#meet-the-python-data-model",
    "href": "posts/python_expert/python-expert-decorator-generator-contextmanager.html#meet-the-python-data-model",
    "title": "Being Python Expert",
    "section": "Meet the Python Data Model",
    "text": "Meet the Python Data Model\nWith python, there are almost always a lower level __ method corresponds to a higher-level function/syntax. It’s useful to change these behavior and understand how the Python Data Model work. You can find all the dunder __ method here. https://docs.python.org/3/reference/datamodel.html\n x + y   --&gt; __add__\n repr(s) --&gt; __repr__\n x()     --&gt; __call__\n\n## Metaclasses\n`metaclass` exists to allow library code works nicely with user code. For example, how can a library author ensure that its user will follow its protocol and not using it wrongly? Again you can dive into `__new__` for ensuring that. In practice, the code is already written for `abc` and people use `@abstractmethod` `abc.ABCMeta`\n\n\n# Meet the `__builtins__` library\nimport builtins\n__build_class__\n\nclass Nok:\n    pass\n\nNok() # This is possible because class is a Python keyword\n\n# You can actually construct a class with a function.\nbuiltins.__build_class__(\"Nok\")\n\nTypeError: __build_class__: not enough arguments"
  },
  {
    "objectID": "posts/python_expert/python-expert-decorator-generator-contextmanager.html#generator",
    "href": "posts/python_expert/python-expert-decorator-generator-contextmanager.html#generator",
    "title": "Being Python Expert",
    "section": "Generator",
    "text": "Generator\n\nEager vs Lazy\nProcess when data comes - memory efficient and no wait.\nyield control - interleaving. Idea of executing some code, then passing the output back to user, do something and continue.\n\n\ndef temptable(cur):\n    print(\"Create Table\")\n    yield\n    print(\"Drop Table\")\n\n\nclass T:\n    def __enter__(self):\n        self.gen = temptable(\"123\")\n        next(self.gen)\n        return self\n\n    def __exit__(self, *args):\n        return next(self.gen, None)\n\nwith T():\n    print(\"Finish\")\n\n\nCreate Table\nFinish\nDrop Table"
  },
  {
    "objectID": "posts/understand_namespace/2023-09-26-understand-kedro-namespace-pipeline.html",
    "href": "posts/understand_namespace/2023-09-26-understand-kedro-namespace-pipeline.html",
    "title": "A Guide to Kedro Namespace Pipelines for Time Series Forecasting",
    "section": "",
    "text": "Kedro’s Namespace Pipeline is a powerful feature that allows for flexible pipeline reuse, especially handy for tasks like time series forecasting.\n\n\nThis blog post is based on this example Kedro project. Consider a basic pipeline node for monthly predictions:\n\ndef make_monthly_predictions(input_data):\n    # Fill your actual logic here!\n    output_data = input_data\n    return output_data\n\nNow, let’s create a simple one node pipeline using Kedro, this will be the building block of our pipelines:\n\nfrom kedro.pipeline.modular_pipeline import pipeline\nfrom kedro.pipeline import node\n\nbase_pipeline = pipeline(\n    [\n        node(\n            func=make_monthly_predictions,\n            inputs=[\"input_data\"],\n            outputs=[\"output_data\"]\n        )\n    ]\n)\n\nIt basically just call the make_monthly_prediction, and define what are the “inputs” and “outputs”.\n\n\n\n\n\n\ntime series diagram\n\n\nNow that we have our base pipeline, For time series forecasting, where predictions depend on previous results, we can efficiently handle this with [Namespace Pipelines]((https://docs.kedro.org/en/0.18.0/tutorial/namespace_pipelines.html). Start by creating a Namespace Pipeline:\n\nnamespace_pipeline = pipeline(\n    [\n        node(\n            func=make_monthly_predictions,\n            inputs=[\"input_data\"],\n            outputs=[\"output_data\"]\n        )\n    ],\n    namespace=\"namespace\"\n)\n\nThe namespace argument automatically adds a prefix to inputs and outputs. You can inspect the pipeline by printing it.\n\nnamespace_pipeline\n\nPipeline([\nNode(make_monthly_predictions, ['input_data'], ['namespace.output_data'], None)\n])\n\n\nIf you want to keep some datasets from namespacing, you can specify the inputs or outputs argument of the pipeline function to overide it:\n(https://docs.kedro.org/en/stable/nodes_and_pipelines/modular_pipelines.html#using-the-modular-pipeline-wrapper-to-provide-overrides). \n\nnamespace_pipeline = pipeline(\n    [\n        node(\n            \n            func=make_monthly_predictions,\n            inputs=[\"input_data\"],\n            outputs=[\"output_data\"]\n        )\n    ],\n    inputs=[\"input_data\"],  # Escape from namespace\n    namespace=\"namespace\"\n)\n\n\nnamespace_pipeline\n\nPipeline([\nNode(make_monthly_predictions, ['input_data'], ['namespace.output_data'], None)\n])\n\n\n\n\n\nNow that we understand the mechanics of namespace, let’s build a time-series pipeline by iterating through months and connecting pipelines:\n\nmonths = [\"jan\", \"feb\", \"mar\", \"apr\"]\n\ndef create_pipeline(months):\n    pipelines = []\n    for i in range(len(months) - 1):\n        curr, next = months[i], months[i+1]\n        pipelines.append(pipeline(base_pipeline,\n            outputs={\"output_data\": f\"{next}.input_data\"},\n            namespace=curr))\n    return pipeline(pipelines)\n\nfinal_pipeline = create_pipeline(months)\n\n\nfinal_pipeline\n\nPipeline([\nNode(make_monthly_predictions, ['jan.input_data'], ['feb.input_data'], None),\nNode(make_monthly_predictions, ['feb.input_data'], ['mar.input_data'], None),\nNode(make_monthly_predictions, ['mar.input_data'], ['apr.input_data'], None)\n])\n\n\nBy visualizing the pipeline with kedro viz, you can observe the connections between each step."
  },
  {
    "objectID": "posts/understand_namespace/2023-09-26-understand-kedro-namespace-pipeline.html#getting-started",
    "href": "posts/understand_namespace/2023-09-26-understand-kedro-namespace-pipeline.html#getting-started",
    "title": "A Guide to Kedro Namespace Pipelines for Time Series Forecasting",
    "section": "",
    "text": "This blog post is based on this example Kedro project. Consider a basic pipeline node for monthly predictions:\n\ndef make_monthly_predictions(input_data):\n    # Fill your actual logic here!\n    output_data = input_data\n    return output_data\n\nNow, let’s create a simple one node pipeline using Kedro, this will be the building block of our pipelines:\n\nfrom kedro.pipeline.modular_pipeline import pipeline\nfrom kedro.pipeline import node\n\nbase_pipeline = pipeline(\n    [\n        node(\n            func=make_monthly_predictions,\n            inputs=[\"input_data\"],\n            outputs=[\"output_data\"]\n        )\n    ]\n)\n\nIt basically just call the make_monthly_prediction, and define what are the “inputs” and “outputs”."
  },
  {
    "objectID": "posts/understand_namespace/2023-09-26-understand-kedro-namespace-pipeline.html#utilizing-namespace-for-efficiency",
    "href": "posts/understand_namespace/2023-09-26-understand-kedro-namespace-pipeline.html#utilizing-namespace-for-efficiency",
    "title": "A Guide to Kedro Namespace Pipelines for Time Series Forecasting",
    "section": "",
    "text": "time series diagram\n\n\nNow that we have our base pipeline, For time series forecasting, where predictions depend on previous results, we can efficiently handle this with [Namespace Pipelines]((https://docs.kedro.org/en/0.18.0/tutorial/namespace_pipelines.html). Start by creating a Namespace Pipeline:\n\nnamespace_pipeline = pipeline(\n    [\n        node(\n            func=make_monthly_predictions,\n            inputs=[\"input_data\"],\n            outputs=[\"output_data\"]\n        )\n    ],\n    namespace=\"namespace\"\n)\n\nThe namespace argument automatically adds a prefix to inputs and outputs. You can inspect the pipeline by printing it.\n\nnamespace_pipeline\n\nPipeline([\nNode(make_monthly_predictions, ['input_data'], ['namespace.output_data'], None)\n])\n\n\nIf you want to keep some datasets from namespacing, you can specify the inputs or outputs argument of the pipeline function to overide it:\n(https://docs.kedro.org/en/stable/nodes_and_pipelines/modular_pipelines.html#using-the-modular-pipeline-wrapper-to-provide-overrides). \n\nnamespace_pipeline = pipeline(\n    [\n        node(\n            \n            func=make_monthly_predictions,\n            inputs=[\"input_data\"],\n            outputs=[\"output_data\"]\n        )\n    ],\n    inputs=[\"input_data\"],  # Escape from namespace\n    namespace=\"namespace\"\n)\n\n\nnamespace_pipeline\n\nPipeline([\nNode(make_monthly_predictions, ['input_data'], ['namespace.output_data'], None)\n])"
  },
  {
    "objectID": "posts/understand_namespace/2023-09-26-understand-kedro-namespace-pipeline.html#building-the-time-series-pipeline",
    "href": "posts/understand_namespace/2023-09-26-understand-kedro-namespace-pipeline.html#building-the-time-series-pipeline",
    "title": "A Guide to Kedro Namespace Pipelines for Time Series Forecasting",
    "section": "",
    "text": "Now that we understand the mechanics of namespace, let’s build a time-series pipeline by iterating through months and connecting pipelines:\n\nmonths = [\"jan\", \"feb\", \"mar\", \"apr\"]\n\ndef create_pipeline(months):\n    pipelines = []\n    for i in range(len(months) - 1):\n        curr, next = months[i], months[i+1]\n        pipelines.append(pipeline(base_pipeline,\n            outputs={\"output_data\": f\"{next}.input_data\"},\n            namespace=curr))\n    return pipeline(pipelines)\n\nfinal_pipeline = create_pipeline(months)\n\n\nfinal_pipeline\n\nPipeline([\nNode(make_monthly_predictions, ['jan.input_data'], ['feb.input_data'], None),\nNode(make_monthly_predictions, ['feb.input_data'], ['mar.input_data'], None),\nNode(make_monthly_predictions, ['mar.input_data'], ['apr.input_data'], None)\n])\n\n\nBy visualizing the pipeline with kedro viz, you can observe the connections between each step."
  },
  {
    "objectID": "posts/kedro_duckdb/kedro-duckdb.html",
    "href": "posts/kedro_duckdb/kedro-duckdb.html",
    "title": "Kedro DuckDB",
    "section": "",
    "text": "Exploring DuckDB and how can we use it with kedro"
  },
  {
    "objectID": "posts/kedro_duckdb/kedro-duckdb.html#practical-sql-for-data-analysis",
    "href": "posts/kedro_duckdb/kedro-duckdb.html#practical-sql-for-data-analysis",
    "title": "Kedro DuckDB",
    "section": "Practical SQL for Data Analysis",
    "text": "Practical SQL for Data Analysis\n\nWhat you can do together with Pandas\n\n!pip install --quiet duckdb\n\n\nimport pandas as pd\nimport numpy as np\n\nimport sqlite3\nimport duckdb\nimport time\n\nPreparation\nDownload the data and set up the Pandas data frames. We read the data into a Pandas DataFrame using DuckDB’s built-in Parquet reader.\n\n!wget -q https://github.com/cwida/duckdb-data/releases/download/v1.0/lineitemsf1.snappy.parquet\n\n  HTTP/1.1 301 Moved Permanently\n  Server: GitHub.com\n  Date: Tue, 15 Nov 2022 22:34:17 GMT\n  Content-Type: text/html; charset=utf-8\n  Vary: X-PJAX, X-PJAX-Container, Turbo-Visit, Turbo-Frame, Accept-Encoding, Accept, X-Requested-With\n  Location: https://github.com/duckdb/duckdb-data/releases/download/v1.0/lineitemsf1.snappy.parquet\n  Cache-Control: no-cache\n  Strict-Transport-Security: max-age=31536000; includeSubdomains; preload\n  X-Frame-Options: deny\n  X-Content-Type-Options: nosniff\n  X-XSS-Protection: 0\n  Referrer-Policy: origin-when-cross-origin, strict-origin-when-cross-origin\n  Content-Security-Policy: default-src 'none'; base-uri 'self'; block-all-mixed-content; child-src github.com/assets-cdn/worker/ gist.github.com/assets-cdn/worker/; connect-src 'self' uploads.github.com objects-origin.githubusercontent.com www.githubstatus.com collector.github.com raw.githubusercontent.com api.github.com github-cloud.s3.amazonaws.com github-production-repository-file-5c1aeb.s3.amazonaws.com github-production-upload-manifest-file-7fdce7.s3.amazonaws.com github-production-user-asset-6210df.s3.amazonaws.com cdn.optimizely.com logx.optimizely.com/v1/events *.actions.githubusercontent.com wss://*.actions.githubusercontent.com online.visualstudio.com/api/v1/locations github-production-repository-image-32fea6.s3.amazonaws.com github-production-release-asset-2e65be.s3.amazonaws.com insights.github.com wss://alive.github.com; font-src github.githubassets.com; form-action 'self' github.com gist.github.com objects-origin.githubusercontent.com; frame-ancestors 'none'; frame-src viewscreen.githubusercontent.com notebooks.githubusercontent.com; img-src 'self' data: github.githubassets.com media.githubusercontent.com camo.githubusercontent.com identicons.github.com avatars.githubusercontent.com github-cloud.s3.amazonaws.com objects.githubusercontent.com objects-origin.githubusercontent.com secured-user-images.githubusercontent.com/ opengraph.githubassets.com github-production-user-asset-6210df.s3.amazonaws.com customer-stories-feed.github.com spotlights-feed.github.com *.githubusercontent.com; manifest-src 'self'; media-src github.com user-images.githubusercontent.com/ secured-user-images.githubusercontent.com/; script-src github.githubassets.com; style-src 'unsafe-inline' github.githubassets.com; worker-src github.com/assets-cdn/worker/ gist.github.com/assets-cdn/worker/\n  Content-Length: 0\n  X-GitHub-Request-Id: D47F:F2DE:2F3E29B:303AC52:637413E9\n  HTTP/1.1 302 Found\n  Server: GitHub.com\n  Date: Tue, 15 Nov 2022 22:34:17 GMT\n  Content-Type: text/html; charset=utf-8\n  Vary: X-PJAX, X-PJAX-Container, Turbo-Visit, Turbo-Frame, Accept-Encoding, Accept, X-Requested-With\n  Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/263853960/33e88e80-95cb-11ea-8bb7-2dfa0654592c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221115%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221115T223417Z&X-Amz-Expires=300&X-Amz-Signature=07d1673053f9e8676510f46b62993e3b9b2428a17f00a613162f67690318e82f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=263853960&response-content-disposition=attachment%3B%20filename%3Dlineitemsf1.snappy.parquet&response-content-type=application%2Foctet-stream\n  Cache-Control: no-cache\n  Strict-Transport-Security: max-age=31536000; includeSubdomains; preload\n  X-Frame-Options: deny\n  X-Content-Type-Options: nosniff\n  X-XSS-Protection: 0\n  Referrer-Policy: no-referrer-when-downgrade\n  Content-Security-Policy: default-src 'none'; base-uri 'self'; block-all-mixed-content; child-src github.com/assets-cdn/worker/ gist.github.com/assets-cdn/worker/; connect-src 'self' uploads.github.com objects-origin.githubusercontent.com www.githubstatus.com collector.github.com raw.githubusercontent.com api.github.com github-cloud.s3.amazonaws.com github-production-repository-file-5c1aeb.s3.amazonaws.com github-production-upload-manifest-file-7fdce7.s3.amazonaws.com github-production-user-asset-6210df.s3.amazonaws.com cdn.optimizely.com logx.optimizely.com/v1/events *.actions.githubusercontent.com wss://*.actions.githubusercontent.com online.visualstudio.com/api/v1/locations github-production-repository-image-32fea6.s3.amazonaws.com github-production-release-asset-2e65be.s3.amazonaws.com insights.github.com wss://alive.github.com; font-src github.githubassets.com; form-action 'self' github.com gist.github.com objects-origin.githubusercontent.com; frame-ancestors 'none'; frame-src viewscreen.githubusercontent.com notebooks.githubusercontent.com; img-src 'self' data: github.githubassets.com media.githubusercontent.com camo.githubusercontent.com identicons.github.com avatars.githubusercontent.com github-cloud.s3.amazonaws.com objects.githubusercontent.com objects-origin.githubusercontent.com secured-user-images.githubusercontent.com/ opengraph.githubassets.com github-production-user-asset-6210df.s3.amazonaws.com customer-stories-feed.github.com spotlights-feed.github.com *.githubusercontent.com; manifest-src 'self'; media-src github.com user-images.githubusercontent.com/ secured-user-images.githubusercontent.com/; script-src github.githubassets.com; style-src 'unsafe-inline' github.githubassets.com; worker-src github.com/assets-cdn/worker/ gist.github.com/assets-cdn/worker/\n  Content-Length: 0\n  X-GitHub-Request-Id: D47F:F2DE:2F3E323:303ACEF:637413E9\n  HTTP/1.1 200 OK\n  Connection: keep-alive\n  Content-Length: 206368635\n  Content-Type: application/octet-stream\n  Last-Modified: Tue, 07 Dec 2021 13:35:44 GMT\n  ETag: \"0x8D9B986787C89B4\"\n  Server: Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0\n  x-ms-request-id: b588900b-a01e-0060-6d42-f95efa000000\n  x-ms-version: 2020-04-08\n  x-ms-creation-time: Tue, 17 Aug 2021 11:28:44 GMT\n  x-ms-lease-status: unlocked\n  x-ms-lease-state: available\n  x-ms-blob-type: BlockBlob\n  Content-Disposition: attachment; filename=lineitemsf1.snappy.parquet\n  x-ms-server-encrypted: true\n  Fastly-Restarts: 1\n  Accept-Ranges: bytes\n  Age: 0\n  Date: Tue, 15 Nov 2022 22:34:18 GMT\n  Via: 1.1 varnish\n  X-Served-By: cache-lhr7337-LHR\n  X-Cache: MISS\n  X-Cache-Hits: 0\n  X-Timer: S1668551658.108887,VS0,VE259\n\n\n\n!wget -q https://github.com/cwida/duckdb-data/releases/download/v1.0/orders.parquet\n\n\n%%time\nlineitem = duckdb.query(\"SELECT * FROM 'lineitemsf1.snappy.parquet'\").to_df()\norders = duckdb.query(\"SELECT * FROM 'orders.parquet'\").to_df()\n\nCPU times: user 7.62 s, sys: 5.43 s, total: 13 s\nWall time: 14.1 s\n\n\n\n%%time\n_ = pd.read_parquet(\"lineitemsf1.snappy.parquet\")\n_ = pd.read_parquet(\"orders.parquet\")\n\nCPU times: user 6.29 s, sys: 1.5 s, total: 7.78 s\nWall time: 5.81 s\n\n\n\ncon = duckdb.connect()\ncon.execute('PRAGMA threads=2')\n\ndef timeit(fun, name):\n    import time\n    start_time = time.monotonic()\n    fun()\n    return [name, time.monotonic() - start_time]\n\ndef plot_results(results, title):\n  df = pd.DataFrame.from_dict({\n      'name': [x[0] for x in results],\n      'time': [x[1] for x in results]\n  })\n  print(title)\n  print(df)\n\nUngrouped Aggregates\nThis performs a simple set of ungrouped aggregates (sum, min, max, avg) over a column without any filters or other complex operations.\n\nungrouped_aggregate = '''\n    SELECT SUM(l_extendedprice), MIN(l_extendedprice), MAX(l_extendedprice), AVG(l_extendedprice) FROM lineitem\n'''\n\ndef duckdb_ungrouped_aggregate(d_con):\n    print(d_con.query(ungrouped_aggregate).to_df())\n\ndef duckdb_ungrouped_aggregate_1t():\n    duckdb_ungrouped_aggregate(duckdb)\n\ndef duckdb_ungrouped_aggregate_2t():\n    duckdb_ungrouped_aggregate(con)\n\ndef pandas_ungrouped_aggregate():\n  result = lineitem.groupby(\n    ['l_returnflag', 'l_linestatus']\n  ).agg(\n    Sum=('l_extendedprice', 'sum'),\n    Min=('l_extendedprice', 'min'),\n    Max=('l_extendedprice', 'max'),\n    Avg=('l_extendedprice', 'mean')\n  )\n  print(result)\n    # print(lineitem.agg(Sum=('l_extendedprice', 'sum'), Min=('l_extendedprice', 'min'), Max=('l_extendedprice', 'max'), Avg=('l_extendedprice', 'mean')))\n\nua_results = []\nua_results.append(timeit(duckdb_ungrouped_aggregate_1t, 'DuckDB (1T)'))\nua_results.append(timeit(duckdb_ungrouped_aggregate_2t, 'DuckDB (2T)'))\nua_results.append(timeit(pandas_ungrouped_aggregate, 'Pandas'))\nplot_results(ua_results, 'Ungrouped Aggregate')\n\n   sum(l_extendedprice)  min(l_extendedprice)  max(l_extendedprice)  \\\n0          2.295773e+11                 901.0              104949.5   \n\n   avg(l_extendedprice)  \n0          38255.138485  \n   sum(l_extendedprice)  min(l_extendedprice)  max(l_extendedprice)  \\\n0          2.295773e+11                 901.0              104949.5   \n\n   avg(l_extendedprice)  \n0          38255.138485  \n                                    Sum    Min       Max           Avg\nl_returnflag l_linestatus                                             \nA            F             5.658655e+10  904.0  104949.5  38273.129735\nN            F             1.487505e+09  920.0  104049.5  38284.467761\n             O             1.149352e+11  901.0  104749.5  38248.015609\nR            F             5.656804e+10  904.0  104899.5  38250.854626\nUngrouped Aggregate\n          name      time\n0  DuckDB (1T)  0.052544\n1  DuckDB (2T)  0.066239\n2       Pandas  0.801278\n\n\nGrouped Aggregates\nThis performs the same set of aggregates, but this time grouped by two other columns (l_returnflag and l_linestatus).\n\ngrouped_aggregate = '''\nSELECT l_returnflag,\n       l_linestatus,\n       SUM(l_extendedprice),\n       MIN(l_extendedprice),\n       MAX(l_extendedprice),\n       AVG(l_extendedprice)\nFROM lineitem\nGROUP BY l_returnflag,\n         l_linestatus\n'''\n\ndef duckdb_grouped_aggregate(d_con):\n    print(d_con.query(grouped_aggregate).to_df())\n\ndef duckdb_grouped_aggregate_1t():\n    duckdb_grouped_aggregate(duckdb)\n\ndef duckdb_grouped_aggregate_2t():\n    duckdb_grouped_aggregate(con)\n\ndef pandas_grouped_aggregate():\n    print(lineitem.groupby(['l_returnflag', 'l_linestatus']).agg(Sum=('l_extendedprice', 'sum'), Min=('l_extendedprice', 'min'), Max=('l_extendedprice', 'max'), Avg=('l_extendedprice', 'mean')))\n\nresults = []\nresults.append(timeit(duckdb_grouped_aggregate_1t, 'DuckDB (1T)'))\nresults.append(timeit(duckdb_grouped_aggregate_2t, 'DuckDB (2T)'))\nresults.append(timeit(pandas_grouped_aggregate, 'Pandas'))\nplot_results(results, 'Grouped Aggregate')\n\n  l_returnflag l_linestatus  sum(l_extendedprice)  min(l_extendedprice)  \\\n0            N            O          1.149352e+11                 901.0   \n1            R            F          5.656804e+10                 904.0   \n2            A            F          5.658655e+10                 904.0   \n3            N            F          1.487505e+09                 920.0   \n\n   max(l_extendedprice)  avg(l_extendedprice)  \n0              104749.5          38248.015609  \n1              104899.5          38250.854626  \n2              104949.5          38273.129735  \n3              104049.5          38284.467761  \n  l_returnflag l_linestatus  sum(l_extendedprice)  min(l_extendedprice)  \\\n0            N            O          1.149352e+11                 901.0   \n1            R            F          5.656804e+10                 904.0   \n2            A            F          5.658655e+10                 904.0   \n3            N            F          1.487505e+09                 920.0   \n\n   max(l_extendedprice)  avg(l_extendedprice)  \n0              104749.5          38248.015609  \n1              104899.5          38250.854626  \n2              104949.5          38273.129735  \n3              104049.5          38284.467761  \n                                    Sum    Min       Max           Avg\nl_returnflag l_linestatus                                             \nA            F             5.658655e+10  904.0  104949.5  38273.129735\nN            F             1.487505e+09  920.0  104049.5  38284.467761\n             O             1.149352e+11  901.0  104749.5  38248.015609\nR            F             5.656804e+10  904.0  104899.5  38250.854626\nGrouped Aggregate\n          name      time\n0  DuckDB (1T)  0.115463\n1  DuckDB (2T)  0.222520\n2       Pandas  0.708696\n\n\nGrouped Aggregate with a Filter\nThis benchmark performs a grouped aggregate with a filter over the shipdate column.\nAs Pandas does not perform any projection pushdown, we include a version where we manually perform the projection pushdown by filtering only the columns we actually need before running the filter and aggregate.\nThis optimization is performed automatically in DuckDB by the query optimizer.\n\ndef duckdb_grouped_aggregate_filter(d_con):\n    print(d_con.query('''\nSELECT l_returnflag,\n       l_linestatus,\n       SUM(l_extendedprice),\n       MIN(l_extendedprice),\n       MAX(l_extendedprice),\n       AVG(l_extendedprice)\nFROM lineitem\nWHERE\n    l_shipdate &lt;= DATE '1998-09-02'\nGROUP BY l_returnflag,\n         l_linestatus\n''').to_df())\n\ndef duckdb_grouped_aggregate_filter_1t():\n    duckdb_grouped_aggregate_filter(duckdb)\n\ndef duckdb_grouped_aggregate_filter_2t():\n    duckdb_grouped_aggregate_filter(con)\n\ndef pandas_grouped_aggregate_filter():\n  filtered_df = lineitem[lineitem['l_shipdate'] &lt; \"1998-09-02\"]\n  print(filtered_df.groupby(['l_returnflag', 'l_linestatus']).agg(Sum=('l_extendedprice', 'sum'), Min=('l_extendedprice', 'min'), Max=('l_extendedprice', 'max'), Avg=('l_extendedprice', 'mean')))\n\ndef pandas_grouped_aggregate_filter_projection_pushdown():\n  pushed_down_df = lineitem[['l_shipdate', 'l_returnflag', 'l_linestatus', 'l_extendedprice']]\n  filtered_df = pushed_down_df[pushed_down_df['l_shipdate'] &lt; \"1998-09-02\"]\n  print(filtered_df.groupby(['l_returnflag', 'l_linestatus']).agg(Sum=('l_extendedprice', 'sum'), Min=('l_extendedprice', 'min'), Max=('l_extendedprice', 'max'), Avg=('l_extendedprice', 'mean')))\n\nresults = []\nresults.append(timeit(duckdb_grouped_aggregate_filter_1t, 'DuckDB (1T)'))\nresults.append(timeit(duckdb_grouped_aggregate_filter_2t, 'DuckDB (2T)'))\nresults.append(timeit(pandas_grouped_aggregate_filter, 'Pandas'))\nresults.append(timeit(pandas_grouped_aggregate_filter_projection_pushdown, 'Pandas (manual pushdown)'))\nplot_results(results, 'Grouped Aggregate + Filter')\n\n  l_returnflag l_linestatus  sum(l_extendedprice)  min(l_extendedprice)  \\\n0            N            O          1.117017e+11                 901.0   \n1            A            F          5.658655e+10                 904.0   \n2            R            F          5.656804e+10                 904.0   \n3            N            F          1.487505e+09                 920.0   \n\n   max(l_extendedprice)  avg(l_extendedprice)  \n0              104749.5          38249.117989  \n1              104949.5          38273.129735  \n2              104899.5          38250.854626  \n3              104049.5          38284.467761  \n  l_returnflag l_linestatus  sum(l_extendedprice)  min(l_extendedprice)  \\\n0            N            O          1.117017e+11                 901.0   \n1            A            F          5.658655e+10                 904.0   \n2            R            F          5.656804e+10                 904.0   \n3            N            F          1.487505e+09                 920.0   \n\n   max(l_extendedprice)  avg(l_extendedprice)  \n0              104749.5          38249.117989  \n1              104949.5          38273.129735  \n2              104899.5          38250.854626  \n3              104049.5          38284.467761  \n                                    Sum    Min       Max           Avg\nl_returnflag l_linestatus                                             \nA            F             5.658655e+10  904.0  104949.5  38273.129735\nN            F             1.487505e+09  920.0  104049.5  38284.467761\n             O             1.116318e+11  901.0  104749.5  38249.322811\nR            F             5.656804e+10  904.0  104899.5  38250.854626\n                                    Sum    Min       Max           Avg\nl_returnflag l_linestatus                                             \nA            F             5.658655e+10  904.0  104949.5  38273.129735\nN            F             1.487505e+09  920.0  104049.5  38284.467761\n             O             1.116318e+11  901.0  104749.5  38249.322811\nR            F             5.656804e+10  904.0  104899.5  38250.854626\nGrouped Aggregate + Filter\n                       name      time\n0               DuckDB (1T)  0.281653\n1               DuckDB (2T)  0.356302\n2                    Pandas  2.889015\n3  Pandas (manual pushdown)  1.625353\n\n\nGrouped Aggregate with Join and Filter\nIn this benchmark we expand on the previous benchmark by including a join and a filter on the joined-on table.\nNote that the naive version in Pandas is extremely slow, as it performs a full join of the entire table including all the columns that are not used and all the rows that will be filtered out. For that reason we have included a separate benchmark in which we have manually optimized the Pandas code by pushing down the projections and the filters.\nThese optimizations are performed automatically in DuckDB by the query optimizer.\n\n\n# projection & filter on lineitem table\nlineitem_projected = lineitem[\n  ['l_shipdate',\n   'l_orderkey',\n   'l_linestatus',\n   'l_returnflag',\n   'l_extendedprice']\n]\nlineitem_filtered = lineitem_projected[\n  lineitem_projected['l_shipdate'] &lt; \"1998-09-02\"]\n# projection and filter on order table\norders_projected = orders[\n  ['o_orderkey',\n   'o_orderstatus']\n]\norders_filtered = orders_projected[\n  orders_projected['o_orderstatus'] == 'O']\n# perform the join\nmerged = lineitem_filtered.merge(\n  orders_filtered,\n  left_on='l_orderkey',\n  right_on='o_orderkey')\n# perform the aggregate\nresult = merged.groupby(\n  ['l_returnflag', 'l_linestatus']\n).agg(\n  Sum=('l_extendedprice', 'sum'),\n  Min=('l_extendedprice', 'min'),\n  Max=('l_extendedprice', 'max'),\n  Avg=('l_extendedprice', 'mean')\n)\n\n\nresult\n\n\n\n\n\n\n\n\n\nSum\nMin\nMax\nAvg\n\n\nl_returnflag\nl_linestatus\n\n\n\n\n\n\n\n\nN\nO\n1.080448e+11\n901.0\n104749.5\n38250.662806\n\n\n\n\n\n\n\n\ndef duckdb_grouped_aggregate_filter_join(d_con):\n    print(d_con.query('''\nSELECT l_returnflag,\n       l_linestatus,\n       sum(l_extendedprice),\n       min(l_extendedprice),\n       max(l_extendedprice),\n       avg(l_extendedprice)\nFROM lineitem lineitem\nJOIN orders orders ON (l_orderkey=o_orderkey)\nWHERE l_shipdate &lt;= DATE '1998-09-02'\n  AND o_orderstatus='O'\nGROUP BY l_returnflag,\n         l_linestatus\n''').to_df())\n\ndef duckdb_grouped_aggregate_filter_join_1t():\n    duckdb_grouped_aggregate_filter_join(duckdb)\n\ndef duckdb_grouped_aggregate_filter_join_2t():\n    duckdb_grouped_aggregate_filter_join(con)\n\ndef pandas_grouped_aggregate_filter_join():\n    merged = lineitem.merge(orders, left_on='l_orderkey', right_on='o_orderkey')\n    filtered_a = merged[merged['l_shipdate'] &lt; \"1998-09-02\"]\n    filtered_b = filtered_a[filtered_a['o_orderstatus'] == 'O']\n    result = filtered_b.groupby(['l_returnflag', 'l_linestatus']).agg(Sum=('l_extendedprice', 'sum'), Min=('l_extendedprice', 'min'), Max=('l_extendedprice', 'max'), Avg=('l_extendedprice', 'mean'))\n    print(result)\n\ndef pandas_grouped_aggregate_filter_join_manual_pushdown():\n    lineitem_projected = lineitem[['l_shipdate', 'l_orderkey', 'l_linestatus', 'l_returnflag', 'l_extendedprice']]\n    lineitem_filtered = lineitem_projected[lineitem_projected['l_shipdate'] &lt; \"1998-09-02\"]\n    orders_projected = orders[['o_orderkey', 'o_orderstatus']]\n    orders_filtered = orders_projected[orders_projected['o_orderstatus'] == 'O']\n    merged = lineitem_filtered.merge(orders_filtered, left_on='l_orderkey', right_on='o_orderkey')\n    result = merged.groupby(['l_returnflag', 'l_linestatus']).agg(Sum=('l_extendedprice', 'sum'), Min=('l_extendedprice', 'min'), Max=('l_extendedprice', 'max'), Avg=('l_extendedprice', 'mean'))\n    print(result)\n\nresults = []\nresults.append(timeit(duckdb_grouped_aggregate_filter_join_1t, 'DuckDB (1T)'))\nresults.append(timeit(duckdb_grouped_aggregate_filter_join_2t, 'DuckDB (2T)'))\nresults.append(timeit(pandas_grouped_aggregate_filter_join, 'Pandas'))\nresults.append(timeit(pandas_grouped_aggregate_filter_join_manual_pushdown, 'Pandas (manual pushdown)'))\nplot_results(results, 'Grouped Aggregate Join')\n\n\n  l_returnflag l_linestatus  sum(l_extendedprice)  min(l_extendedprice)  \\\n0            N            O          1.081147e+11                 901.0   \n\n   max(l_extendedprice)  avg(l_extendedprice)  \n0              104749.5          38250.450307  \n  l_returnflag l_linestatus  sum(l_extendedprice)  min(l_extendedprice)  \\\n0            N            O          1.081147e+11                 901.0   \n\n   max(l_extendedprice)  avg(l_extendedprice)  \n0              104749.5          38250.450307  \n                                    Sum    Min       Max           Avg\nl_returnflag l_linestatus                                             \nN            O             1.080448e+11  901.0  104749.5  38250.662806\n                                    Sum    Min       Max           Avg\nl_returnflag l_linestatus                                             \nN            O             1.080448e+11  901.0  104749.5  38250.662806\nGrouped Aggregate Join\n                       name       time\n0               DuckDB (1T)   0.218088\n1               DuckDB (2T)   0.376592\n2                    Pandas  11.403579\n3  Pandas (manual pushdown)   2.765103"
  },
  {
    "objectID": "posts/pandas_expert/kedro-meta-analysis.html",
    "href": "posts/pandas_expert/kedro-meta-analysis.html",
    "title": "Understanding the Kedro codebase - A quick dirty meta-analysis - (Part I)",
    "section": "",
    "text": "Inspired by this talk\n\nHow many lines of code in Kedro?\n\nfrom pathlib import Path\nimport pandas as pd\nfrom collections import Counter\n\n\nREPO_PATH = Path(\"/Users/Nok_Lam_Chan/GitHub/kedro\")\nlist(REPO_PATH.iterdir())\n\n[PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test_requirements.txt'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/CODE_OF_CONDUCT.md'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/LICENSE.md'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tools'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro_technical_charter.pdf'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.DS_Store'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.pytest_cache'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/derby.log'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro.egg-info'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.pre-commit-config.yaml'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.coverage'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/Makefile'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/CITATION.cff'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/CODEOWNERS'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/pyproject.toml'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/trufflehog-ignore.txt'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/dependency'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/MANIFEST.in'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/docs'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.readthedocs.yml'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/dep_tree.log'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/README.md'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/RELEASE.md'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/setup.py'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/demo-project'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/logs'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.mypy_cache'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.gitignore'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/static'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/CONTRIBUTING.md'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/behave.ini'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.github'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.gitpod.yml'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/info.log'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/coverage.xml'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/errors.log'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.git'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/htmlcov'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.vscode'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/data'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/conf'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.circleci'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/import.log'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/notebooks'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.run'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.idea'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/src')]\n\n\n\ndef count_effective_line(counter, fn):\n    with open (fn) as f:\n        for line in f:\n            counter[fn] += 1\n\n\nlines_count = Counter()\nfor fn in REPO_PATH.rglob(\"*/*.py\"):\n#     print(fn)\n    count_effective_line(lines_count, fn)\nprint(lines_count)\n            \n\nCounter({PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/spark/test_spark_dataset.py'): 984, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/pipeline/test_pipeline.py'): 940, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/pipeline/pipeline.py'): 926, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/session/test_session.py'): 891, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/micropkg.py'): 854, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/micropkg/test_micropkg_pull.py'): 846, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/io/core.py'): 748, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/test_cli.py'): 730, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/io/test_data_catalog.py'): 685, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/test_starters.py'): 639, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/cli_steps.py'): 623, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/pipeline/node.py'): 612, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/docs/conf.py'): 598, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/io/data_catalog.py'): 594, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/docs/build/conf.py'): 587, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/micropkg/test_micropkg_package.py'): 581, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/session/test_session_extension_hooks.py'): 576, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/io/test_partitioned_dataset.py'): 565, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/starters.py'): 552, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/io/partitioned_dataset.py'): 551, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/pipeline/test_pipeline.py'): 522, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/io/test_incremental_dataset.py'): 503, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/context/test_context.py'): 485, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/config/test_templated_config.py'): 482, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/test_project.py'): 479, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/test_jupyter.py'): 470, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/utils.py'): 469, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/runner.py'): 456, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/tensorflow/test_tensorflow_model_dataset.py'): 441, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/sql_dataset.py'): 438, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/matplotlib/test_matplotlib_writer.py'): 436, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/pipeline/test_node.py'): 434, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/session/session.py'): 423, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/spark/spark_dataset.py'): 422, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/pipeline/test_modular_pipeline.py'): 418, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/runner/test_parallel_runner.py'): 401, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/project.py'): 392, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/pipeline/test_pipeline_with_transcoding.py'): 391, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_generic_dataset.py'): 383, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/session/conftest.py'): 381, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_sql_dataset.py'): 374, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/project/__init__.py'): 369, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/config/test_config.py'): 354, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/parallel_runner.py'): 353, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/context/context.py'): 345, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_parquet_dataset.py'): 344, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/pipeline.py'): 336, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_gbq_dataset.py'): 315, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/spark/test_spark_hive_dataset.py'): 314, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/gbq_dataset.py'): 314, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/test_catalog.py'): 305, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/ipython/test_ipython.py'): 304, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_csv_dataset.py'): 300, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/hooks/specs.py'): 296, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/pipeline/modular_pipeline.py'): 290, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/jupyter.py'): 282, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_excel_dataset.py'): 281, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/config/templated_config.py'): 281, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/build/lib/resume_kedro/dagascii.py'): 275, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/runner/test_sequential_runner.py'): 273, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pickle/test_pickle_dataset.py'): 269, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/micropkg/test_micropkg_requirements.py'): 266, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/project/test_pipeline_discovery.py'): 260, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/excel_dataset.py'): 254, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/test_startup.py'): 250, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/config/common.py'): 248, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/generic_dataset.py'): 246, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_hdf_dataset.py'): 245, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/matplotlib/matplotlib_writer.py'): 243, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pickle/pickle_dataset.py'): 243, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_xml_dataset.py'): 241, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_json_dataset.py'): 241, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/geojson/test_geojson_dataset.py'): 232, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pillow/test_image_dataset.py'): 231, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/parquet_dataset.py'): 230, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/pipeline/test_pipeline_from_missing.py'): 227, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/io/test_memory_dataset.py'): 226, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/networkx/test_json_dataset.py'): 226, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/email/test_message_dataset.py'): 226, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/docs/source/conf.py'): 225, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/docs/source/conf.py'): 225, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/docs/source/conf.py'): 225, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test/docs/source/conf.py'): 224, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/project/{{ cookiecutter.repo_name }}/docs/source/conf.py'): 224, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/docs/source/conf.py'): 222, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/holoviews/test_holoviews_writer.py'): 220, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_feather_dataset.py'): 220, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/spark/spark_hive_dataset.py'): 220, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/runner/test_thread_runner.py'): 213, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/cli.py'): 211, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/yaml/test_yaml_dataset.py'): 210, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/hdf_dataset.py'): 204, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/json/test_json_dataset.py'): 200, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/io/test_lambda_dataset.py'): 194, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/tracking/test_metrics_dataset.py'): 194, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/csv_dataset.py'): 194, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/feather_dataset.py'): 191, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/redis/redis_dataset.py'): 189, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/networkx/test_gml_dataset.py'): 188, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/networkx/test_graphml_dataset.py'): 188, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/tensorflow/tensorflow_model_dataset.py'): 188, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/email/message_dataset.py'): 188, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/text/test_text_dataset.py'): 187, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/json_dataset.py'): 187, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/tracking/test_json_dataset.py'): 185, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/catalog.py'): 176, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/spark/spark_jdbc_dataset.py'): 175, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/xml_dataset.py'): 171, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/api/test_api_dataset.py'): 170, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/runner/conftest.py'): 168, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/redis/test_redis_dataset.py'): 165, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/ipython/__init__.py'): 164, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/dask/test_parquet_dataset.py'): 162, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/json/json_dataset.py'): 160, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/conftest.py'): 159, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/geopandas/geojson_dataset.py'): 157, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/startup.py'): 156, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/thread_runner.py'): 156, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/yaml/yaml_dataset.py'): 155, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/tools/test_cli.py'): 154, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/plotly/json_dataset.py'): 154, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/networkx/json_dataset.py'): 150, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/networkx/gml_dataset.py'): 145, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/networkx/graphml_dataset.py'): 143, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/io/test_cached_dataset.py'): 142, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pillow/image_dataset.py'): 142, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/api/api_dataset.py'): 142, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/pipeline/test_node_run.py'): 141, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/biosequence/biosequence_dataset.py'): 137, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/holoviews/holoviews_writer.py'): 137, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/dask/parquet_dataset.py'): 136, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/config/config.py'): 134, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/io/memory_dataset.py'): 132, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/text/text_dataset.py'): 131, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/environment.py'): 128, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/test_cli_hooks.py'): 128, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/session/test_session_hook_manager.py'): 126, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/spark/test_spark_jdbc_dataset.py'): 121, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/plotly/plotly_dataset.py'): 117, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/io/cached_dataset.py'): 113, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/io/lambda_dataset.py'): 113, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/plotly/test_plotly_dataset.py'): 108, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/spark/deltatable_dataset.py'): 108, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/bioinformatics/test_biosequence_dataset.py'): 107, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/hooks/manager.py'): 106, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/sh_run.py'): 105, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/project/test_settings.py'): 102, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/plotly/test_json_dataset.py'): 101, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/io/test_core.py'): 96, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/logging/color_logger.py'): 95, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/conftest.py'): 89, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/session/test_store.py'): 89, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/spark/test_deltatable_dataset.py'): 89, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/test_registry.py'): 88, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/sequential_runner.py'): 87, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/util.py'): 84, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/pipeline/test_pipeline_integration.py'): 84, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/pipeline/conftest.py'): 84, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipelines/data_science/nodes.py'): 80, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/project/test_pipeline_registry.py'): 79, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/micropkg/conftest.py'): 79, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/hooks/test_manager.py'): 75, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/iris/nodes.py'): 74, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/iris_demo/nodes.py'): 74, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/resume_kedro/nodes.py'): 74, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/build/lib/resume_kedro/nodes.py'): 74, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/tracking/metrics_dataset.py'): 68, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/spark/test_memory_dataset.py'): 67, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tools/cli.py'): 62, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/iris_demo/settings.py'): 62, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/project/test_logging.py'): 58, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/build/lib/resume_kedro/settings.py'): 56, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipelines/data_engineering/nodes.py'): 51, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/registry.py'): 50, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/hooks/manager.py'): 49, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test/src/test/__main__.py'): 47, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/iris/__main__.py'): 47, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/iris_demo/__main__.py'): 47, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/__main__.py'): 47, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/resume_kedro/__main__.py'): 47, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/build/lib/resume_kedro/__main__.py'): 47, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/project/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/__main__.py'): 47, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/tracking/json_dataset.py'): 47, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/hooks/specs.py'): 46, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/session/shelvestore.py'): 43, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test/src/tests/test_run.py'): 41, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/spark/conftest.py'): 41, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/project/{{ cookiecutter.repo_name }}/src/tests/test_run.py'): 41, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/tests/test_run.py'): 40, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test/src/setup.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/tests/test_run.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/setup.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/tests/test_run.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/setup.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/settings.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/tests/test_run.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/resume_kedro/pipeline.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/config/abstract_config.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/session/store.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/project/{{ cookiecutter.repo_name }}/src/setup.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/project/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/settings.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/__init__.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/build/lib/resume_kedro/pipeline.py'): 38, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test/src/test/settings.py'): 37, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/setup.py'): 37, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/iris/settings.py'): 37, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/setup.py'): 37, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/conftest.py'): 35, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/iris/pipeline.py'): 33, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/iris_demo/pipeline.py'): 33, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/io/__init__.py'): 33, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/test_utils.py'): 30, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/utils.py'): 28, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_plugin/plugin.py'): 27, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipelines/data_science/pipeline.py'): 27, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tools/circleci/github_scripts/kedro_version.py'): 26, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipelines/data_engineering/pipeline.py'): 26, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/settings.py'): 24, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/hooks/test_manager.py'): 22, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/extensions/ipython.py'): 22, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipeline_registry.py'): 19, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/config/__init__.py'): 19, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/iris/pipeline_registry.py'): 18, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/iris_demo/pipeline_registry.py'): 18, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/resume_kedro/pipeline_registry.py'): 18, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/build/lib/resume_kedro/pipeline_registry.py'): 18, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/logging/test_color_logger.py'): 16, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/__init__.py'): 16, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/project/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipeline_registry.py'): 16, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/logging/__init__.py'): 15, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/networkx/__init__.py'): 15, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/spark/__init__.py'): 14, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test/src/test/pipeline_registry.py'): 13, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_plugin/setup.py'): 12, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/__init__.py'): 11, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/hooks/markers.py'): 11, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/plotly/__init__.py'): 11, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/api/__init__.py'): 11, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/tracking/__init__.py'): 11, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/__main__.py'): 10, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/hooks/markers.py'): 10, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/pipeline/{{ cookiecutter.pipeline_name }}/__init__.py'): 10, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/pipeline/{{ cookiecutter.pipeline_name }}/pipeline.py'): 10, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/tests/test_pipeline.py'): 9, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/tests/test_pipeline.py'): 9, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/tests/test_pipeline.py'): 9, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/pipeline/__init__.py'): 9, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/pipeline/{{ cookiecutter.pipeline_name }}/tests/test_pipeline.py'): 9, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/dask/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/redis/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/geopandas/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pillow/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/json/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/biosequence/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/tensorflow/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/matplotlib/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/yaml/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pickle/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/text/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/holoviews/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/email/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipelines/data_science/__init__.py'): 7, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipelines/data_engineering/__init__.py'): 7, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/context/__init__.py'): 7, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/__init__.py'): 7, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/session/__init__.py'): 6, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/hooks/__init__.py'): 5, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/hooks/__init__.py'): 5, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test/src/test/__init__.py'): 4, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/iris/__init__.py'): 4, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/iris_demo/__init__.py'): 4, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/__init__.py'): 4, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/resume_kedro/__init__.py'): 4, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/build/lib/resume_kedro/__init__.py'): 4, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/pipeline/{{ cookiecutter.pipeline_name }}/nodes.py'): 4, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/project/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/__init__.py'): 4, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/resume_kedro/settings.py'): 3, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/__init__.py'): 3, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/extensions/__init__.py'): 3, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/__init__.py'): 2, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/__init__.py'): 1})\n\n\n\n\nClean up the dictionary a little bit\n\nline_counts_df = pd.DataFrame(lines_count.items(), columns=[\"fullpath\",\"line_of_code\"])\nline_counts_df[\"fullpath\"] = line_counts_df[\"fullpath\"].apply(str)\nline_counts_df[\"fullpath\"] =  line_counts_df[\"fullpath\"].str.replace(\"/Users/Nok_Lam_Chan/GitHub/kedro/\", \"\")\nline_counts_df.head(2)\n\n\n\n\n\n\n\n\nfullpath\nline_of_code\n\n\n\n\n0\ntools/cli.py\n62\n\n\n1\nfeatures/environment.py\n128\n\n\n\n\n\n\n\n\nline_counts_df[[\"toplevel\",\"module\",\"submodule\",\"filename\"]] = line_counts_df[\"fullpath\"].str.split(\"/\",expand=True, n=3)\n\n\nline_counts_df\n\n\n\n\n\n\n\n\nfullpath\nline_of_code\ntoplevel\nmodule\nsubmodule\nfilename\n\n\n\n\n0\ntools/cli.py\n62\ntools\ncli.py\nNone\nNone\n\n\n1\nfeatures/environment.py\n128\nfeatures\nenvironment.py\nNone\nNone\n\n\n2\ntests/test_utils.py\n30\ntests\ntest_utils.py\nNone\nNone\n\n\n3\ntests/conftest.py\n89\ntests\nconftest.py\nNone\nNone\n\n\n4\ndocs/conf.py\n598\ndocs\nconf.py\nNone\nNone\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n276\nkedro/extras/datasets/pandas/feather_dataset.py\n191\nkedro\nextras\ndatasets\npandas/feather_dataset.py\n\n\n277\nkedro/extras/datasets/pandas/hdf_dataset.py\n204\nkedro\nextras\ndatasets\npandas/hdf_dataset.py\n\n\n278\nkedro/extras/datasets/pandas/csv_dataset.py\n194\nkedro\nextras\ndatasets\npandas/csv_dataset.py\n\n\n279\nkedro/extras/datasets/pandas/excel_dataset.py\n254\nkedro\nextras\ndatasets\npandas/excel_dataset.py\n\n\n280\nkedro/extras/datasets/pandas/gbq_dataset.py\n314\nkedro\nextras\ndatasets\npandas/gbq_dataset.py\n\n\n\n\n281 rows × 6 columns\n\n\n\n\n## Sort by Top level module\nline_counts_df.groupby([\"toplevel\"]).sum().sort_values(ascending=False, by =\"line_of_code\")\n\n\n\n\n\n\n\n\nline_of_code\n\n\ntoplevel\n\n\n\n\n\ntests\n25341\n\n\nkedro\n18683\n\n\nfeatures\n1587\n\n\ndocs\n1185\n\n\nresume-kedro\n1007\n\n\niris-demo\n550\n\n\niris\n547\n\n\ntest\n405\n\n\ntools\n88\n\n\n\n\n\n\n\nInterstingly we have roughly a 1:1 ratio between tests and kedro\n\nline_counts_df.groupby([\"module\",\"submodule\"]).sum().sort_values(ascending=False, by =\"line_of_code\")\n\n\n\n\n\n\n\n\n\nline_of_code\n\n\nmodule\nsubmodule\n\n\n\n\n\nextras\ndatasets\n15775\n\n\nframework\ncli\n8837\n\n\nsession\n2574\n\n\npipeline\ntest_pipeline.py\n940\n\n\npipeline.py\n926\n\n\n...\n...\n...\n\n\nconfig\n__init__.py\n19\n\n\nrunner\n__init__.py\n16\n\n\npipeline\n__init__.py\n9\n\n\nextras\n__init__.py\n2\n\n\nframework\n__init__.py\n1\n\n\n\n\n74 rows × 1 columns\n\n\n\n\n## Sort by Sub-module\nkedro_line_counts_df = line_counts_df[line_counts_df[\"toplevel\"] == \"kedro\"]\ntmp = kedro_line_counts_df.groupby(\"module\").sum().rename(mapper={\"line_of_code\": \"module_line_of_code\"},axis=1 )\nkedro_line_counts_df_group = kedro_line_counts_df.groupby([\"module\",\"submodule\"]).sum().reset_index().merge(tmp, left_on=\"module\", right_on=\"module\")\n\n\n# .sort_values(ascending=False, by =\"line_of_code\")\n\n\nkedro_line_counts_df.groupby([\"module\"]).sum().sort_values(ascending=False, by =\"line_of_code\")\n\n\n\n\n\n\n\n\nline_of_code\n\n\nmodule\n\n\n\n\n\nextras\n6871\n\n\nframework\n5246\n\n\nio\n2284\n\n\npipeline\n1837\n\n\nrunner\n1068\n\n\nconfig\n721\n\n\ntemplates\n443\n\n\nipython\n164\n\n\nutils.py\n28\n\n\n__init__.py\n11\n\n\n__main__.py\n10\n\n\n\n\n\n\n\n\n# Sort by file \nkedro_line_counts_df_group.sort_values(ascending=False, by =[\"module_line_of_code\",\"line_of_code\"])\n\n\n\n\n\n\n\n\nmodule\nsubmodule\nline_of_code\nmodule_line_of_code\n\n\n\n\n6\nextras\ndatasets\n6734\n6871\n\n\n8\nextras\nlogging\n110\n6871\n\n\n7\nextras\nextensions\n25\n6871\n\n\n5\nextras\n__init__.py\n2\n6871\n\n\n10\nframework\ncli\n3439\n5246\n\n\n14\nframework\nsession\n511\n5246\n\n\n12\nframework\nhooks\n418\n5246\n\n\n13\nframework\nproject\n369\n5246\n\n\n11\nframework\ncontext\n352\n5246\n\n\n15\nframework\nstartup.py\n156\n5246\n\n\n9\nframework\n__init__.py\n1\n5246\n\n\n18\nio\ncore.py\n748\n2284\n\n\n19\nio\ndata_catalog.py\n594\n2284\n\n\n22\nio\npartitioned_dataset.py\n551\n2284\n\n\n21\nio\nmemory_dataset.py\n132\n2284\n\n\n17\nio\ncached_dataset.py\n113\n2284\n\n\n20\nio\nlambda_dataset.py\n113\n2284\n\n\n16\nio\n__init__.py\n33\n2284\n\n\n27\npipeline\npipeline.py\n926\n1837\n\n\n26\npipeline\nnode.py\n612\n1837\n\n\n25\npipeline\nmodular_pipeline.py\n290\n1837\n\n\n24\npipeline\n__init__.py\n9\n1837\n\n\n30\nrunner\nrunner.py\n456\n1068\n\n\n29\nrunner\nparallel_runner.py\n353\n1068\n\n\n32\nrunner\nthread_runner.py\n156\n1068\n\n\n31\nrunner\nsequential_runner.py\n87\n1068\n\n\n28\nrunner\n__init__.py\n16\n1068\n\n\n4\nconfig\ntemplated_config.py\n281\n721\n\n\n2\nconfig\ncommon.py\n248\n721\n\n\n3\nconfig\nconfig.py\n134\n721\n\n\n1\nconfig\nabstract_config.py\n39\n721\n\n\n0\nconfig\n__init__.py\n19\n721\n\n\n34\ntemplates\nproject\n410\n443\n\n\n33\ntemplates\npipeline\n33\n443\n\n\n23\nipython\n__init__.py\n164\n164\n\n\n\n\n\n\n\n\n# Total number of LOC\nkedro_line_counts_df[\"line_of_code\"].sum()\n\n18683\n\n\n\n\nConclusion\nThe kedro codebase is not huge, roughly 20000 line of code, compare to pandas which has &gt; 250000 of code, 10x smaller. The datasets and framework code is the largest module which isn’t surprise to me. The more surprising is how small config actually is, but it creates huge complexity in terms of a kedro project. The cli is also relatively huge as it takes ~3000 lines of code which I didn’t expected."
  },
  {
    "objectID": "posts/2021-08-18-python-file-not-found-long-file-path-window.html",
    "href": "posts/2021-08-18-python-file-not-found-long-file-path-window.html",
    "title": "Python FileNotFoundError or You have a really long file path?",
    "section": "",
    "text": "FileNotFoundError? Not so quick\n\n\n\nscreenshot\n\n\nTo illustrate the issue, I perpared some fake file. The script is simple, it just read a file with plain text, except that the filename is really long.\n\n\n\nerror\n\n\nUnforuntately, even though the file exists, Python gives me a FileNotFoundError, how come? However long debugging, I found out that this is related to the filename that exist only on Windows.\nThis StackOverflow thread explain this issue.\n\nMaximum Path Length Limitation\nIn the Windows API (with some exceptions discussed in the following paragraphs), the maximum length for a path is MAX_PATH, which is defined as 260 characters. A local path is structured in the following order: drive letter, colon, backslash, name components separated by backslashes, and a terminating null character. For example, the maximum path on drive D is “D:*some 256-character path string*” where “” represents the invisible terminating null character for the current system codepage. (The characters &lt; &gt; are used here for visual clarity and cannot be part of a valid path string.)\n\n\n\nSolution - Registry\nUpdating your Registry can solve this problem.\n\n\n\nHello World\n\n\nAfter applying the config, I can finally read the file. :)\n\n\nSummary (TLDR version)\n\nWindow filesystem only allow 256 characters, beyond that you will have trouble to open the file.\nPython will not be able to see this file and throw FileNotFoundError (I have no idea, anyone know why is that?)\nYou can update registry to enable long file path in Window to fix this issue.\n\n(Bonus: Window actually has weird behavior for long filepath, you can try to break it with different ways.)"
  },
  {
    "objectID": "posts/2021-03-27-microsoft-azure-dp100.html",
    "href": "posts/2021-03-27-microsoft-azure-dp100.html",
    "title": "Microsoft Azure - DP100",
    "section": "",
    "text": "Last Updated: 2021-04-22"
  },
  {
    "objectID": "posts/2021-03-27-microsoft-azure-dp100.html#official-suggested-materials",
    "href": "posts/2021-03-27-microsoft-azure-dp100.html#official-suggested-materials",
    "title": "Microsoft Azure - DP100",
    "section": "Official Suggested Materials",
    "text": "Official Suggested Materials\n\n❗ https://docs.microsoft.com/en-us/learn/paths/build-ai-solutions-with-azure-ml-service/ - This should be your main focus, try to finish the labs and read the tutorials. You need to understand the use case of different products and familiar yourself with the syntax of Azure ML SDK etc.\nhttps://docs.microsoft.com/en-us/learn/paths/create-no-code-predictive-models-azure-machine-learning/ - You should at least finish 1 of the lab to get some sense of the UI. It would be included in the exam for sure (2-3 questions maybe)\nhttps://docs.microsoft.com/en-us/learn/paths/create-machine-learn-models/ - I didn’t spend much time on it as most of them are baisc data science concepts. You would need to how to apply different types of models (Regression/Classification/Time Series) & AutoML for given scenario."
  },
  {
    "objectID": "posts/2021-03-27-microsoft-azure-dp100.html#compute-target",
    "href": "posts/2021-03-27-microsoft-azure-dp100.html#compute-target",
    "title": "Microsoft Azure - DP100",
    "section": "Compute Target",
    "text": "Compute Target\nMachine Learning Studio - single/multi - Development/Experiment - Local Machine/Cloud VM. - Scale up to larger data/distributed - training compute target - Azure Machine Learning compute cluster - Azure Machine Learning compute instance - Deploy Compute Target (You need to able to judge the most appropiate option based on the context.) - Local web service - Azure Kubernetes Service (AKS) - Azure Container Instances - Azure Machine Learning compute clusters (Batch Inference)"
  },
  {
    "objectID": "posts/2021-03-27-microsoft-azure-dp100.html#datastore",
    "href": "posts/2021-03-27-microsoft-azure-dp100.html#datastore",
    "title": "Microsoft Azure - DP100",
    "section": "DataStore",
    "text": "DataStore\n\nAzure Storage (blob and file containers)\nAzure Data Lake stores\nAzure SQL Database\nAzure Databricks file system (DBFS)"
  },
  {
    "objectID": "posts/rust_learning/2023-11-11-rust-learning.html",
    "href": "posts/rust_learning/2023-11-11-rust-learning.html",
    "title": "Experimenting Rust with Python",
    "section": "",
    "text": "As a Python only developer, I found it extremly hard to learn a new langauge particular with language like C++ or Rust that involves build steps. (Does it has debugger support like PyCharm?). I tried to learn Rust by reading the book last year, I finished a few chapters but never get to write any programme and I forgot most of it already. The only thing that stays is Rust has a concept of lifetime and borrow checker.\nI am hoping that by pushing my learning journey online will give myself extra motivation. Ultimately I want to learn just enough Rust to optimise performance for Python program, it could be just simple algorithm, or if I ever learn enough I would like to write an event simulation engine (or at least learn to build one).\nKey learning today: - pyo3 seems to be the choice for Rust binding for Python. I checked what ruff used (An insanely fast Python linter and now formatter). I prefer to stick with a good enough choice (noted that building and distributing Python package is a headache, I don’t want to spend my energy on this now) - I followed the PyO3 tutorial to build a Rust binding for Python. - I try to follow as much as possible without forcing myself to understand everything. Building something functional first and try to break it is a better learning process for me. - Use of #[pymodule] and maturin develop to build Python package. - You can also run Python in Rust!\nIt build and install the package string_sum.\n&gt;&gt;&gt; import string_sum\n&gt;&gt;&gt; dir(string_sum)\n['__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'string_sum', 'sum_as_string']\nQuestions - string_sum.cpython-310-darwin.so, this is the only file I find in the site_package alongside __init__.py. How do I find the interface other than doing dir?"
  },
  {
    "objectID": "posts/rust_learning/2023-11-11-rust-learning.html#section",
    "href": "posts/rust_learning/2023-11-11-rust-learning.html#section",
    "title": "Experimenting Rust with Python",
    "section": "",
    "text": "As a Python only developer, I found it extremly hard to learn a new langauge particular with language like C++ or Rust that involves build steps. (Does it has debugger support like PyCharm?). I tried to learn Rust by reading the book last year, I finished a few chapters but never get to write any programme and I forgot most of it already. The only thing that stays is Rust has a concept of lifetime and borrow checker.\nI am hoping that by pushing my learning journey online will give myself extra motivation. Ultimately I want to learn just enough Rust to optimise performance for Python program, it could be just simple algorithm, or if I ever learn enough I would like to write an event simulation engine (or at least learn to build one).\nKey learning today: - pyo3 seems to be the choice for Rust binding for Python. I checked what ruff used (An insanely fast Python linter and now formatter). I prefer to stick with a good enough choice (noted that building and distributing Python package is a headache, I don’t want to spend my energy on this now) - I followed the PyO3 tutorial to build a Rust binding for Python. - I try to follow as much as possible without forcing myself to understand everything. Building something functional first and try to break it is a better learning process for me. - Use of #[pymodule] and maturin develop to build Python package. - You can also run Python in Rust!\nIt build and install the package string_sum.\n&gt;&gt;&gt; import string_sum\n&gt;&gt;&gt; dir(string_sum)\n['__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'string_sum', 'sum_as_string']\nQuestions - string_sum.cpython-310-darwin.so, this is the only file I find in the site_package alongside __init__.py. How do I find the interface other than doing dir?"
  },
  {
    "objectID": "posts/2021-06-26-jupyter-sql-notebook.html",
    "href": "posts/2021-06-26-jupyter-sql-notebook.html",
    "title": "Jupyter Superpower - Extend SQL analysis with Python",
    "section": "",
    "text": "If you have ever written SQL queries to extract data from a database, chances are you are familiar with an IDE like the screenshot below. The IDE offers features like auto-completion, visualize the query output, display the table schema and the ER diagram. Whenever you need to write a query, this is your go-to tool. However, you may want to add Jupyter Notebook into your toolkit. It improves my productivity by complementing some missing features in IDE.\nCode\n# !pip install ipython_sql\n%load_ext sql\n%config SqlMagic.displaycon = False\n%config SqlMagic.feedback = False\n# Download the file from https://github.com/cwoodruff/ChinookDatabase/blob/master/Scripts/Chinook_Sqlite.sqlite\n%sql sqlite:///sales.sqlite.db\n    \nfrom pathlib import Path\nDATA_DIR = Path('../_demo/sql_notebook')\n%%sql\nselect ProductId, Sum(Unit) from Sales group by ProductId;\n\n\n\n\nProductId\nSum(Unit)\n\n\n1\n210\n\n\n2\n50\n\n\n3\n30"
  },
  {
    "objectID": "posts/2021-06-26-jupyter-sql-notebook.html#notebook-as-a-self-contained-report",
    "href": "posts/2021-06-26-jupyter-sql-notebook.html#notebook-as-a-self-contained-report",
    "title": "Jupyter Superpower - Extend SQL analysis with Python",
    "section": "Notebook as a self-contained report",
    "text": "Notebook as a self-contained report\nAs a data scientist/data analyst, you write SQL queries for ad-hoc analyses all the time. After getting the right data, you make nice-looking charts and put them in a PowerPoint and you are ready to present your findings. Unlike a well-defined ETL job, you are exploring the data and testing your hypotheses all the time. You make assumptions, which is often wrong but you only realized it after a few weeks. But all you got is a CSV that you cannot recall how it was generated in the first place.\nData is not stationary, why should your analysis be? I have seen many screenshots, fragmented scripts flying around in organizations. As a data scientist, I learned that you need to be cautious about what you heard. Don’t trust peoples’ words easily, verify the result! To achieve that, we need to know exactly how the data was extracted, what kind of assumptions have been made? Unfortunately, this information usually is not available. As a result, people are redoing the same analysis over and over. You will be surprised that this is very common in organizations. In fact, numbers often do not align because every department has its own definition for a given metric. It is not shared among the organization, and verbal communication is inaccurate and error-prone. It would be really nice if anyone in the organization can reproduce the same result with just a single click. Jupyter Notebook can achieve that reproducibility and keep your entire analysis (documentation, data, and code) in the same place."
  },
  {
    "objectID": "posts/2021-06-26-jupyter-sql-notebook.html#notebook-as-an-extension-of-ide",
    "href": "posts/2021-06-26-jupyter-sql-notebook.html#notebook-as-an-extension-of-ide",
    "title": "Jupyter Superpower - Extend SQL analysis with Python",
    "section": "Notebook as an extension of IDE",
    "text": "Notebook as an extension of IDE\nWriting SQL queries in a notebook gives you extra flexibility of a full programming language alongside SQL. For example:\n\nWrite complex processing logic that is not easy in pure SQL\nCreate visualizations directly from SQL results without exporting to an intermediate CSV\n\nFor instance, you can pipe your SQL query with pandas and then make a plot. It allows you to generate analysis with richer content. If you find bugs in your code, you can modify the code and re-run the analysis. This reduces the hustles to reproduce an analysis greatly. In contrast, if your analysis is reading data from an anonymous exported CSV, it is almost guaranteed that the definition of the data will be lost. No one will be able to reproduce the dataset.\nYou can make use of the ipython_sql library to make queries in a notebook. To do this, you need to use the magic function with the inline magic % or cell magic %%.\n\nsales = %sql SELECT * from sales LIMIT 3\nsales\n\n\n\n\nProductId\nUnit\nIsDeleted\n\n\n1\n10\n1\n\n\n1\n10\n1\n\n\n2\n10\n0\n\n\n\n\n\nTo make it fancier, you can even parameterize your query with variables. Tools like papermill allows you to parameterize your notebook. If you execute the notebook regularly with a scheduler, you can get a updated dashboard. To reference the python variable, the $ sign is used.\n\ntable = \"sales\"\nquery = f\"SELECT * from {table} LIMIT 3\"\nsales = %sql $query\nsales\n\n\n\n\nProductId\nUnit\nIsDeleted\n\n\n1\n10\n1\n\n\n1\n10\n1\n\n\n2\n10\n0\n\n\n\n\n\nWith a little bit of python code, you can make a nice plot to summarize your finding. You can even make an interactive plot if you want. This is a very powerful way to extend your analysis.\n\nimport seaborn as sns\nsales = %sql SELECT * FROM SALES\nsales_df = sales.DataFrame()\nsales_df = sales_df.groupby('ProductId', as_index=False).sum()\nax = sns.barplot(x='ProductId', y='Unit', data=sales_df)\nax.set_title('Sales by ProductId');"
  },
  {
    "objectID": "posts/2021-06-26-jupyter-sql-notebook.html#notebook-as-a-collaboration-tool",
    "href": "posts/2021-06-26-jupyter-sql-notebook.html#notebook-as-a-collaboration-tool",
    "title": "Jupyter Superpower - Extend SQL analysis with Python",
    "section": "Notebook as a collaboration tool",
    "text": "Notebook as a collaboration tool\nJupyter Notebook is flexible and it fits extremely well with exploratory data analysis. To share to a non-coder, you can share the notebook or export it as an HTML file. They can read the report or any cached executed result. If they need to verify the data or add some extra plots, they can do it easily themselves.\nIt is true that Jupyter Notebook has an infamous reputation. It is not friendly to version control, it’s hard to collaborate with notebooks. Luckily, there are efforts that make collaboration in notebook a lot easier now.\nHere what I did not show you is that the table has an isDeleted column. Some of the records are invalid and we should exclude them. In reality, this happens frequently when you are dealing with hundreds of tables that you are not familiar with. These tables are made for applications, transactions, and they do not have analytic in mind. Data Analytic is usually an afterthought. Therefore, you need to consult the SME or the maintainer of that tables. It takes many iterations to get the correct data that can be used to produce useful insight.\nWith ReviewNB, you can publish your result and invite some domain expert to review your analysis. This is where notebook shine, this kind of workflow is not possible with just the SQL script or a screenshot of your finding. The notebook itself is a useful documentation and collaboration tool.\n\nStep 1 - Review PR online\n\n\n\nStep1\n\n\nYou can view your notebook and add comments on a particular cell on ReviewNB. This lowers the technical barrier as your analysts do not have to understand Git. He can review changes and make comments on the web without the need to pull code at all. As soon as your analyst makes a suggestion, you can make changes.\n\n\nStep 2 - Review Changes\n\n\n\nStep2\n\n\nOnce you have made changes to the notebook, you can review it side by side. This is very trivial to do it in your local machine. Without ReviewNB, you have to pull both notebooks separately. As Git tracks line-level changes, you can’t really read the changes as it consists of a lot of confusing noise. It would also be impossible to view changes about the chart with git.\n\n\nStep 3 - Resolve Discussion\n\n\n\nStep3\n\n\nOnce the changes are reviewed, you can resolve the discussion and share your insight with the team. You can publish the notebook to internal sharing platform like knowledge-repo to organize the analysis.\nI hope this convince you that Notebook is a good choice for adhoc analytics. It is possible to collaborate with notebook with proper software in place. Regarless if you use notebook or not, you should try your best to document the process. Let’s make more reproducible analyses!"
  },
  {
    "objectID": "posts/python_single_dispatch/python-dispatch-typehint.html",
    "href": "posts/python_single_dispatch/python-dispatch-typehint.html",
    "title": "Function overloading - singledispatch in Python with type hint",
    "section": "",
    "text": "With Python&gt;=3.7, the @singledispatch method can now understand the type hints. It behaves like function overloading but it’s more dynamic than the static langauge.\nHere is a quick example to demonstrate it.\n\nfrom functools import singledispatch\n\n@singledispatch\ndef foo(x):\n    print(\"foo\")\n\n\n@foo.register\ndef _(x: float):\n    print(\"It's a float\")\n\n\n@foo.register\ndef _(x: str):\n    print(\"It's a string now!\")\n\nLet’s see how it works.\n\nfoo(1)\n\nfoo\n\n\n\nfoo(1.0)\n\nIt's a float\n\n\n\nfoo(\"1\")\n\nIt's a string now!\n\n\nThe function foo now understand the type of the argument and dispatch the corresponding functions. This is nicer than a big chunk of if/else statement since it’s less couple. It’s also easy to extend this. Imagine the foo function is import from a package, it’s easy to extend it.\n\n# Imagine `foo` was imported from a package\n# Now that you have a special type and you want to extend it from your own library, you don't need to touch the source code at all.\n\n# from bar import foo\nclass Nok:\n    ...\n\n\n@foo.register\ndef _(x: Nok):\n    print(\"Nok\")\n\n\nnok = Nok()\nfoo(nok)\n\nNok\n\n\nThis is only possible because Python is a dynamic language. In contrast, to achieve the same functionalities with monkey patching, you would need to copy the source code of the function and extend the if/else block.\nLet’s dive a bit deeper to the decorator.\n\nprint([attr for attr in dir(foo) if not attr.startswith(\"_\")])\n\n['dispatch', 'register', 'registry']\n\n\n\nfoo.dispatch\n\n&lt;function functools.singledispatch.&lt;locals&gt;.dispatch(cls)&gt;\n\n\n\nfoo.register\n\n&lt;function functools.singledispatch.&lt;locals&gt;.register(cls, func=None)&gt;\n\n\n\nfoo.registry\n\nmappingproxy({object: &lt;function __main__.foo(x)&gt;,\n              float: &lt;function __main__._(x: float)&gt;,\n              str: &lt;function __main__._(x: str)&gt;,\n              __main__.Nok: &lt;function __main__._(x: __main__.Nok)&gt;,\n              __main__.Nok: &lt;function __main__._(x: __main__.Nok)&gt;})\n\n\n\nfrom collections import abc\nisinstance(foo.registry, abc.Mapping)\n\nThe foo.registry is the most interesting part. Basically, it’s a dictionary-like object which store the types. It behaves like\nif type(x) == \"int\":\n    do_something()\nelif type(x) == \"float\":\n    do_somthing_else()\nelse:\n    do_this_instead()"
  },
  {
    "objectID": "posts/2021-03-21-full-stack-deep-learning-lecture-01.html",
    "href": "posts/2021-03-21-full-stack-deep-learning-lecture-01.html",
    "title": "Full Stack Deep Learning Notes - Lecture 01",
    "section": "",
    "text": "Models become hardware agnostic\nCode is clear to read because engineering code is abstracted away\nEasier to reproduce\nMake fewer mistakes because lightning handles the tricky engineering\nKeeps all the flexibility (LightningModules are still PyTorch modules), but removes a ton of boilerplate\nLightning has dozens of integrations with popular machine learning tools.\nTested rigorously with every new PR. We test every combination of PyTorch and Python supported versions, every OS, multi GPUs and even TPUs.\nMinimal running speed overhead (about 300 ms per epoch compared with pure PyTorch)."
  },
  {
    "objectID": "posts/2021-03-21-full-stack-deep-learning-lecture-01.html#advantages-over-unstructured-pytorch",
    "href": "posts/2021-03-21-full-stack-deep-learning-lecture-01.html#advantages-over-unstructured-pytorch",
    "title": "Full Stack Deep Learning Notes - Lecture 01",
    "section": "",
    "text": "Models become hardware agnostic\nCode is clear to read because engineering code is abstracted away\nEasier to reproduce\nMake fewer mistakes because lightning handles the tricky engineering\nKeeps all the flexibility (LightningModules are still PyTorch modules), but removes a ton of boilerplate\nLightning has dozens of integrations with popular machine learning tools.\nTested rigorously with every new PR. We test every combination of PyTorch and Python supported versions, every OS, multi GPUs and even TPUs.\nMinimal running speed overhead (about 300 ms per epoch compared with pure PyTorch)."
  },
  {
    "objectID": "posts/2022-11-08-kedro-debugging.html",
    "href": "posts/2022-11-08-kedro-debugging.html",
    "title": "Demo of debugging Kedro pipeline with noetebook",
    "section": "",
    "text": "Steps to debug Kedro pipeline in a notebook\n\nRead from stack trace - find out the line of code that produce the error\nFind which node this function belongs to\nTrying to rerun the pipeline just before this node\nIf it’s not a persisted dataset, you need to change it in catalog.yml, and re-run the pipeline, error is thrown again\nsession has already been used once, so if you call session again it will throw error. (so he had a wrapper function that recreate session and do something similar to session.run\nCreate a new session or %reload_kedro?\nNow catalog.load that persisted dataset, i.e. func(catalog.load(\"some_data\"))\nCopy the source code of func to notebook, it would work if the function itself is the node function, but if it is some function buried deep down, that’s a lot more copy-pasting and change of import maybe.\nChange the source code and make it work in the notebook\nRerun the pipeline to ensure everything works\n\n\n\nRunning Session as Usual\n\n%reload_kedro\n\n[11/08/22 16:44:22] INFO     Resolved project path as:                                              __init__.py:132\n                             /Users/Nok_Lam_Chan/dev/kedro_gallery/jupyter-debug-demo.                             \n                             To set a different path, run '%reload_kedro &lt;project_root&gt;'                           \n\n\n\n[11/08/22 16:44:24] INFO     Kedro project jupyter_debug_demo                                       __init__.py:101\n\n\n\n                    INFO     Defined global variable 'context', 'session', 'catalog' and            __init__.py:102\n                             'pipelines'                                                                           \n\n\n\n                    INFO     Registered line magic 'run_viz'                                        __init__.py:108\n\n\n\n\nsession\n\n&lt;kedro.framework.session.session.KedroSession object at 0x7fc47a1a0be0&gt;\n\n\n\n\npipelines\n\n{'__default__': Pipeline([\nNode(split_data, ['example_iris_data', 'parameters'], ['X_train', 'X_test', 'y_train', 'y_test'], 'split'),\nNode(make_predictions, ['X_train', 'X_test', 'y_train'], 'y_pred', 'make_predictions'),\nNode(report_accuracy, ['y_pred', 'y_test'], None, 'report_accuracy')\n])}\n\n\n\n\nsession.run()\n\n                    INFO     Kedro project jupyter-debug-demo                                        session.py:340\n\n\n\n[11/08/22 16:44:25] INFO     Loading data from 'example_iris_data' (CSVDataSet)...              data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'parameters' (MemoryDataSet)...                  data_catalog.py:343\n\n\n\n                    INFO     Running node: split: split_data([example_iris_data,parameters]) -&gt;         node.py:327\n                             [X_train,X_test,y_train,y_test]                                                       \n\n\n\n                    INFO     Saving data to 'X_train' (MemoryDataSet)...                        data_catalog.py:382\n\n\n\n                    INFO     Saving data to 'X_test' (MemoryDataSet)...                         data_catalog.py:382\n\n\n\n                    INFO     Saving data to 'y_train' (MemoryDataSet)...                        data_catalog.py:382\n\n\n\n                    INFO     Saving data to 'y_test' (PickleDataSet)...                         data_catalog.py:382\n\n\n\n                    INFO     Completed 1 out of 3 tasks                                     sequential_runner.py:85\n\n\n\n                    INFO     Loading data from 'X_train' (MemoryDataSet)...                     data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'X_test' (MemoryDataSet)...                      data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'y_train' (MemoryDataSet)...                     data_catalog.py:343\n\n\n\n                    INFO     Running node: make_predictions: make_predictions([X_train,X_test,y_train]) node.py:327\n                             -&gt; [y_pred]                                                                           \n\n\n\n1\n\n\n                    INFO     Saving data to 'y_pred' (PickleDataSet)...                         data_catalog.py:382\n\n\n\n                    INFO     Completed 2 out of 3 tasks                                     sequential_runner.py:85\n\n\n\n                    INFO     Loading data from 'y_pred' (PickleDataSet)...                      data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'y_test' (PickleDataSet)...                      data_catalog.py:343\n\n\n\n                    INFO     Running node: report_accuracy: report_accuracy([y_pred,y_test]) -&gt; None    node.py:327\n\n\n\n                    ERROR    Node 'report_accuracy: report_accuracy([y_pred,y_test]) -&gt; None' failed    node.py:352\n                             with error:                                                                           \n                             Simulate some bug here                                                                \n\n\n\n                    WARNING  There are 1 nodes that have not run.                                     runner.py:202\n                             You can resume the pipeline run from the nearest nodes with persisted                 \n                             inputs by adding the following argument to your previous command:                     \n                               --from-nodes \"report_accuracy\"                                                      \n\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ /var/folders/dv/bz0yz1dn71d2hygq110k3xhw0000gp/T/ipykernel_7863/833844929.py:1 in &lt;cell line: 1&gt; │\n│                                                                                                  │\n│ [Errno 2] No such file or directory:                                                             │\n│ '/var/folders/dv/bz0yz1dn71d2hygq110k3xhw0000gp/T/ipykernel_7863/833844929.py'                   │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/session/session.py:404 in run                   │\n│                                                                                                  │\n│   401 │   │   )                                                                                  │\n│   402 │   │                                                                                      │\n│   403 │   │   try:                                                                               │\n│ ❱ 404 │   │   │   run_result = runner.run(                                                       │\n│   405 │   │   │   │   filtered_pipeline, catalog, hook_manager, session_id                       │\n│   406 │   │   │   )                                                                              │\n│   407 │   │   │   self._run_called = True                                                        │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/runner.py:88 in run                                │\n│                                                                                                  │\n│    85 │   │   │   self._logger.info(                                                             │\n│    86 │   │   │   │   \"Asynchronous mode is enabled for loading and saving data\"                 │\n│    87 │   │   │   )                                                                              │\n│ ❱  88 │   │   self._run(pipeline, catalog, hook_manager, session_id)                             │\n│    89 │   │                                                                                      │\n│    90 │   │   self._logger.info(\"Pipeline execution completed successfully.\")                    │\n│    91                                                                                            │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/sequential_runner.py:70 in _run                    │\n│                                                                                                  │\n│   67 │   │                                                                                       │\n│   68 │   │   for exec_index, node in enumerate(nodes):                                           │\n│   69 │   │   │   try:                                                                            │\n│ ❱ 70 │   │   │   │   run_node(node, catalog, hook_manager, self._is_async, session_id)           │\n│   71 │   │   │   │   done_nodes.add(node)                                                        │\n│   72 │   │   │   except Exception:                                                               │\n│   73 │   │   │   │   self._suggest_resume_scenario(pipeline, done_nodes, catalog)                │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/runner.py:304 in run_node                          │\n│                                                                                                  │\n│   301 │   if is_async:                                                                           │\n│   302 │   │   node = _run_node_async(node, catalog, hook_manager, session_id)                    │\n│   303 │   else:                                                                                  │\n│ ❱ 304 │   │   node = _run_node_sequential(node, catalog, hook_manager, session_id)               │\n│   305 │                                                                                          │\n│   306 │   for name in node.confirms:                                                             │\n│   307 │   │   catalog.confirm(name)                                                              │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/runner.py:398 in _run_node_sequential              │\n│                                                                                                  │\n│   395 │   )                                                                                      │\n│   396 │   inputs.update(additional_inputs)                                                       │\n│   397 │                                                                                          │\n│ ❱ 398 │   outputs = _call_node_run(                                                              │\n│   399 │   │   node, catalog, inputs, is_async, hook_manager, session_id=session_id               │\n│   400 │   )                                                                                      │\n│   401                                                                                            │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/runner.py:366 in _call_node_run                    │\n│                                                                                                  │\n│   363 │   │   │   is_async=is_async,                                                             │\n│   364 │   │   │   session_id=session_id,                                                         │\n│   365 │   │   )                                                                                  │\n│ ❱ 366 │   │   raise exc                                                                          │\n│   367 │   hook_manager.hook.after_node_run(                                                      │\n│   368 │   │   node=node,                                                                         │\n│   369 │   │   catalog=catalog,                                                                   │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/runner.py:356 in _call_node_run                    │\n│                                                                                                  │\n│   353 ) -&gt; Dict[str, Any]:                                                                       │\n│   354 │   # pylint: disable=too-many-arguments                                                   │\n│   355 │   try:                                                                                   │\n│ ❱ 356 │   │   outputs = node.run(inputs)                                                         │\n│   357 │   except Exception as exc:                                                               │\n│   358 │   │   hook_manager.hook.on_node_error(                                                   │\n│   359 │   │   │   error=exc,                                                                     │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/pipeline/node.py:353 in run                               │\n│                                                                                                  │\n│   350 │   │   # purposely catch all exceptions                                                   │\n│   351 │   │   except Exception as exc:                                                           │\n│   352 │   │   │   self._logger.error(\"Node '%s' failed with error: \\n%s\", str(self), str(exc))   │\n│ ❱ 353 │   │   │   raise exc                                                                      │\n│   354 │                                                                                          │\n│   355 │   def _run_with_no_inputs(self, inputs: Dict[str, Any]):                                 │\n│   356 │   │   if inputs:                                                                         │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/pipeline/node.py:344 in run                               │\n│                                                                                                  │\n│   341 │   │   │   elif isinstance(self._inputs, str):                                            │\n│   342 │   │   │   │   outputs = self._run_with_one_input(inputs, self._inputs)                   │\n│   343 │   │   │   elif isinstance(self._inputs, list):                                           │\n│ ❱ 344 │   │   │   │   outputs = self._run_with_list(inputs, self._inputs)                        │\n│   345 │   │   │   elif isinstance(self._inputs, dict):                                           │\n│   346 │   │   │   │   outputs = self._run_with_dict(inputs, self._inputs)                        │\n│   347                                                                                            │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/pipeline/node.py:384 in _run_with_list                    │\n│                                                                                                  │\n│   381 │   │   │   │   f\"{sorted(inputs.keys())}.\"                                                │\n│   382 │   │   │   )                                                                              │\n│   383 │   │   # Ensure the function gets the inputs in the correct order                         │\n│ ❱ 384 │   │   return self._func(*(inputs[item] for item in node_inputs))                         │\n│   385 │                                                                                          │\n│   386 │   def _run_with_dict(self, inputs: Dict[str, Any], node_inputs: Dict[str, str]):         │\n│   387 │   │   # Node inputs and provided run inputs should completely overlap                    │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/dev/kedro_gallery/jupyter-debug-demo/src/jupyter_debug_demo/nodes.py:74 in   │\n│ report_accuracy                                                                                  │\n│                                                                                                  │\n│   71 │   │   y_pred: Predicted target.                                                           │\n│   72 │   │   y_test: True target.                                                                │\n│   73 │   \"\"\"                                                                                     │\n│ ❱ 74 │   raise ValueError(\"Simulate some bug here\")                                              │\n│   75 │   accuracy = (y_pred == y_test).sum() / len(y_test)                                       │\n│   76 │   logger = logging.getLogger(__name__)                                                    │\n│   77 │   logger.info(\"Model has accuracy of %.3f on test data.\", accuracy)                       │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nValueError: Simulate some bug here\n\n\n\n\nRead from stack trace - find out the line of code that produce the error\nFind which node this function belongs to\nTrying to rerun the pipeline just before this node\nIf it’s not a persisted dataset, you need to change it in catalog.yml, and re-run the pipeline, error is thrown again\nsession has already been used once, so if you call session again it will throw error. (so he had a wrapper function that recreate session and do something similar to session.run\nCreate a new session or %reload_kedro and re-run?\n\nThis is not efficient because in interactive workflow, these intermdiate variables is likely store in the catalog already.\n\n%reload_kedro\n\n[11/08/22 16:46:49] INFO     Resolved project path as:                                              __init__.py:132\n                             /Users/Nok_Lam_Chan/dev/kedro_gallery/jupyter-debug-demo.                             \n                             To set a different path, run '%reload_kedro &lt;project_root&gt;'                           \n\n\n\n[11/08/22 16:46:50] INFO     Kedro project jupyter_debug_demo                                       __init__.py:101\n\n\n\n                    INFO     Defined global variable 'context', 'session', 'catalog' and            __init__.py:102\n                             'pipelines'                                                                           \n\n\n\n                    INFO     Registered line magic 'run_viz'                                        __init__.py:108\n\n\n\n\nsession.run()\n\n[11/08/22 16:46:53] INFO     Kedro project jupyter-debug-demo                                        session.py:340\n\n\n\n[11/08/22 16:46:54] INFO     Loading data from 'example_iris_data' (CSVDataSet)...              data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'parameters' (MemoryDataSet)...                  data_catalog.py:343\n\n\n\n                    INFO     Running node: split: split_data([example_iris_data,parameters]) -&gt;         node.py:327\n                             [X_train,X_test,y_train,y_test]                                                       \n\n\n\n                    INFO     Saving data to 'X_train' (MemoryDataSet)...                        data_catalog.py:382\n\n\n\n                    INFO     Saving data to 'X_test' (MemoryDataSet)...                         data_catalog.py:382\n\n\n\n                    INFO     Saving data to 'y_train' (MemoryDataSet)...                        data_catalog.py:382\n\n\n\n                    INFO     Saving data to 'y_test' (PickleDataSet)...                         data_catalog.py:382\n\n\n\n                    INFO     Completed 1 out of 3 tasks                                     sequential_runner.py:85\n\n\n\n                    INFO     Loading data from 'X_train' (MemoryDataSet)...                     data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'X_test' (MemoryDataSet)...                      data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'y_train' (MemoryDataSet)...                     data_catalog.py:343\n\n\n\n                    INFO     Running node: make_predictions: make_predictions([X_train,X_test,y_train]) node.py:327\n                             -&gt; [y_pred]                                                                           \n\n\n\n1\n\n\n                    INFO     Saving data to 'y_pred' (PickleDataSet)...                         data_catalog.py:382\n\n\n\n                    INFO     Completed 2 out of 3 tasks                                     sequential_runner.py:85\n\n\n\n                    INFO     Loading data from 'y_pred' (PickleDataSet)...                      data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'y_test' (PickleDataSet)...                      data_catalog.py:343\n\n\n\n                    INFO     Running node: report_accuracy: report_accuracy([y_pred,y_test]) -&gt; None    node.py:327\n\n\n\n                    ERROR    Node 'report_accuracy: report_accuracy([y_pred,y_test]) -&gt; None' failed    node.py:352\n                             with error:                                                                           \n                             Simulate some bug here                                                                \n\n\n\n                    WARNING  There are 1 nodes that have not run.                                     runner.py:202\n                             You can resume the pipeline run from the nearest nodes with persisted                 \n                             inputs by adding the following argument to your previous command:                     \n                               --from-nodes \"report_accuracy\"                                                      \n\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ /var/folders/dv/bz0yz1dn71d2hygq110k3xhw0000gp/T/ipykernel_7863/833844929.py:1 in &lt;cell line: 1&gt; │\n│                                                                                                  │\n│ [Errno 2] No such file or directory:                                                             │\n│ '/var/folders/dv/bz0yz1dn71d2hygq110k3xhw0000gp/T/ipykernel_7863/833844929.py'                   │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/session/session.py:404 in run                   │\n│                                                                                                  │\n│   401 │   │   )                                                                                  │\n│   402 │   │                                                                                      │\n│   403 │   │   try:                                                                               │\n│ ❱ 404 │   │   │   run_result = runner.run(                                                       │\n│   405 │   │   │   │   filtered_pipeline, catalog, hook_manager, session_id                       │\n│   406 │   │   │   )                                                                              │\n│   407 │   │   │   self._run_called = True                                                        │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/runner.py:88 in run                                │\n│                                                                                                  │\n│    85 │   │   │   self._logger.info(                                                             │\n│    86 │   │   │   │   \"Asynchronous mode is enabled for loading and saving data\"                 │\n│    87 │   │   │   )                                                                              │\n│ ❱  88 │   │   self._run(pipeline, catalog, hook_manager, session_id)                             │\n│    89 │   │                                                                                      │\n│    90 │   │   self._logger.info(\"Pipeline execution completed successfully.\")                    │\n│    91                                                                                            │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/sequential_runner.py:70 in _run                    │\n│                                                                                                  │\n│   67 │   │                                                                                       │\n│   68 │   │   for exec_index, node in enumerate(nodes):                                           │\n│   69 │   │   │   try:                                                                            │\n│ ❱ 70 │   │   │   │   run_node(node, catalog, hook_manager, self._is_async, session_id)           │\n│   71 │   │   │   │   done_nodes.add(node)                                                        │\n│   72 │   │   │   except Exception:                                                               │\n│   73 │   │   │   │   self._suggest_resume_scenario(pipeline, done_nodes, catalog)                │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/runner.py:304 in run_node                          │\n│                                                                                                  │\n│   301 │   if is_async:                                                                           │\n│   302 │   │   node = _run_node_async(node, catalog, hook_manager, session_id)                    │\n│   303 │   else:                                                                                  │\n│ ❱ 304 │   │   node = _run_node_sequential(node, catalog, hook_manager, session_id)               │\n│   305 │                                                                                          │\n│   306 │   for name in node.confirms:                                                             │\n│   307 │   │   catalog.confirm(name)                                                              │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/runner.py:398 in _run_node_sequential              │\n│                                                                                                  │\n│   395 │   )                                                                                      │\n│   396 │   inputs.update(additional_inputs)                                                       │\n│   397 │                                                                                          │\n│ ❱ 398 │   outputs = _call_node_run(                                                              │\n│   399 │   │   node, catalog, inputs, is_async, hook_manager, session_id=session_id               │\n│   400 │   )                                                                                      │\n│   401                                                                                            │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/runner.py:366 in _call_node_run                    │\n│                                                                                                  │\n│   363 │   │   │   is_async=is_async,                                                             │\n│   364 │   │   │   session_id=session_id,                                                         │\n│   365 │   │   )                                                                                  │\n│ ❱ 366 │   │   raise exc                                                                          │\n│   367 │   hook_manager.hook.after_node_run(                                                      │\n│   368 │   │   node=node,                                                                         │\n│   369 │   │   catalog=catalog,                                                                   │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/runner.py:356 in _call_node_run                    │\n│                                                                                                  │\n│   353 ) -&gt; Dict[str, Any]:                                                                       │\n│   354 │   # pylint: disable=too-many-arguments                                                   │\n│   355 │   try:                                                                                   │\n│ ❱ 356 │   │   outputs = node.run(inputs)                                                         │\n│   357 │   except Exception as exc:                                                               │\n│   358 │   │   hook_manager.hook.on_node_error(                                                   │\n│   359 │   │   │   error=exc,                                                                     │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/pipeline/node.py:353 in run                               │\n│                                                                                                  │\n│   350 │   │   # purposely catch all exceptions                                                   │\n│   351 │   │   except Exception as exc:                                                           │\n│   352 │   │   │   self._logger.error(\"Node '%s' failed with error: \\n%s\", str(self), str(exc))   │\n│ ❱ 353 │   │   │   raise exc                                                                      │\n│   354 │                                                                                          │\n│   355 │   def _run_with_no_inputs(self, inputs: Dict[str, Any]):                                 │\n│   356 │   │   if inputs:                                                                         │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/pipeline/node.py:344 in run                               │\n│                                                                                                  │\n│   341 │   │   │   elif isinstance(self._inputs, str):                                            │\n│   342 │   │   │   │   outputs = self._run_with_one_input(inputs, self._inputs)                   │\n│   343 │   │   │   elif isinstance(self._inputs, list):                                           │\n│ ❱ 344 │   │   │   │   outputs = self._run_with_list(inputs, self._inputs)                        │\n│   345 │   │   │   elif isinstance(self._inputs, dict):                                           │\n│   346 │   │   │   │   outputs = self._run_with_dict(inputs, self._inputs)                        │\n│   347                                                                                            │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/GitHub/kedro/kedro/pipeline/node.py:384 in _run_with_list                    │\n│                                                                                                  │\n│   381 │   │   │   │   f\"{sorted(inputs.keys())}.\"                                                │\n│   382 │   │   │   )                                                                              │\n│   383 │   │   # Ensure the function gets the inputs in the correct order                         │\n│ ❱ 384 │   │   return self._func(*(inputs[item] for item in node_inputs))                         │\n│   385 │                                                                                          │\n│   386 │   def _run_with_dict(self, inputs: Dict[str, Any], node_inputs: Dict[str, str]):         │\n│   387 │   │   # Node inputs and provided run inputs should completely overlap                    │\n│                                                                                                  │\n│ /Users/Nok_Lam_Chan/dev/kedro_gallery/jupyter-debug-demo/src/jupyter_debug_demo/nodes.py:74 in   │\n│ report_accuracy                                                                                  │\n│                                                                                                  │\n│   71 │   │   y_pred: Predicted target.                                                           │\n│   72 │   │   y_test: True target.                                                                │\n│   73 │   \"\"\"                                                                                     │\n│ ❱ 74 │   raise ValueError(\"Simulate some bug here\")                                              │\n│   75 │   accuracy = (y_pred == y_test).sum() / len(y_test)                                       │\n│   76 │   logger = logging.getLogger(__name__)                                                    │\n│   77 │   logger.info(\"Model has accuracy of %.3f on test data.\", accuracy)                       │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nValueError: Simulate some bug here\n\n\n\n\nNow catalog.load that persisted dataset, i.e. func(catalog.load(\"some_data\"))\n\n\ny_pred = catalog.load(\"y_pred\")\ny_test = catalog.load(\"y_test\")\n\n[11/08/22 16:47:19] INFO     Loading data from 'y_pred' (PickleDataSet)...                      data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'y_test' (PickleDataSet)...                      data_catalog.py:343\n\n\n\n\ncatalog.datasets.y_pred.load().head()  # This is the alternative way to use auto-discovery which can be improved\n\n0     setosa\n2     setosa\n7     setosa\n20    setosa\n21    setosa\nName: species, dtype: object\n\n\n\n\nCopy the source code of func to notebook, it would work if the function itself is the node function, but if it is some function buried deep down, that’s a lot more copy-pasting and change of import maybe.\n\n\ndef report_accuracy(y_pred: pd.Series, y_test: pd.Series):\n    \"\"\"Calculates and logs the accuracy.\n\n    Args:\n        y_pred: Predicted target.\n        y_test: True target.\n    \"\"\"\n    raise ValueError(\"Simulate some bug here\")\n    accuracy = (y_pred == y_test).sum() / len(y_test)\n    logger = logging.getLogger(__name__)\n    logger.info(\"Model has accuracy of %.3f on test data.\", accuracy)\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ /var/folders/dv/bz0yz1dn71d2hygq110k3xhw0000gp/T/ipykernel_7863/1415042900.py:1 in &lt;cell line:   │\n│ 1&gt;                                                                                               │\n│                                                                                                  │\n│ [Errno 2] No such file or directory:                                                             │\n│ '/var/folders/dv/bz0yz1dn71d2hygq110k3xhw0000gp/T/ipykernel_7863/1415042900.py'                  │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nNameError: name 'pd' is not defined\n\n\n\nThis won’t work immediately work, a couple of copy&paste is needed\n\nmanual copy the imports\nRemove the function now - copy the source code as a cell instead\n\n\nimport pandas as pd\nimport logging\n\n\nraise ValueError(\"Simulate some bug here\")\naccuracy = (y_pred == y_test).sum() / len(y_test)\nlogger = logging.getLogger(__name__)\nlogger.info(\"Model has accuracy of %.3f on test data.\", accuracy)\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ /var/folders/dv/bz0yz1dn71d2hygq110k3xhw0000gp/T/ipykernel_7863/2816569123.py:1 in &lt;cell line:   │\n│ 1&gt;                                                                                               │\n│                                                                                                  │\n│ [Errno 2] No such file or directory:                                                             │\n│ '/var/folders/dv/bz0yz1dn71d2hygq110k3xhw0000gp/T/ipykernel_7863/2816569123.py'                  │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nValueError: Simulate some bug here\n\n\n\nAssume we know that the first line is buggy, let’s remove it\n\n# raise ValueError(\"Simulate some bug here\")\naccuracy = (y_pred == y_test).sum() / len(y_test)\nlogger = logging.getLogger(__name__)\nlogger.info(\"Model has accuracy of %.3f on test data.\", accuracy)\n# It now works - lets copy this block back into the function and rerun\n\n\nChange the source code and make it work in the notebook\nRerun the pipeline to ensure everything works\n\n\n%reload_kedro\nsession.run()\n\n[11/08/22 16:50:48] INFO     Resolved project path as:                                              __init__.py:132\n                             /Users/Nok_Lam_Chan/dev/kedro_gallery/jupyter-debug-demo.                             \n                             To set a different path, run '%reload_kedro &lt;project_root&gt;'                           \n\n\n\n[11/08/22 16:50:49] INFO     Kedro project jupyter_debug_demo                                       __init__.py:101\n\n\n\n                    INFO     Defined global variable 'context', 'session', 'catalog' and            __init__.py:102\n                             'pipelines'                                                                           \n\n\n\n                    INFO     Registered line magic 'run_viz'                                        __init__.py:108\n\n\n\n                    INFO     Kedro project jupyter-debug-demo                                        session.py:340\n\n\n\n[11/08/22 16:50:50] INFO     Loading data from 'example_iris_data' (CSVDataSet)...              data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'parameters' (MemoryDataSet)...                  data_catalog.py:343\n\n\n\n                    INFO     Running node: split: split_data([example_iris_data,parameters]) -&gt;         node.py:327\n                             [X_train,X_test,y_train,y_test]                                                       \n\n\n\n                    INFO     Saving data to 'X_train' (MemoryDataSet)...                        data_catalog.py:382\n\n\n\n                    INFO     Saving data to 'X_test' (MemoryDataSet)...                         data_catalog.py:382\n\n\n\n                    INFO     Saving data to 'y_train' (MemoryDataSet)...                        data_catalog.py:382\n\n\n\n                    INFO     Saving data to 'y_test' (PickleDataSet)...                         data_catalog.py:382\n\n\n\n                    INFO     Completed 1 out of 3 tasks                                     sequential_runner.py:85\n\n\n\n                    INFO     Loading data from 'X_train' (MemoryDataSet)...                     data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'X_test' (MemoryDataSet)...                      data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'y_train' (MemoryDataSet)...                     data_catalog.py:343\n\n\n\n                    INFO     Running node: make_predictions: make_predictions([X_train,X_test,y_train]) node.py:327\n                             -&gt; [y_pred]                                                                           \n\n\n\n1\n\n\n                    INFO     Saving data to 'y_pred' (PickleDataSet)...                         data_catalog.py:382\n\n\n\n                    INFO     Completed 2 out of 3 tasks                                     sequential_runner.py:85\n\n\n\n                    INFO     Loading data from 'y_pred' (PickleDataSet)...                      data_catalog.py:343\n\n\n\n                    INFO     Loading data from 'y_test' (PickleDataSet)...                      data_catalog.py:343\n\n\n\n                    INFO     Running node: report_accuracy: report_accuracy([y_pred,y_test]) -&gt; None    node.py:327\n\n\n\n                    INFO     Model has accuracy of 0.933 on test data.                                  nodes.py:77\n\n\n\n                    INFO     Completed 3 out of 3 tasks                                     sequential_runner.py:85\n\n\n\n                    INFO     Pipeline execution completed successfully.                                runner.py:90\n\n\n\n{}\n\n\n\nIt works now!\nDebugging with interactive session is not uncommon - compare to IDE/breakpoint. * You can make plots and see the data * You can intercept the variable and continue with the program - espeically useful when it is computation intensive.\nSee more comments from Antony\nMore to optimize 1st PoC * %load_node - populate all neccessary data where the node throws error * When pipeline fail - raise something like %load_node debug=True - the traceback should have information about which node the error is coming from. * Is there anything we can use viz? Sometimes I get question from people can kedro-viz help with debugging too.\nMore to optimize: * What if the error is not in the node function but somewhere deeper in the call stack? * Handle case when the inputs are not in catalog - how to recompute the necessary inputs? Potentially we can use the backtracking to do it in a more efficient way."
  },
  {
    "objectID": "posts/kedro-pipeline-slicing-pipeline/2024-03-06-Kedro-Pipeline-Slicing-Pipeline.html",
    "href": "posts/kedro-pipeline-slicing-pipeline/2024-03-06-Kedro-Pipeline-Slicing-Pipeline.html",
    "title": "Kedro Pipeline (1) - Slicing Pipeline Effortlessly 🍕",
    "section": "",
    "text": "from kedro.pipeline import pipeline, node\nfrom kedro.pipeline.node import Node\ndef foo():\n   return \"bar\""
  },
  {
    "objectID": "posts/kedro-pipeline-slicing-pipeline/2024-03-06-Kedro-Pipeline-Slicing-Pipeline.html#node-uniqueness",
    "href": "posts/kedro-pipeline-slicing-pipeline/2024-03-06-Kedro-Pipeline-Slicing-Pipeline.html#node-uniqueness",
    "title": "Kedro Pipeline (1) - Slicing Pipeline Effortlessly 🍕",
    "section": "Node Uniqueness",
    "text": "Node Uniqueness\nThe pipeline in Kedro automatically validates Node instances. Specifically, nodes cannot produce the same output (though they can share the same input), and there cannot be duplicate nodes within the pipeline. This validation is crucial to ensure that the pipeline forms an executable Directed Acyclic Graph (DAG), allowing for proper execution and preventing any cyclic dependencies.\n\npipeline([node_a, node_a])\n\nValueError: Pipeline nodes must have unique names. The following node names appear more than once:\n\nFree nodes:\n  - foo(None) -&gt; [output_a]\n\nYou can name your nodes using the last argument of 'node()'.\n\n\nOn the other hand, Node are considered equal if they have the same inputs, outputsand function (and node name if provided, it is an optional argument)\n\nnode_b = node(foo, inputs=None, outputs=\"output_a\")\n\n\nnode_b == node_a\n\nTrue\n\n\nInternally, it is comparing the name attribute, which is a combination of namespace, function name, inputs and outputs. This is not important to most Kedro users and are only used by Kedro internally.\n\nnode_a.name\n\n\n'foo(None) -&gt; [output_a]'\n\n\n\nNode.__str__??\n\nSignature: Node.__str__(self)\nDocstring: Return str(self).\nSource:   \n    def __str__(self):\n        def _set_to_str(xset):\n            return f\"[{','.join(xset)}]\"\n\n        out_str = _set_to_str(self.outputs) if self._outputs else \"None\"\n        in_str = _set_to_str(self.inputs) if self._inputs else \"None\"\n\n        prefix = self._name + \": \" if self._name else \"\"\n        return prefix + f\"{self._func_name}({in_str}) -&gt; {out_str}\"\nFile:      ~/miniconda3/envs/blog/lib/python3.10/site-packages/kedro/pipeline/node.py\nType:      function"
  },
  {
    "objectID": "posts/kedro-pipeline-slicing-pipeline/2024-03-06-Kedro-Pipeline-Slicing-Pipeline.html#pipeline-arithmetic",
    "href": "posts/kedro-pipeline-slicing-pipeline/2024-03-06-Kedro-Pipeline-Slicing-Pipeline.html#pipeline-arithmetic",
    "title": "Kedro Pipeline (1) - Slicing Pipeline Effortlessly 🍕",
    "section": "Pipeline Arithmetic",
    "text": "Pipeline Arithmetic\nThe closest analogy to Pipeline is the Python set. They share simliary characteristics: - The elements cannot be repeated. - Pipelines can be added or subtracted\n\npipeline([node_a]) + pipeline([node_a])\n\nPipeline([\nNode(foo, None, 'output_a', None)\n])\n\n\n\na = node(foo, None, \"a\")\nb = node(foo, None, \"b\")\nc = node(foo, None, \"c\")\nd = node(foo, None, \"d\")\n\noriginal_set = set([\"a\",\"b\",\"c\"])\noriginal_pipeline = pipeline([a,b,c])\n\n\npipeline([a]) + pipeline([b])\n\nPipeline([\nNode(foo, None, 'a', None),\nNode(foo, None, 'b', None)\n])\n\n\n\npipeline([a, b]) - pipeline([b])\n\nPipeline([\nNode(foo, None, 'a', None)\n])\n\n\n\npipeline([a, b]) - pipeline([a])\n\nPipeline([\nNode(foo, None, 'b', None)\n])\n\n\n\noriginal_set| set([\"b\",\"c\",\"d\"])\n\n{'a', 'b', 'c', 'd'}\n\n\n\npipeline([a,b,c])| pipeline([b,c,d]) # nodes in both pipelines\n\nPipeline([\nNode(foo, None, 'a', None),\nNode(foo, None, 'b', None),\nNode(foo, None, 'c', None),\nNode(foo, None, 'd', None)\n])\n\n\n\noriginal_set & set([\"b\",\"c\",\"d\"])\n\n{'b', 'c'}\n\n\n\npipeline([a,b,c]) & pipeline([b,c,d]) # only nodes that exist in both pipelines\n\nPipeline([\nNode(foo, None, 'b', None),\nNode(foo, None, 'c', None)\n])\n\n\nPipeline arithmetic is more useful for pipeline registration i.e. pipeline_registry.py. For example, you can combine your development pipeline and inference pipeline in different way.\n\ndef fake_node(name):\n    return node(foo, inputs=None, outputs=name, name=name)\n\n# For simplicaition, let's assume each pipeline is just one single node.\nspark_pipeline = pipeline([fake_node(\"spark\")])\nfeature_engineering = pipeline([fake_node(\"feature_engineering\")])\nmodel_training = pipeline([fake_node(\"model_pipeline\")])\ninference = pipeline([fake_node(\"inference\")])\n\nWith 4 base pipelines, you can combined them in different ways. For example you want a e2e pipeline which add all of them.\n\ne2e = spark_pipeline + feature_engineering + model_training + inference\n\nYou can also have a local pipeline that skip only the spark pipeline.\n\nlocal = e2e - spark_pipeline\nlocal\n\nPipeline([\nNode(foo, None, 'feature_engineering', 'feature_engineering'),\nNode(foo, None, 'inference', 'inference'),\nNode(foo, None, 'model_pipeline', 'model_pipeline')\n])"
  },
  {
    "objectID": "posts/kedro-pipeline-slicing-pipeline/2024-03-06-Kedro-Pipeline-Slicing-Pipeline.html#advance-pipeline-slicing",
    "href": "posts/kedro-pipeline-slicing-pipeline/2024-03-06-Kedro-Pipeline-Slicing-Pipeline.html#advance-pipeline-slicing",
    "title": "Kedro Pipeline (1) - Slicing Pipeline Effortlessly 🍕",
    "section": "Advance Pipeline Slicing",
    "text": "Advance Pipeline Slicing\nKedro provides an interaction visualisation that you can play around with, for this post I am gonna stick with the demo project and explains concepts about Pipeline and how you can slice pipeline and compose them.\n\n#hide\n%load_ext kedro.ipython\n%cd /Users/Nok_Lam_Chan/dev/kedro-viz/demo-project\n\n\n%reload_kedro /Users/Nok_Lam_Chan/dev/kedro-viz/demo-project\n\n[03/07/24 14:02:54] INFO     Reached after_catalog_created hook                                        plugin.py:15\n\n\n\n                    INFO     Kedro project modular-spaceflights                                     __init__.py:134\n\n\n\n                    INFO     Defined global variable 'context', 'session', 'catalog' and            __init__.py:135\n                             'pipelines'                                                                           \n\n\n\nBy using the reload_kedro inside a notebook, you can access the project pipelines object. Let’s say I want to filter out the highlighted pipeline like this (click on the “Create Derived Features”): \nTo filter this with the Pipeline API, you need two options. from-nodes(downstream) and to-nodes (upstream).\n\npipelines.keys()\n\n\n\n\n\ndict_keys(['__default__', 'Data ingestion', 'Modelling stage', 'Feature engineering', 'Reporting stage', 'Pre-modelling'])\n\n\n\n\nfull_pipeline\n\n\n\n\n\n\nPipeline([\nNode(apply_types_to_companies, 'companies', 'ingestion.int_typed_companies', 'apply_types_to_companies'),\nNode(apply_types_to_reviews, ['reviews', 'params:ingestion.typing.reviews.columns_as_floats'], 'ingestion.int_typed_reviews', 'apply_types_to_reviews'),\nNode(apply_types_to_shuttles, 'shuttles', 'ingestion.int_typed_shuttles@pandas1', 'apply_types_to_shuttles'),\nNode(aggregate_company_data, 'ingestion.int_typed_companies', 'ingestion.prm_agg_companies', 'company_agg'),\nNode(combine_shuttle_level_information, {'shuttles': 'ingestion.int_typed_shuttles@pandas2', 'reviews': 'ingestion.int_typed_reviews', 'companies': 'ingestion.prm_agg_companies'}, ['prm_shuttle_company_reviews', 'prm_spine_table'], 'combine_step'),\nNode(create_derived_features, ['prm_spine_table', 'prm_shuttle_company_reviews', 'params:feature_engineering.feature.derived'], 'feature_engineering.feat_derived_features', 'create_derived_features'),\nNode(create_feature_importance, 'prm_spine_table', 'feature_importance_output', None),\nNode(create_static_features, ['prm_shuttle_company_reviews', 'params:feature_engineering.feature.static'], 'feature_engineering.feat_static_features', None),\nNode(&lt;lambda&gt;, 'prm_spine_table', 'ingestion.prm_spine_table_clone', None),\nNode(create_matplotlib_chart, 'prm_shuttle_company_reviews', 'reporting.confusion_matrix', None),\n...\n])\n\n\n\n\nnode_name = \"feature_engineering.create_derived_features\" # make s|apipeline\nfull_pipeline.filter(from_nodes=[node_name], to_nodes=[node_name])\n\n\n\n\n\n\nPipeline([\nNode(create_derived_features, ['prm_spine_table', 'prm_shuttle_company_reviews', 'params:feature_engineering.feature.derived'], 'feature_engineering.feat_derived_features', 'create_derived_features')\n])\n\n\n\nThis only select one node because by default the filter method apply both method as an and condition. So we need to apply the filter method separately.\n\nfull_pipeline.filter(from_nodes=[node_name]) | full_pipeline.filter(to_nodes=[node_name])\n\n\n\n\n\n\nPipeline([\nNode(apply_types_to_companies, 'companies', 'ingestion.int_typed_companies', 'apply_types_to_companies'),\nNode(apply_types_to_reviews, ['reviews', 'params:ingestion.typing.reviews.columns_as_floats'], 'ingestion.int_typed_reviews', 'apply_types_to_reviews'),\nNode(apply_types_to_shuttles, 'shuttles', 'ingestion.int_typed_shuttles@pandas1', 'apply_types_to_shuttles'),\nNode(aggregate_company_data, 'ingestion.int_typed_companies', 'ingestion.prm_agg_companies', 'company_agg'),\nNode(combine_shuttle_level_information, {'shuttles': 'ingestion.int_typed_shuttles@pandas2', 'reviews': 'ingestion.int_typed_reviews', 'companies': 'ingestion.prm_agg_companies'}, ['prm_shuttle_company_reviews', 'prm_spine_table'], 'combine_step'),\nNode(create_derived_features, ['prm_spine_table', 'prm_shuttle_company_reviews', 'params:feature_engineering.feature.derived'], 'feature_engineering.feat_derived_features', 'create_derived_features'),\nNode(joiner, ['prm_spine_table', 'feature_engineering.feat_static_features', 'feature_engineering.feat_derived_features'], 'model_input_table', None),\nNode(split_data, ['model_input_table', 'params:split_options'], ['X_train', 'X_test', 'y_train', 'y_test'], None),\nNode(train_model, ['X_train', 'y_train', 'params:train_evaluation.model_options.linear_regression'], ['train_evaluation.linear_regression.regressor', 'train_evaluation.linear_regression.experiment_params'], None),\nNode(train_model, ['X_train', 'y_train', 'params:train_evaluation.model_options.random_forest'], ['train_evaluation.random_forest.regressor', 'train_evaluation.random_forest.experiment_params'], None),\n...\n])\n\n\n\nNow we get the correct filtered pipeline as expected."
  },
  {
    "objectID": "posts/kedro-pipeline-slicing-pipeline/2024-03-06-Kedro-Pipeline-Slicing-Pipeline.html#more-notes",
    "href": "posts/kedro-pipeline-slicing-pipeline/2024-03-06-Kedro-Pipeline-Slicing-Pipeline.html#more-notes",
    "title": "Kedro Pipeline (1) - Slicing Pipeline Effortlessly 🍕",
    "section": "More notes",
    "text": "More notes\n\nThe Pipeline.filter support or operator\n\n\nSelecting or slicing multiple pipeline with kedro run\nSince Pipeline API support arithmetic, it would be quite straight forward to support things like kedro run --pipeline a+b or kedro run --pipeline a-b. Let’s have a look what’s options are available for the CLI.\n\n!kedro run --help\n\nUsage: kedro run [OPTIONS]\n\n  Run the pipeline.\n\nOptions:\n  --from-inputs TEXT         A list of dataset names which should be used as a\n                             starting point.\n  --to-outputs TEXT          A list of dataset names which should be used as\n                             an end point.\n  --from-nodes TEXT          A list of node names which should be used as a\n                             starting point.\n  --to-nodes TEXT            A list of node names which should be used as an\n                             end point.\n  -n, --nodes TEXT           Run only nodes with specified names.\n  -r, --runner TEXT          Specify a runner that you want to run the\n                             pipeline with. Available runners:\n                             'SequentialRunner', 'ParallelRunner' and\n                             'ThreadRunner'.\n  --async                    Load and save node inputs and outputs\n                             asynchronously with threads. If not specified,\n                             load and save datasets synchronously.\n  -e, --env TEXT             Kedro configuration environment name. Defaults to\n                             `local`.\n  -t, --tags TEXT            Construct the pipeline using only nodes which\n                             have this tag attached. Option can be used\n                             multiple times, what results in a pipeline\n                             constructed from nodes having any of those tags.\n  -lv, --load-versions TEXT  Specify a particular dataset version (timestamp)\n                             for loading.\n  -p, --pipeline TEXT        Name of the registered pipeline to run. If not\n                             set, the '__default__' pipeline is run.\n  -ns, --namespace TEXT      Name of the node namespace to run.\n  -c, --config FILE          Specify a YAML configuration file to load the run\n                             command arguments from. If command line arguments\n                             are provided, they will override the loaded ones.\n  --conf-source PATH         Path of a directory where project configuration\n                             is stored.\n  --params TEXT              Specify extra parameters that you want to pass to\n                             the context initialiser. Items must be separated\n                             by comma, keys - by colon or equals sign,\n                             example: param1=value1,param2=value2. Each\n                             parameter is split by the first comma, so\n                             parameter values are allowed to contain colons,\n                             parameter keys are not. To pass a nested\n                             dictionary as parameter, separate keys by '.',\n                             example: param_group.param1:value1.\n  -h, --help                 Show this message and exit.\n\n\nThis is what happen when you do kedro run -p training -t model_a, it’s a two steps flitering: 1. Apply the -p pipeline name to select a key from the pipeline dictionary, it’s just pipelines[pipeline_name], note this mean you can only select ONE pipeline at a time. 2. The pipeline is then further filtered with Pipeline.filter\n\nfrom kedro.pipeline.pipeline import Pipeline\nPipeline.filter??\n\nSignature:\nPipeline.filter(\n    self,\n    tags: 'Iterable[str] | None' = None,\n    from_nodes: 'Iterable[str] | None' = None,\n    to_nodes: 'Iterable[str] | None' = None,\n    node_names: 'Iterable[str] | None' = None,\n    from_inputs: 'Iterable[str] | None' = None,\n    to_outputs: 'Iterable[str] | None' = None,\n    node_namespace: 'str | None' = None,\n) -&gt; 'Pipeline'\nSource:   \n    def filter(  # noqa: PLR0913\n        self,\n        tags: Iterable[str] | None = None,\n        from_nodes: Iterable[str] | None = None,\n        to_nodes: Iterable[str] | None = None,\n        node_names: Iterable[str] | None = None,\n        from_inputs: Iterable[str] | None = None,\n        to_outputs: Iterable[str] | None = None,\n        node_namespace: str | None = None,\n    ) -&gt; Pipeline:\n        \"\"\"Creates a new ``Pipeline`` object with the nodes that meet all of the\n        specified filtering conditions.\n\n        The new pipeline object is the intersection of pipelines that meet each\n        filtering condition. This is distinct from chaining multiple filters together.\n\n        Args:\n            tags: A list of node tags which should be used to lookup\n                the nodes of the new ``Pipeline``.\n            from_nodes: A list of node names which should be used as a\n                starting point of the new ``Pipeline``.\n            to_nodes:  A list of node names which should be used as an\n                end point of the new ``Pipeline``.\n            node_names: A list of node names which should be selected for the\n                new ``Pipeline``.\n            from_inputs: A list of inputs which should be used as a starting point\n                of the new ``Pipeline``\n            to_outputs: A list of outputs which should be the final outputs of\n                the new ``Pipeline``.\n            node_namespace: One node namespace which should be used to select\n                nodes in the new ``Pipeline``.\n\n        Returns:\n            A new ``Pipeline`` object with nodes that meet all of the specified\n                filtering conditions.\n\n        Raises:\n            ValueError: The filtered ``Pipeline`` has no nodes.\n\n        Example:\n        ::\n\n            &gt;&gt;&gt; pipeline = Pipeline(\n            &gt;&gt;&gt;     [\n            &gt;&gt;&gt;         node(func, \"A\", \"B\", name=\"node1\"),\n            &gt;&gt;&gt;         node(func, \"B\", \"C\", name=\"node2\"),\n            &gt;&gt;&gt;         node(func, \"C\", \"D\", name=\"node3\"),\n            &gt;&gt;&gt;     ]\n            &gt;&gt;&gt; )\n            &gt;&gt;&gt; pipeline.filter(node_names=[\"node1\", \"node3\"], from_inputs=[\"A\"])\n            &gt;&gt;&gt; # Gives a new pipeline object containing node1 and node3.\n        \"\"\"\n        # Use [node_namespace] so only_nodes_with_namespace can follow the same\n        # *filter_args pattern as the other filtering methods, which all take iterables.\n        node_namespace_iterable = [node_namespace] if node_namespace else None\n\n        filter_methods = {\n            self.only_nodes_with_tags: tags,\n            self.from_nodes: from_nodes,\n            self.to_nodes: to_nodes,\n            self.only_nodes: node_names,\n            self.from_inputs: from_inputs,\n            self.to_outputs: to_outputs,\n            self.only_nodes_with_namespace: node_namespace_iterable,\n        }\n\n        subset_pipelines = {\n            filter_method(*filter_args)  # type: ignore\n            for filter_method, filter_args in filter_methods.items()\n            if filter_args\n        }\n\n        # Intersect all the pipelines subsets. We apply each filter to the original\n        # pipeline object (self) rather than incrementally chaining filter methods\n        # together. Hence the order of filtering does not affect the outcome, and the\n        # resultant pipeline is unambiguously defined.\n        # If this were not the case then, for example,\n        # pipeline.filter(node_names=[\"node1\", \"node3\"], from_inputs=[\"A\"])\n        # would give different outcomes depending on the order of filter methods:\n        # only_nodes and then from_inputs would give node1, while only_nodes and then\n        # from_inputs would give node1 and node3.\n        filtered_pipeline = Pipeline(self.nodes)\n        for subset_pipeline in subset_pipelines:\n            filtered_pipeline &= subset_pipeline\n\n        if not filtered_pipeline.nodes:\n            raise ValueError(\n                \"Pipeline contains no nodes after applying all provided filters\"\n            )\n        return filtered_pipeline\nFile:      ~/dev/kedro/kedro/pipeline/pipeline.py\nType:      function\n\n\nThis means that, if you have tags applied across multiple pipeline, you cannot filter it by tag, unless you apply the filter in the largest pipeline that contains all nodes. What if we can support things like: kedro run -p feature+training -t model_a?"
  },
  {
    "objectID": "posts/2021-03-19-deepcopy-lightgbm-and-🥒pickles.html",
    "href": "posts/2021-03-19-deepcopy-lightgbm-and-🥒pickles.html",
    "title": "deepcopy, LGBM and pickle",
    "section": "",
    "text": "To start with, let’s look at some code to get some context."
  },
  {
    "objectID": "posts/2021-03-19-deepcopy-lightgbm-and-🥒pickles.html#deepcopy-or-no-copy",
    "href": "posts/2021-03-19-deepcopy-lightgbm-and-🥒pickles.html#deepcopy-or-no-copy",
    "title": "deepcopy, LGBM and pickle",
    "section": "deepcopy or no copy?",
    "text": "deepcopy or no copy?\n\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom copy import deepcopy\n\nparams = {\n'objective': 'regression',\n'verbose': -1,\n'num_leaves': 3\n}\n\nX = np.random.rand(100,2)\nY = np.ravel(np.random.rand(100,1))\nlgbm = lgb.train(params, lgb.Dataset(X,label=Y),num_boost_round=1)\nprint(\"Parameters of the model: \", lgbm.params)\n\nParameters of the model:  {'objective': 'regression', 'verbose': -1, 'num_leaves': 3, 'num_iterations': 1, 'early_stopping_round': None}\n\n\n\n## Deep copy will missing params\nnew_model = deepcopy(lgbm)\n\nFinished loading model, total used 1 iterations\n\n\nYou would expect new_model.parameters return the same dict right? Not quite.\n\nprint(\"Parameters of the copied model: \", new_model.params)\n\nParameters of the copied model:  {}\n\n\nSurprise, surprise. It’s an empty dict, where did the parameters go? To dive deep into the issue, let’s have a look at the source code of deepcopy to understand how does it work.\nreference: https://github.com/python/cpython/blob/e8e341993e3f80a3c456fb8e0219530c93c13151/Lib/copy.py#L128\ndef deepcopy(x, memo=None, _nil=[]):\n    \"\"\"Deep copy operation on arbitrary Python objects.\n    See the module's __doc__ string for more info.\n    \"\"\"\n\n    ... # skip some irrelevant code  \n\n    cls = type(x)\n\n    copier = _deepcopy_dispatch.get(cls)\n    if copier is not None:\n        y = copier(x, memo)\n    else:\n        if issubclass(cls, type):\n            y = _deepcopy_atomic(x, memo)\n        else:\n            copier = getattr(x, \"__deepcopy__\", None)\n            if copier is not None:\n                y = copier(memo)\n            else:\n                ... # skip irrelevant code\n\n    # If is its own copy, don't memoize.\n    if y is not x:\n        memo[d] = y\n        _keep_alive(x, memo) # Make sure x lives at least as long as d\n    return y\nIn particular, line 17 is what we care.\ncopier = getattr(x, \"__deepcopy__\", None)\nIf a particular class has implement the __deepcopy__ method, deepcopy will try to invoke that instead of the standard copy. The following dummy class should illustrate this clearly.\n\nclass DummyClass():\n    def __deepcopy__(self, _):\n        print('Just hanging around and not copying.')\n\n\no = DummyClass()\ndeepcopy(o)\n\nJust hanging around and not copying.\n\n\na lightgbm model is actually a Booster object and implement its own __deepcopy__. It only copy the model string but nothing else, this explains why deepcopy(lgbm).paramters is an empty dictionary.\n def __deepcopy__(self, _): \n     model_str = self.model_to_string(num_iteration=-1) \n     booster = Booster(model_str=model_str) \n     return booster \nReference: https://github.com/microsoft/LightGBM/blob/d6ebd063fff7ff9ed557c3f2bcacc8f9456583e6/python-package/lightgbm/basic.py#L2279-L2282\nOkay, so why lightgbm need to have an custom implementation? I thought this is a bug, but turns out there are some deeper reason behind this. I created an issue on GitHub.\nhttps://github.com/microsoft/LightGBM/issues/4085 Their response is &gt; Custom deepcopy is needed to make Booster class picklable."
  },
  {
    "objectID": "posts/2021-03-19-deepcopy-lightgbm-and-🥒pickles.html#italian-bmt-lettuce-tomato-and-some-pickles-please",
    "href": "posts/2021-03-19-deepcopy-lightgbm-and-🥒pickles.html#italian-bmt-lettuce-tomato-and-some-pickles-please",
    "title": "deepcopy, LGBM and pickle",
    "section": "🥖Italian BMT, 🥬Lettuce 🍅 tomato and some 🥒pickles please",
    "text": "🥖Italian BMT, 🥬Lettuce 🍅 tomato and some 🥒pickles please\nWhat does pickle really is? and what makes an object pickable?\n\nPython Pickle is used to serialize and deserialize a python object structure. Any object on python can be pickled so that it can be saved on disk.\n\nSerialization roughly means translating the data in memory into a format that can be stored on disk or sent over network. It’s like ordering a chair from Ikea, they will send you a box, but not a chair.\nThe process of decomposing the chair and put it into a box is serialization, while putting it together is deserialization. With pickle terms, we called it Pickling and Unpickling.\n\n\n\ndeserialize and serialize\n\n\n\nWhat is Pickle\nPickle is a protocol for Python, you and either pickling a Python object to memory or to file.\n\nimport pickle\n\n\nd = {'a': 1}\npickle_d = pickle.dumps(d)\npickle_d\n\nb'\\x80\\x04\\x95\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00}\\x94\\x8c\\x01a\\x94K\\x01s.'\n\n\nThe python dict is now transfrom into a series of binary str, this string can be only understand by Python. We can also deserialize a binary string back to a python dict.\n\nbinary_str = b'\\x80\\x04\\x95\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00}\\x94\\x8c\\x01a\\x94K\\x01s.'\npickle.loads(binary_str)\n\n{'a': 1}\n\n\nReference: https://www.python.org/dev/peps/pep-0574/#:~:text=The%20pickle%20protocol%20was%20originally%20designed%20in%201995,copying%20temporary%20data%20before%20writing%20it%20to%20disk.\n\n\nWhat makes something picklable\nFinally, we come back to our initial questions. &gt; What makes something picklable? Why lightgbm need to have deepcopy to make the Booster class picklable?\n\nWhat can be pickled and unpickled? The following types can be pickled:\n* None, True, and False\n* integers, floating point numbers, complex numbers\n* strings, bytes, bytearrays\n* tuples, lists, sets, and dictionaries containing only picklable objects\n* functions defined at the top level of a module (using def, not lambda)\n* built-in functions defined at the top level of a module\n* classes that are defined at the top level of a module\n\nSo pretty much common datatype, functions and classes are picklable. Let’s see without __deepcopy__, the Booster class is not serializable as it claims.\n\nimport lightgbm\nfrom lightgbm import Booster\ndel Booster.__deepcopy__\n\nparams = {\n'objective': 'regression',\n'verbose': -1,\n'num_leaves': 3\n}\n\nX = np.random.rand(100,2)\nY = np.ravel(np.random.rand(100,1))\nlgbm = lgb.train(params, lgb.Dataset(X,label=Y),num_boost_round=1)\n\n\ndeepcopy_lgbm = deepcopy(lgbm)\nlgbm.params, deepcopy_lgbm.params\n\n({'objective': 'regression',\n  'verbose': -1,\n  'num_leaves': 3,\n  'num_iterations': 1,\n  'early_stopping_round': None},\n {'objective': 'regression',\n  'verbose': -1,\n  'num_leaves': 3,\n  'num_iterations': 1,\n  'early_stopping_round': None})\n\n\n\npickle.dumps(deepcopy_lgbm) == pickle.dumps(lgbm)\n\nTrue\n\n\n\nunpickle_model = pickle.loads(pickle.dumps(deepcopy_lgbm))\nunpickle_deepcopy_model = pickle.loads(pickle.dumps(lgbm))\n\n\nunpickle_model.params, unpickle_deepcopy_model.params\n\n({'objective': 'regression',\n  'verbose': -1,\n  'num_leaves': 3,\n  'num_iterations': 1,\n  'early_stopping_round': None},\n {'objective': 'regression',\n  'verbose': -1,\n  'num_leaves': 3,\n  'num_iterations': 1,\n  'early_stopping_round': None})\n\n\n\nunpickle_model.model_to_string() == unpickle_deepcopy_model.model_to_string()\n\nTrue\n\n\n\nunpickle_deepcopy_model.predict(X)\n\narray([0.48439803, 0.48439803, 0.50141491, 0.48439803, 0.48439803,\n       0.48439803, 0.50141491, 0.48439803, 0.48439803, 0.48439803,\n       0.49029787, 0.49029787, 0.48439803, 0.48439803, 0.48439803,\n       0.49029787, 0.48439803, 0.50141491, 0.50141491, 0.50141491,\n       0.48439803, 0.50141491, 0.48439803, 0.49029787, 0.50141491,\n       0.50141491, 0.48439803, 0.49029787, 0.49029787, 0.49029787,\n       0.49029787, 0.50141491, 0.48439803, 0.50141491, 0.48439803,\n       0.49029787, 0.50141491, 0.48439803, 0.48439803, 0.48439803,\n       0.48439803, 0.50141491, 0.50141491, 0.48439803, 0.49029787,\n       0.48439803, 0.48439803, 0.50141491, 0.48439803, 0.48439803,\n       0.48439803, 0.48439803, 0.48439803, 0.48439803, 0.50141491,\n       0.49029787, 0.48439803, 0.50141491, 0.49029787, 0.49029787,\n       0.50141491, 0.50141491, 0.48439803, 0.50141491, 0.48439803,\n       0.48439803, 0.48439803, 0.48439803, 0.50141491, 0.48439803,\n       0.48439803, 0.50141491, 0.50141491, 0.49029787, 0.50141491,\n       0.48439803, 0.49029787, 0.48439803, 0.48439803, 0.50141491,\n       0.50141491, 0.48439803, 0.49029787, 0.48439803, 0.48439803,\n       0.50141491, 0.49029787, 0.50141491, 0.50141491, 0.49029787,\n       0.48439803, 0.49029787, 0.48439803, 0.48439803, 0.48439803,\n       0.48439803, 0.48439803, 0.48439803, 0.50141491, 0.49029787])\n\n\n\n\nLast Word\nWell…. It seems actually picklable? I may need to investigate the issue a bit more. For now, the __deepcopy__ does not seems to be necessary.\nI tried to dig into lightgbm source code and find this potential related issue. https://github.com/microsoft/LightGBM/blame/dc1bc23adf1137ef78722176e2da69f8411b1feb/python-package/lightgbm/basic.py#L2298"
  },
  {
    "objectID": "posts/2021-03-05-pyodbc-linux.html",
    "href": "posts/2021-03-05-pyodbc-linux.html",
    "title": "Setting up pyodbc for Impala connection, works on both Linux and Window",
    "section": "",
    "text": "Introduction\nLong story short, connect with Impala is a big headache in Windows. pyhive, impyla are both buggy. At the end, I stick with pyodbc as it works on both Linux and Windows, and seems to have better performance. There are not many steps, but it would be tricky if you try to Google as there are not much guide that just work out of the box\n\n\nSetup\nFirst, you need to download the ODBC driver from Cloudera.\nThen you need to instsall the driver properly.\ndpkg -i docker/clouderaimpalaodbc_2.6.10.1010-2_amd64.deb\nAdd this file to the directory /etc/odbcinst.ini, if you already have add, append this to the file.\n# /etc/odbcinst.ini\n[ODBC Drivers]\nCloudera Impala ODBC Driver 32-bit=Installed\nCloudera Impala ODBC Driver 64-bit=Installed\n[Cloudera Impala ODBC Driver 32-bit]\nDescription=Cloudera Impala ODBC Driver (32-bit)\nDriver=/opt/cloudera/impalaodbc/lib/32/libclouderaimpalaodbc32.so\n[Cloudera Impala ODBC Driver 64-bit]\nDescription=Cloudera Impala ODBC Driver (64-bit)\nDriver=/opt/cloudera/impalaodbc/lib/64/libclouderaimpalaodbc64.so\nThen install some additional package.\napt-get update && apt-get -y install gnupg apt-transport-https\napt-get update && apt-get -y install libssl1.0.0 unixodbc unixodbc-dev \\\n&& ACCEPT_EULA=Y apt-get -y install msodbcsql17\napt-get install unixodbc-dev -y\nLast, pip install pyodbc and have fun.\nTo read a database table, you can simply do this.\nimport pyodbc\nimport pandas as pd\n\nconn = pyodbc.connect(f\"\"\"\nDriver=Cloudera ODBC Driver for Impala 64-bit;\nPWD=password;\nUID=username;\nDatabase=database\n\"\"\")\nThere are multiple way to connect, but I found using a connection string is the most straight forward solution that does not require any additional enviornment variable setup."
  },
  {
    "objectID": "posts/2022-07-11-europython2022-summary.html",
    "href": "posts/2022-07-11-europython2022-summary.html",
    "title": "EuroPython 2022 - Conference Notes & Summary",
    "section": "",
    "text": "EuroPython2022\nSchedule: https://ep2022.europython.eu/schedule/\nSession that I attended: #europython"
  },
  {
    "objectID": "posts/2022-07-11-europython2022-summary.html#bulletproof-python-property-based-testing-with-hypothesis",
    "href": "posts/2022-07-11-europython2022-summary.html#bulletproof-python-property-based-testing-with-hypothesis",
    "title": "EuroPython 2022 - Conference Notes & Summary",
    "section": "Bulletproof Python – Property-Based Testing with Hypothesis",
    "text": "Bulletproof Python – Property-Based Testing with Hypothesis\nThe term property based testing isn’t too important. In a nutshell hypothesis is a python library that help you to write (better) tests by modifying your workflow.\n\nPrepare mock data Provide a specification of data, let hypothesis do the work\nPerform some operation\nAssert the result with expected value\n\nThe rationale behind this is\n\n\n\n\n\n\nNote\n\n\n\n** People write code don’t come up with good test. **\n\n\nFor example, you can generate integers with hypotesis.strategies.integers, it does something smart under the hood so it’s not just random number but more meaningful test. For example, you usually want to test for zero, negative number, positive number, large number. hypoethsis try to maximize the variety of tests and you just need to give it a specification.\nYou can also generate more sophisticated data, for example, a tuple of two integers, where the second integer has to be larger than the first one.\n@st.composite\n\ndef list_and_index(draw, elements=st.integers()):\n    first = draw(elements)\n    second = draw(st.integers(min_value=first + 1))\n    return (first, second)\nThink of it as your virtual QA buddy."
  },
  {
    "objectID": "posts/2022-07-11-europython2022-summary.html#tdd-development-with-pytest",
    "href": "posts/2022-07-11-europython2022-summary.html#tdd-development-with-pytest",
    "title": "EuroPython 2022 - Conference Notes & Summary",
    "section": "TDD Development with pytest",
    "text": "TDD Development with pytest\nWorkflow for TDD 1. Pick one bug/missing feature 2. Write a test that fails 3. Minimal amount of code that pass - (even hard coded!) 4. Refactor\nThere are good questions asked * In case of you don’t know what’s the expected answer, how do you write test that fails meaningfully?\nI jump out of the session because of a call, so not too many comments about this session. In general I like the idea of TDD but struggle to apply the textbook version of TDD as examples are often much simpler than the real application.\nFew key points * Tests as specification about your program (What it does and what not) * Understand why you test fail and pass. * Tests are also good source of documentation.\nThinking about test first also force you to think more about the design, you almost start from pseudocode (you function doesn’t even exist!)."
  },
  {
    "objectID": "posts/2022-07-11-europython2022-summary.html#introduction-to-apache-tvm",
    "href": "posts/2022-07-11-europython2022-summary.html#introduction-to-apache-tvm",
    "title": "EuroPython 2022 - Conference Notes & Summary",
    "section": "Introduction to Apache TVM",
    "text": "Introduction to Apache TVM\n\n\n\nApache TVM\n\n\n\nApache TVM is a framework that try to squeeze extra performance from specialized hardware.\n\nIn practice, the workflow roughly go like this 1. Trained a model with your favored libraries (PyTorch/Tensorflow etc) 2. Use TVM to compile and tune -&gt; After this you get a compiled module as output 3. Use TVM python API for inference\nThe performance gains are mainly from hardware architecture that can give better performance, TVM did some architecture search and try to find the optimal one.\n\n\n\n\n\n\nNote\n\n\n\nMaybe one side benefit of this is it does not need the deep learning pipeline dependecenies since you just need the TVM Python API and the model file for inference."
  },
  {
    "objectID": "posts/2022-07-11-europython2022-summary.html#how-many-modules-imported-in-python-by-default",
    "href": "posts/2022-07-11-europython2022-summary.html#how-many-modules-imported-in-python-by-default",
    "title": "EuroPython 2022 - Conference Notes & Summary",
    "section": "How many modules imported in Python by default?",
    "text": "How many modules imported in Python by default?\n\nPython Shell - 79\nIPython - 646!\nJupyter - 1020!!!\n\nIt’s quite surprising how many libraries are imported by default, and this explains why it takes some time whenever you do ipython on a shell, as the Python Interpreter is busy reading all the files and evalute it.\nSome other interesting notes: * Python use a Finder and Loader to import modules * sys.path is the order that Python Interpreter search for modules, and the first match wins (This is important if you have duplicate namespace or if you do namespace package) * Don’t do sys.path.append although you will find this very common if you do a Stackoverflow search, use environment variable PYTHONPATH=some_path instead"
  },
  {
    "objectID": "posts/2021-05-29-python-internal-series-python-gil-and-memory.html",
    "href": "posts/2021-05-29-python-internal-series-python-gil-and-memory.html",
    "title": "Python Internal Series - Global Interpreter Lock (GIL) and Memory Management",
    "section": "",
    "text": "Most people first learn about GIL because of how it slows down Python program and prevent multi-threading running efficiently, however, the GIL is one of the reason why Python survive 30 years and still growing healthyly.\nGIL is nothing like the stereotype people think, legacy, slow. There are multiple benefits GIL provide:\n\nIt speed ups single thread program.\nIt is compatible with many C Program thanks to the C API of CPysthon."
  },
  {
    "objectID": "posts/2021-05-29-python-internal-series-python-gil-and-memory.html#is-gil-a-bad-design",
    "href": "posts/2021-05-29-python-internal-series-python-gil-and-memory.html#is-gil-a-bad-design",
    "title": "Python Internal Series - Global Interpreter Lock (GIL) and Memory Management",
    "section": "",
    "text": "Most people first learn about GIL because of how it slows down Python program and prevent multi-threading running efficiently, however, the GIL is one of the reason why Python survive 30 years and still growing healthyly.\nGIL is nothing like the stereotype people think, legacy, slow. There are multiple benefits GIL provide:\n\nIt speed ups single thread program.\nIt is compatible with many C Program thanks to the C API of CPysthon."
  },
  {
    "objectID": "posts/2021-05-29-python-internal-series-python-gil-and-memory.html#global-interpreter-lock-a.k.a-mutex-lock",
    "href": "posts/2021-05-29-python-internal-series-python-gil-and-memory.html#global-interpreter-lock-a.k.a-mutex-lock",
    "title": "Python Internal Series - Global Interpreter Lock (GIL) and Memory Management",
    "section": "Global Interpreter Lock a.k.a Mutex Lock",
    "text": "Global Interpreter Lock a.k.a Mutex Lock\nTo start with, GIL is a mutex lock."
  },
  {
    "objectID": "posts/2021-05-29-python-internal-series-python-gil-and-memory.html#why-gil-is-needed-in-the-first-place",
    "href": "posts/2021-05-29-python-internal-series-python-gil-and-memory.html#why-gil-is-needed-in-the-first-place",
    "title": "Python Internal Series - Global Interpreter Lock (GIL) and Memory Management",
    "section": "Why GIL is needed in the first place?",
    "text": "Why GIL is needed in the first place?\nMemory management. Python use something called “reference counting”, which make it different from many modern programming lanaguage. It is what allow Python programmer to lay back and let Python take care when to release memory. Precisely, it is actually the C program controlling the memory life cycle for Python (Cpython). Cpython is known as the default Python interpreter. It first compiles Python to intermediate bytecode (.pyc files). These bytecode then being interpreted by a virtual machine ane executed. It is worth to mention that other variants of Python exist, i.e. IronPython(C#), Jython(Java), Pypy(Python) and they have different memory management mechanisms.\n\nPython Memory Management - Reference Count & Garbage Collection (gc)\n\nimport sys\n\n\nsys.getrefcount(a)\n\n3\n\n\nReference counting is a simple idea. The intuition is that if a particular object is not referenced by anything, it can be recycled since it will not be used anymore.\nFor example, the list [1] is now referenced by the variable a, so the reference count is incremented by 1.\n\nimport sys\na = [1]\nsys.getrefcount(a)\n\n2\n\n\nNote that the reference count is 2 instead of 1. 1. The first reference is a = [1] 2. When the variable a is passed to sys.getrefcount(a) as an argument, it also increases the reference count.\n\ndel a\n\nWhen del a is called, the list [1] have 0 reference count, and it is collected by Python automatically behind the scene.\n\n\nLock & Deadlock\n\n\nMemory Management"
  },
  {
    "objectID": "posts/2021-06-20-logging-config-dict-issue-kedro.html",
    "href": "posts/2021-06-20-logging-config-dict-issue-kedro.html",
    "title": "A logging.config.dictConfig() issue in python",
    "section": "",
    "text": "import logging\nfrom clearml import Task\nconf_logging = {\"version\":1, \n                \"formatters\":{\n                      \"simple\":{\n                             \"format\":\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"}\n                      }\n                  }\nt = Task.init(project_name=\"test\")\nlogging.config.dictConfig(conf_logging)\nlogging.info(\"INFO!\")\nlogging.debug(\"DEBUG!\")\nlogging.warning(\"WARN!\")\nprint(\"PRINT!\")\nWith this code block, you will find no print() or logging is sent to ClearML logging Console. Turns out kedro use logging.config.dictConfig(conf_logging) as the default and causing this issue.\nA quick fix is to add \"incremental\": True in the config dict. In the standard documentation, the default is False, which means the configuration will replace existing one, thus removing the clearml handlers, and causing the issue I had.\nconf_logging = {\"version\":1, \n                \"incremental\": True\n                \"formatters\":{\n                      \"simple\":{\n                             \"format\":\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"}\n                      }\n                  }"
  },
  {
    "objectID": "posts/advance_pytest/2023-11-15-advance-pytest-fixtures-with-params.html",
    "href": "posts/advance_pytest/2023-11-15-advance-pytest-fixtures-with-params.html",
    "title": "Advance Pytest - parameterize your fixture",
    "section": "",
    "text": "Today I encountered a little testing challenge where I need to parameterize my tests. We use pytest extensively for our unit tests. Particularly, I need to duplicate all my tests for a new folder structure. At first, I thought about just writing new tests, then I quickly realise that I will end up duplicating a big chunk of our tests. On the other hand, some tests only make sense for a particular structure. If you are familiar with pytest, you may know that you can use pytest.mark.parametrize for testing a matrix of inputs and outputs. This is not applicable because I want to parameterise my fixture instead of the parameters of a test.\nAfter a while of Googling, I found Parameterize fixtures. It wasn’t immediately obivous to me how I can use this to apply a matrix of tests while keeping it flexible enough to use only part of the fixtures. It sounds complicated but it will make more sense as I show you more example\n\n\nimport pytest\n\n@pytest.fixture\ndef a():\n    ...\n\n\ndef test_foo(a):\n    ...\nIf you have use pytest before, you will know that pytest.fixture is the recommended way to reuse test setup. To test a matrix of input and outputs, you may use pytest.mark.parameterize.\nimport pytest\n\n@pytest.fixture\ndef a():\n    ...\n\npytest.mark.parametrize(\"input\", [1,2,3])\ndef test_foo(input):\n    ...\nThis will create 3 tests and each of them will have an input 1, 2, 3 respectively. However this is not applicable to my use case because I want to parameterize my fixtures instead of a test. The test requires a fairly complicated setup, which involves creating dummy files and folder. This wasn’t a problem before we only create one specific type of structure. Recently, we want to make it generic and support different types of structure, so I need to expand the tests to cover this.\nTo do this, I discovered that I can use pytest.fixture with a params argument.\n@pytest.fixture(params=[\"structure_a\", \"structure_b\",\"structure_c\"])\ndef a(request):\n    return request.param\n\n\ndef test_a(a):\n   ...\nThis also creates 3 tests, which is great! Now for most of the cases, this is fine, but some tests only make sense for structure_a but not the other, should I duplicate another set of fixture? This is a feasible option but it’s not ideal. Turns out I can reuse the setup logic and create a combination of fixture easily. There are two different styles to do this, essentially instead of creating the fixture directly, we keep it as a function and create the a combination of fixtures base on this setup function.\ndef setup(params):  # Note this is just a function but not a fixture\n    ...\n\n\nuse_all_folder_structure = pytest.fixture(setup, params=[\"a\",\"b\",\"c\"]), name=\"use_all_folder_structure\")\ndef test_foo_a(use_all_folder_structure):\n    print()\n\n\n\n\n_ = pytest.fixture(setup, params= [\"a\",\"b\",\"c\"]) ,name=\"use_all_folder_structure\")\nuse_all_folder_structure = pytest.mark.usefixtures(\"use_all_folder_structure\",\n\n)\n@use_all_folder_structure\ndef test_foo_b():\n    print()\nI can easily create another fixture to run on a subset of fixture.\n_ = pytest.fixture(setup, params= [\"a\"]) ,name=\"use_folder_structure_a\")\n\n@use_folder_structure_a\ndef test_bar_b():\n    print()"
  },
  {
    "objectID": "posts/advance_pytest/2023-11-15-advance-pytest-fixtures-with-params.html#parameterize-tests",
    "href": "posts/advance_pytest/2023-11-15-advance-pytest-fixtures-with-params.html#parameterize-tests",
    "title": "Advance Pytest - parameterize your fixture",
    "section": "",
    "text": "import pytest\n\n@pytest.fixture\ndef a():\n    ...\n\n\ndef test_foo(a):\n    ...\nIf you have use pytest before, you will know that pytest.fixture is the recommended way to reuse test setup. To test a matrix of input and outputs, you may use pytest.mark.parameterize.\nimport pytest\n\n@pytest.fixture\ndef a():\n    ...\n\npytest.mark.parametrize(\"input\", [1,2,3])\ndef test_foo(input):\n    ...\nThis will create 3 tests and each of them will have an input 1, 2, 3 respectively. However this is not applicable to my use case because I want to parameterize my fixtures instead of a test. The test requires a fairly complicated setup, which involves creating dummy files and folder. This wasn’t a problem before we only create one specific type of structure. Recently, we want to make it generic and support different types of structure, so I need to expand the tests to cover this.\nTo do this, I discovered that I can use pytest.fixture with a params argument.\n@pytest.fixture(params=[\"structure_a\", \"structure_b\",\"structure_c\"])\ndef a(request):\n    return request.param\n\n\ndef test_a(a):\n   ...\nThis also creates 3 tests, which is great! Now for most of the cases, this is fine, but some tests only make sense for structure_a but not the other, should I duplicate another set of fixture? This is a feasible option but it’s not ideal. Turns out I can reuse the setup logic and create a combination of fixture easily. There are two different styles to do this, essentially instead of creating the fixture directly, we keep it as a function and create the a combination of fixtures base on this setup function.\ndef setup(params):  # Note this is just a function but not a fixture\n    ...\n\n\nuse_all_folder_structure = pytest.fixture(setup, params=[\"a\",\"b\",\"c\"]), name=\"use_all_folder_structure\")\ndef test_foo_a(use_all_folder_structure):\n    print()\n\n\n\n\n_ = pytest.fixture(setup, params= [\"a\",\"b\",\"c\"]) ,name=\"use_all_folder_structure\")\nuse_all_folder_structure = pytest.mark.usefixtures(\"use_all_folder_structure\",\n\n)\n@use_all_folder_structure\ndef test_foo_b():\n    print()\nI can easily create another fixture to run on a subset of fixture.\n_ = pytest.fixture(setup, params= [\"a\"]) ,name=\"use_folder_structure_a\")\n\n@use_folder_structure_a\ndef test_bar_b():\n    print()"
  },
  {
    "objectID": "posts/2021-03-17-pytest-data-test-truncated-error.html",
    "href": "posts/2021-03-17-pytest-data-test-truncated-error.html",
    "title": "Data Test as CI",
    "section": "",
    "text": "I am running test with great_expectations that validate data in UAT and production server with CI, so it would be nice if the log can capture this.\nI created a custom error class that would do the job, however, pytest truncated my AssertionError since it is quite long.\nI am using pytest magic from https://github.com/akaihola/ipython_pytest which allow me to run pytest in a Jupyter notebook cell.\nIt is quite simple with a few tens of lines.\n\n%%writefile ipython_pytest.py\nimport os\nimport shlex\nimport sys\nfrom pathlib import Path\n\nimport tempfile\nfrom IPython.core import magic\nfrom pytest import main as pytest_main\n\n\nTEST_MODULE_NAME = '_ipytesttmp'\n\ndef pytest(line, cell):\n    with tempfile.TemporaryDirectory() as root:\n        oldcwd = os.getcwd()\n        os.chdir(root)\n        tests_module_path = '{}.py'.format(TEST_MODULE_NAME)\n        try:\n            Path(tests_module_path).write_text(cell)\n            args = shlex.split(line)\n            os.environ['COLUMNS'] = '80'\n            pytest_main(args + [tests_module_path])\n            if TEST_MODULE_NAME in sys.modules:\n                del sys.modules[TEST_MODULE_NAME]\n        finally:\n            os.chdir(oldcwd)\n\ndef load_ipython_extension(ipython):\n    magic.register_cell_magic(pytest)\n\nWriting ipython_pytest.py\n\n\n\n# !pip install pytest\n%load_ext ipython_pytest\n\nThe ipython_pytest extension is already loaded. To reload it, use:\n  %reload_ext ipython_pytest\n\n\n\n%%pytest\n\ndef test_long_assertion_error():\n    x = \"placeholder\"\n    expect = \"abcdefg\\n\"*20 # Long string\n    assert x == expect\n\n============================= test session starts =============================\nplatform win32 -- Python 3.8.3, pytest-6.2.2, py-1.10.0, pluggy-0.13.1\nrootdir: C:\\Users\\channo\\AppData\\Local\\Temp\\tmpohw9e_9w\ncollected 1 item\n\n_ipytesttmp.py F                                                         [100%]\n\n================================== FAILURES ===================================\n__________________________ test_long_assertion_error __________________________\n\n    def test_long_assertion_error():\n        x = \"placeholder\"\n        expect = \"abcdefg\\n\"*20 # Long string\n&gt;       assert x == expect\nE       AssertionError: assert 'placeholder' == 'abcdefg\\nabc...fg\\nabcdefg\\n'\nE         + placeholder\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg...\nE         \nE         ...Full output truncated (15 lines hidden), use '-vv' to show\n\n_ipytesttmp.py:5: AssertionError\n=========================== short test summary info ===========================\nFAILED _ipytesttmp.py::test_long_assertion_error - AssertionError: assert 'pl...\n============================== 1 failed in 0.06s ==============================\n\n\nYou can see that pytest truncated my error with ... Here is how I solve ths issue\n\n%%pytest -vv\n\ndef test_long_assertion_error():\n    x = \"placeholder\"\n    expect = \"abcdefg\\n\"*20 # Long string\n    assert x == expect\n\n============================= test session starts =============================\nplatform win32 -- Python 3.8.3, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- c:\\programdata\\miniconda3\\python.exe\ncachedir: .pytest_cache\nrootdir: C:\\Users\\channo\\AppData\\Local\\Temp\\tmpyic4vcra\ncollecting ... collected 1 item\n\n_ipytesttmp.py::test_long_assertion_error FAILED                         [100%]\n\n================================== FAILURES ===================================\n__________________________ test_long_assertion_error __________________________\n\n    def test_long_assertion_error():\n        x = \"placeholder\"\n        expect = \"abcdefg\\n\"*20 # Long string\n&gt;       assert x == expect\nE       AssertionError: assert 'placeholder' == ('abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n')\nE         + placeholder\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\n\n_ipytesttmp.py:5: AssertionError\n=========================== short test summary info ===========================\nFAILED _ipytesttmp.py::test_long_assertion_error - AssertionError: assert 'pl...\n============================== 1 failed in 0.06s =============================="
  },
  {
    "objectID": "posts/2021-07-10-5minutes-data-science-design-pattern-callback.html",
    "href": "posts/2021-07-10-5minutes-data-science-design-pattern-callback.html",
    "title": "5 Minutes Data Science Design Patterns I - Callback",
    "section": "",
    "text": "Note\n\n\n\nThese series are written as a quick introduction to software design for data scientists, something that is lightweight than the Design Pattern Bible - Clean Code I wish exists when I first started to learn. Design patterns refer to reusable solutions to some common problems, and some happen to be useful for data science. There is a good chance that someone else has solved your problem before. When used wisely, it helps to reduce the complexity of your code."
  },
  {
    "objectID": "posts/2021-07-10-5minutes-data-science-design-pattern-callback.html#so-what-is-callback-after-all",
    "href": "posts/2021-07-10-5minutes-data-science-design-pattern-callback.html#so-what-is-callback-after-all",
    "title": "5 Minutes Data Science Design Patterns I - Callback",
    "section": "So, What is Callback after all?",
    "text": "So, What is Callback after all?\nCallback function, or call after, simply means a function will be called after another function. It is a piece of executable code (function) that passed as an argument to another function. [1]\n\ndef foo(x, callback=None):\n    print('foo!')\n    if callback:\n        callback(x)\n\n\nfoo('123')\n\nfoo!\n\n\n\nfoo('123', print)\n\nfoo!\n123\n\n\nHere I pass the function print as a callback, hence the string 123 get printed after foo!."
  },
  {
    "objectID": "posts/2021-07-10-5minutes-data-science-design-pattern-callback.html#why-do-i-need-to-use-callback",
    "href": "posts/2021-07-10-5minutes-data-science-design-pattern-callback.html#why-do-i-need-to-use-callback",
    "title": "5 Minutes Data Science Design Patterns I - Callback",
    "section": "Why do I need to use Callback?",
    "text": "Why do I need to use Callback?\nCallback is very common in high-level deep learning libraries, most likely you will find them in the training loop. * fastai - fastai provide high-level API for PyTorch * Keras - the high-level API for Tensorflow * ignite - they use event & handler, which provides more flexibility in their opinion\n\nimport numpy as np\n\n# A boring training Loop\ndef train(x):\n    n_epochs = 3\n    n_batches = 2\n    loss = 20\n\n    for epoch in range(n_epochs):\n        for batch in range(n_batches):\n            loss = loss - 1  # Pretend we are training the model\n\n\nx = np.ones(10)\ntrain(x);\n\nSo, let’s say you now want to print the loss at the end of an epoch. You can just add 1 lines of code.\n\nThe simple approach\n\ndef train_with_print(x):\n    n_epochs = 3\n    n_batches = 2\n    loss = 20\n\n    for epoch in range(n_epochs):\n        for batch in range(n_batches):\n            loss = loss - 1 # Pretend we are training the model\n        print(f'End of Epoch. Epoch: {epoch}, Loss: {loss}')\n    return loss\n\n\ntrain_with_print(x);\n\nEnd of Epoch. Epoch: 0, Loss: 18\nEnd of Epoch. Epoch: 1, Loss: 16\nEnd of Epoch. Epoch: 2, Loss: 14\n\n\n\n\nCallback approach\nOr you call add a PrintCallback, which does the same thing but with a bit more code.\n\nclass Callback:\n    def on_epoch_start(self, x):\n        pass\n\n    def on_epoch_end(self, x):\n        pass\n\n    def on_batch_start(self, x):\n        pass\n\n    def on_batch_end(self, x):\n        pass\n\n\nclass PrintCallback(Callback):\n    def on_epoch_end(self, x):\n        print(f'End of Epoch. Loss: {x}')\n\n\ndef train_with_callback(x, callback=None):\n    n_epochs = 3\n    n_batches = 2\n    loss = 20\n\n    for epoch in range(n_epochs):\n\n        callback.on_epoch_start(loss)\n\n        for batch in range(n_batches):\n            callback.on_batch_start(loss)\n            loss = loss - 1  # Pretend we are training the model\n            callback.on_batch_end(loss)\n\n        callback.on_epoch_end(loss)\n\n\ntrain_with_callback(x, callback=PrintCallback());\n\nEnd of Epoch. Loss: 18\nEnd of Epoch. Loss: 16\nEnd of Epoch. Loss: 14\n\n\nUsually, a callback defines a few particular events on_xxx_xxx, which indicate that the function will be executed according to the corresponding condition. So all callbacks will inherit the base class Callback, and override the desired function, here we only implemented the on_epoch_end method because we only want to show the loss at the end.\nIt may seem awkward to write so many more code to do one simple thing, but there are good reasons. Consider now you need to add more features, how would you do it?\n\nModelCheckpoint\nEarly Stopping\nLearningRateScheduler\n\nYou can just add code in the loop, but it will start growing into a really giant function. It is impossible to test this function because it does 10 things at the same time. In addition, the extra code may not even be related to the training logic, they are just there to save the model or plot a chart. So, it is best to separate the logic. A function should only do 1 thing according to the Single Responsibility Principle. It helps you to reduce the complexity as it provides a nice abstraction, you are only modifying code within the specific callback you are interested.\n\n\nAdd some more sauce!\nWhen using the Callback Pattern, I can just implement a few more classes and the training loop is barely touched. Here we introduce a new class Callbacks because we need to execute more than 1 callback, it is used for holding all callbacks and executed them sequentially.\n\nclass Callbacks:\n    \"\"\"\n    It is the container for callback\n    \"\"\"\n\n    def __init__(self, callbacks):\n        self.callbacks = callbacks\n\n    def on_epoch_start(self, x):\n        for callback in self.callbacks:\n            callback.on_epoch_start(x)\n\n    def on_epoch_end(self, x):\n        for callback in self.callbacks:\n            callback.on_epoch_end(x)\n\n    def on_batch_start(self, x):\n        for callback in self.callbacks:\n            callback.on_batch_start(x)\n\n    def on_batch_end(self, x):\n        for callback in self.callbacks:\n            callback.on_batch_end(x)\n\nThen we implement the new Callback one by one, here we only have the pseudocode, but you should get the gist. For example, we only need to save the model at the end of an epoch, thus we implement the method on_epoch_end with a ModelCheckPoint callback.\n\nclass PrintCallback(Callback):\n    def on_epoch_end(self, x):\n        print(f'[{type(self).__name__}]: End of Epoch. Loss: {x}')\n\n\nclass ModelCheckPoint(Callback):\n    def on_epoch_end(self, x):\n        print(f'[{type(self).__name__}]: Save Model')\n\n\nclass EarlyStoppingCallback(Callback):\n    def on_epoch_end(self, x):\n        if x &lt; 16:\n            print(f'[{type(self).__name__}]: Early Stopped')\n\n\nclass LearningRateScheduler(Callback):\n    def on_batch_end(self, x):\n        print(f'    [{type(self).__name__}]: Reduce learning rate')\n\nAnd we also modify the training loop a bit, the argument now takes a Callbacks which contain zero to many callbacks.\n\ndef train_with_callbacks(x, callbacks=None):\n    n_epochs = 2\n    n_batches = 3\n    loss = 20\n\n    for epoch in range(n_epochs):\n\n        callbacks.on_epoch_start(loss)                             # on_epoch_start\n        for batch in range(n_batches):\n            callbacks.on_batch_start(loss)                         # on_batch_start\n            loss = loss - 1  # Pretend we are training the model\n            callbacks.on_batch_end(loss)                           # on_batch_end\n        callbacks.on_epoch_end(loss)                               # on_epoch_end\n\n\ncallbacks = Callbacks([PrintCallback(), ModelCheckPoint(),\n                      EarlyStoppingCallback(), LearningRateScheduler()])\ntrain_with_callbacks(x, callbacks=callbacks)\n\n    [LearningRateScheduler]: Reduce learning rate\n    [LearningRateScheduler]: Reduce learning rate\n    [LearningRateScheduler]: Reduce learning rate\n[PrintCallback]: End of Epoch. Loss: 17\n[ModelCheckPoint]: Save Model\n    [LearningRateScheduler]: Reduce learning rate\n    [LearningRateScheduler]: Reduce learning rate\n    [LearningRateScheduler]: Reduce learning rate\n[PrintCallback]: End of Epoch. Loss: 14\n[ModelCheckPoint]: Save Model\n[EarlyStoppingCallback]: Early Stopped\n\n\nHopefully, it convinces you Callback makes the code cleaner and easier to maintain. If you just use plain if-else statements, you may end up with a big chunk of if-else clauses.\n\nfastai - fastai provide high-level API for PyTorch\nKeras - the high-level API for Tensorflow\nignite - they use event & handler, which provides more flexibility in their opinion"
  },
  {
    "objectID": "posts/2021-07-10-5minutes-data-science-design-pattern-callback.html#reference",
    "href": "posts/2021-07-10-5minutes-data-science-design-pattern-callback.html#reference",
    "title": "5 Minutes Data Science Design Patterns I - Callback",
    "section": "Reference",
    "text": "Reference\n\nhttps://stackoverflow.com/questions/824234/what-is-a-callback-function"
  },
  {
    "objectID": "posts/2020-02-09-MixUp-and-Beta-Distribution.html",
    "href": "posts/2020-02-09-MixUp-and-Beta-Distribution.html",
    "title": "data augmentation - Understand MixUp and Beta Distribution",
    "section": "",
    "text": "GitHub: https://github.com/noklam/notadatascientist/tree/master/demo/mixup-beta"
  },
  {
    "objectID": "posts/2020-02-09-MixUp-and-Beta-Distribution.html#beta-distribution",
    "href": "posts/2020-02-09-MixUp-and-Beta-Distribution.html#beta-distribution",
    "title": "data augmentation - Understand MixUp and Beta Distribution",
    "section": "Beta Distribution",
    "text": "Beta Distribution\nBeta distribution is control by two parameters, α and β with interval [0, 1], which make it useful for Mixup. Mixup is basically a superposition of two image with a parameter t. Instead of using a dog image, with Mixup, you may end up have a image which is 0.7 dog + 0.3 cat\nTo get some sense of what a beta distribution is, let plot beta distribution with different alpha and beta to see its effect\nimport math\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch import tensor\n# PyTorch has a log-gamma but not a gamma, so we'll create one\nΓ = lambda x: x.lgamma().exp()\nfacts = [math.factorial(i) for i in range(7)]\n\nplt.plot(range(7), facts, 'ro')\nplt.plot(torch.linspace(0,6), Γ(torch.linspace(0,6)+1))\nplt.legend(['factorial','Γ']);\n\n\n\npng"
  },
  {
    "objectID": "posts/kedro_config_loader/2023-11-16-kedro-config-loader-dive-deep.html",
    "href": "posts/kedro_config_loader/2023-11-16-kedro-config-loader-dive-deep.html",
    "title": "How Kedro Config Loader works",
    "section": "",
    "text": "Kedro offers a configuration system, it involves concepts such as - environment (it can be multiple files) - variable interpolation (advance templating) - globals - custom resolvers - runtime parameters (override via CLI or programatically)\nThe official documentation has plenty of examples, so I am going to focus on explaining the internals.\n\nConfig Resolution (Without Globals)\nTo enable all of the features, the resolution order is actually quite complicated. It involes multiple stages of merging in different scope. This diagram belows describe what happen under the hood. The key is identify two things 1. Some feature apply in local scope (it is not cross environment) 2. There are different merging strategy, some are using OmegaConf, some are pure dictionary merge 3. Configuration are resolved and then merge across environments. i.e. you cannot interpolate a value from another environment.\n\n\n\nimage.png\n\n\n\n\nConfig Resolution (With Globals)"
  },
  {
    "objectID": "posts/2019-01-01-codespace-template.html",
    "href": "posts/2019-01-01-codespace-template.html",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "",
    "text": "Today, we are going to show you how to set up a literate programming environment, allowing you to use an IDE (VS Code) and an interactive computing environment (Jupyter), without leaving your browser, for free, in under 5 minutes. You’ll even see how VSCode and Jupyter work together automatically! But first, what is literate programming? And how did I go from skeptic to a zealot of literate programming?"
  },
  {
    "objectID": "posts/2019-01-01-codespace-template.html#introduction",
    "href": "posts/2019-01-01-codespace-template.html#introduction",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "Introduction",
    "text": "Introduction\n\nLiterate programming is a programming paradigm introduced by Donald Knuth in which a computer program is given an explanation of its logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which compilable source code can be generated. According to Knuth, literate programming provides higher-quality programs by forcing programmers to explicitly state the thoughts behind the program. This process makes poorly thought-out design decisions more obvious. Knuth also claims that literate programming provides a first-rate documentation system, which is not an add-on, but is grown naturally in the process of exposition of one’s thoughts during a program’s creation. 1\n\nWhen I first learned about literate programming, I was quite skeptical. For the longest time, I had wrongly equated Jupyter notebooks with literate programming. Indeed, Jupyter is a brilliant interactive computing system, which was awarded the Association of Computing Machinery (ACM) Software System Award, and is loved by many developers. However, Jupyter falls short of the literate programming paradigm for the following reasons:2\n\nIt can be difficult to compile source code from notebooks.\nIt can be difficult to diff and use version control with notebooks because they are not stored in plain text.\nIt is not clear how to automatically generate documentation from notebooks.\nIt is not clear how to properly run tests suites when writing code in notebooks.\n\nMy skepticism quickly evaporated when I began using nbdev, a project that extends notebooks to complete the literate programming ideal. I spent a month, full time, using nbdev while contributing to the python library fastcore, and can report that Donald Knuth was definitely onto something. The process of writing prose and tests alongside code forced me to deeply understand why the code does what it does, and to think deeply about its design. Furthermore, the reduced cognitive load and speed of iteration of having documentation, code, and tests in one location boosted my productivity to levels I have never before experienced as a software developer. Furthermore, I found that developing this way bolstered collaboration such that code reviews not only happened faster but were more meaningful. In short, nbdev may be the most profound productivity tool I have ever used.\nAs a teaser, look how easy it is to instantiate this literate programming environment, which includes a notebook, a docs site and an IDE with all dependencies pre-installed! :point_down:\n{% include video.html url=“https://github.com/machine-learning-apps/demo-videos/raw/master/codespaces-nbdev/e2e_small.mp4” %}"
  },
  {
    "objectID": "posts/2019-01-01-codespace-template.html#features-of-nbdev",
    "href": "posts/2019-01-01-codespace-template.html#features-of-nbdev",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "Features of nbdev",
    "text": "Features of nbdev\nAs discussed in the docs, nbdev provides the following features:\n\nSearchable, hyperlinked documentation, which can be automatically hosted on GitHub Pages for free.\nPython modules, following best practices such as automatically defining __all__ with your exported functions, classes, and variables.\nPip and Conda installers.\nTests defined directly in notebooks which run in parallel. This testing system has been thoroughly tested with GitHub Actions.\nNavigate and edit your code in a standard text editor or IDE, and export any changes automatically back into your notebooks.\n\nSince you are in a notebook, you can also add charts, text, links, images, videos, etc, that are included automatically in the documentation of your library, along with standardized documentation generated automatically from your code. This site is an example of docs generated automatically by nbdev."
  },
  {
    "objectID": "posts/2019-01-01-codespace-template.html#github-codespaces",
    "href": "posts/2019-01-01-codespace-template.html#github-codespaces",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "GitHub Codespaces",
    "text": "GitHub Codespaces\nThanks to Conda and nbdev_template, setting up a development environment with nbdev is far easier than it used to be. However, we realized it could be even easier, thanks to a new GitHub product called Codespaces. Codespaces is a fully functional development environment in your browser, accessible directly from GitHub, that provides the following features:\n\nA full VS Code IDE.\nAn environment that has files from the repository mounted into the environment, along with your GitHub credentials.\nA development environment with dependencies pre-installed, backed by Docker.\nThe ability to serve additional applications on arbitrary ports. For nbdev, we serve a Jupyter notebook server as well as a Jekyll based documentation site.\nA shared file system, which facilitates editing code in one browser tab and rendering the results in another.\n… and more.\n\nCodespaces enables developers to immediately participate in a project without wasting time on DevOps or complicated setup steps. Most importantly, CodeSpaces with nbdev allows developers to quickly get started with creating their own software with literate programming."
  },
  {
    "objectID": "posts/2019-01-01-codespace-template.html#a-demo-of-nbdev-codespaces",
    "href": "posts/2019-01-01-codespace-template.html#a-demo-of-nbdev-codespaces",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "A demo of nbdev + Codespaces",
    "text": "A demo of nbdev + Codespaces\nThis demo uses the project fastai/fastcore, which was built with nbdev, as an example. First, we can navigate to this repo and launch a Codespace:\n{% include video.html url=“https://github.com/machine-learning-apps/demo-videos/raw/master/codespaces-nbdev/1_open.mp4” %}\n\n\n\nIf you are launching a fresh Codespace, it may take several minutes to set up. Once the environment is ready, we can verify that all dependencies we want are installed (in this case fastcore and nbdev):\n{% include video.html url=“https://github.com/machine-learning-apps/demo-videos/raw/master/codespaces-nbdev/2_verify.mp4” %}\n\n\n\nAdditionally, we can serve an arbitrary number of applications on user-specified ports, which we can open through VSCode as shown below:\n{% include video.html url=“https://github.com/machine-learning-apps/demo-videos/raw/master/codespaces-nbdev/3_nb_small.mp4” %}\n\n\n\nIn this case, these applications are a notebook and docs site. Changes to a notebook are reflected immediately in the data docs. Furthermore, we can use the cli command nbdev_build_lib to sync our notebooks with python modules. This functionality is shown below:\n{% include video.html url=“https://github.com/machine-learning-apps/demo-videos/raw/master/codespaces-nbdev/4_reload_small.mp4” %}\n\n\n\nThis is amazing! With a click of a button, I was able to:\n\nLaunch an IDE with all dependencies pre-installed.\nLaunch two additional applications: a Jupyter Notebook server on port 8080 and a docs site on port 4000.\nAutomatically update the docs and modules every time I make a change to a Jupyter notebook.\n\nThis is just the tip of the iceberg. There are additional utilities for writing and executing tests, diffing notebooks, special flags for hiding, showing, and collapsing cells in the generated docs, as well as git hooks for automation. This and more functionality is covered in the nbdev docs."
  },
  {
    "objectID": "posts/2019-01-01-codespace-template.html#give-it-a-try-for-yourself",
    "href": "posts/2019-01-01-codespace-template.html#give-it-a-try-for-yourself",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "Give It A Try For Yourself",
    "text": "Give It A Try For Yourself\nTo try out nbdev yourself, take this tutorial, which will walk you through everything you need to know. The tutorial also shows you how to use a repository template with the configuration files necessary to enable Codespaces with nbdev."
  },
  {
    "objectID": "posts/2019-01-01-codespace-template.html#you-can-write-blogs-with-notebooks-too",
    "href": "posts/2019-01-01-codespace-template.html#you-can-write-blogs-with-notebooks-too",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "You Can Write Blogs With Notebooks, Too!",
    "text": "You Can Write Blogs With Notebooks, Too!\nThis blog post was written in fastpages which is also built on nbdev! We recommend fastpages if you want an easy way to blog with Jupyter notebooks."
  },
  {
    "objectID": "posts/2019-01-01-codespace-template.html#additional-resources",
    "href": "posts/2019-01-01-codespace-template.html#additional-resources",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nThe GitHub Codepaces site.\nThe official docs for Codespaces.\nThe nbdev docs.\nThe nbdev GitHub repo.\nfastpages: The project used to write this blog.\nThe GitHub repo fastai/fastcore, which is what we used in this blog post as an example."
  },
  {
    "objectID": "posts/2019-01-01-codespace-template.html#footnotes",
    "href": "posts/2019-01-01-codespace-template.html#footnotes",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWikipedia article: Literate Programming↩︎\nThis is not a criticism of Jupyter. Jupyter doesn’t claim to be a full literate programming system. However, people can sometimes (unfairly) judge Jupyter according to this criteria.↩︎"
  },
  {
    "objectID": "posts/default_node_name/2024-02-08-default-node-name.html",
    "href": "posts/default_node_name/2024-02-08-default-node-name.html",
    "title": "Investigation of the Kedro default node names",
    "section": "",
    "text": "Default node names are problematic #3575\n!kedro -V\n\nkedro, version 0.18.14\n%load_ext kedro.ipython\n%reload_kedro default-node-name/\n\n[02/08/24 15:59:00] WARNING  Kedro extension was registered but couldn't find a Kedro project. Make  __init__.py:40\n                             sure you run '%reload_kedro &lt;project_root&gt;'.                                          \n\n\n\n[02/08/24 15:59:00] INFO     Kedro project default-node-name                                        __init__.py:108\n\n\n\n                    INFO     Defined global variable 'context', 'session', 'catalog' and            __init__.py:109\n                             'pipelines'                                                                           \n\n\n\n[02/08/24 15:59:08] INFO     Registered line magic 'run_viz'                                        __init__.py:115"
  },
  {
    "objectID": "posts/default_node_name/2024-02-08-default-node-name.html#node.name-with-namespace",
    "href": "posts/default_node_name/2024-02-08-default-node-name.html#node.name-with-namespace",
    "title": "Investigation of the Kedro default node names",
    "section": "node.name (with namespace)",
    "text": "node.name (with namespace)\n\nNeeded, node, pipeline, runner (expected to be the public interface)\nit’s using str(self)\n\n@property\ndef name(self) -&gt; str:\n    \"\"\"Node's name.\n\n    Returns:\n        Node's name if provided or the name of its function.\n    \"\"\"\n    node_name = self._name or str(self)\n    if self.namespace:\n        return f\"{self.namespace}.{node_name}\"\n    return node_name"
  },
  {
    "objectID": "posts/default_node_name/2024-02-08-default-node-name.html#node.short_name",
    "href": "posts/default_node_name/2024-02-08-default-node-name.html#node.short_name",
    "title": "Investigation of the Kedro default node names",
    "section": "node.short_name",
    "text": "node.short_name\n\nNot needed for kedro run\nNo reference in the entire codebase\ndeprecated will be a breaking change (technically) ## node._name\nOnly usage in node.py , not used outside\n\nIn [8]: n._unique_key\nOut[8]: ('preprocess_companies_node', 'companies', 'preprocessed_companies')"
  },
  {
    "objectID": "posts/default_node_name/2024-02-08-default-node-name.html#node.unique_key-hashable",
    "href": "posts/default_node_name/2024-02-08-default-node-name.html#node.unique_key-hashable",
    "title": "Investigation of the Kedro default node names",
    "section": "node.unique_key (hashable)",
    "text": "node.unique_key (hashable)\n\nReturn 3 things, tuple of (node name, sorted_input, sorted_output)\nnode comparison, checking of unique node\nhash(node) = hash(node._unique_key)\nless than, larger than , what for? __eq__ make sense.\n__lt__ - Private Kedro PR\n\nDoesn’t seem to be needed until I added the sorted(nodes) to ensure SequentialRunner have deterministic output ## node._func_name Usage:\n\n__str__\n__repr__\nshort_name\n__str__ and __repr__ will call node._func_name\n\n\nn = pipelines[\"__default__\"].nodes[0] # Getting the first node\n\ndef __str__(self) -&gt; str:\n    def _set_to_str(xset: set | list[str]) -&gt; str:\n        return f\"[{';'.join(xset)}]\"\n\n    out_str = _set_to_str(self.outputs) if self._outputs else \"None\"\n    in_str = _set_to_str(self.inputs) if self._inputs else \"None\"\n\n    prefix = self._name + \": \" if self._name else \"\"\n    return prefix + f\"{self._func_name}({in_str}) -&gt; {out_str}\"\n\n\ndef _set_to_str(xset: set | list[str]) -&gt; str:\n    return f\"[{';'.join(xset)}]\"\n\nself = n\nout_str = _set_to_str(self.outputs) if self._outputs else \"None\"\nin_str = _set_to_str(self.inputs) if self._inputs else \"None\"\n\nprefix = self._name + \": \" if self._name else \"\"\nprefix + f\"{self._func_name}({in_str}) -&gt; {out_str}\"\n\n\n\n\n\n'split: split_data([example_iris_data;parameters]) -&gt; [X_train;X_test;y_train;y_test]'\n\n\n\n\nn.__str__??\n\n\nn.__str__()\n\n\n\n\n\n'split: split_data([example_iris_data,parameters]) -&gt; [X_train,X_test,y_train,y_test]'\n\n\n\n\nstr(n)\n\n\n\n\n\n'split: split_data([example_iris_data,parameters]) -&gt; [X_train,X_test,y_train,y_test]'\n\n\n\n\nrepr(n)\n\n\n\n\n\n\"Node(split_data, ['example_iris_data', 'parameters'], ['X_train', 'X_test', 'y_train', 'y_test'], 'split')\"\n\n\n\n\nNotes:\n    - Duplicate Node are checked with `node.name` not `node.unique_key`"
  },
  {
    "objectID": "posts/default_node_name/2024-02-08-default-node-name.html#observation-2",
    "href": "posts/default_node_name/2024-02-08-default-node-name.html#observation-2",
    "title": "Investigation of the Kedro default node names",
    "section": "Observation 2",
    "text": "Observation 2\nhttps://github.com/kedro-org/kedro/pull/568/files - can replace with self._func_name instead of_get_readable_func_name`"
  },
  {
    "objectID": "posts/2019-10-19-Deskto-Notification.html",
    "href": "posts/2019-10-19-Deskto-Notification.html",
    "title": "plyer - Desktop Notification with Python",
    "section": "",
    "text": "from plyer import notification\nimport random\n\nclass DesktopNotification:\n    @staticmethod\n    def notify(title='Hey~', message='Done!', timeout=10):\n        ls = ['👍','✔','✌','👌','👍','😎']\n        notification.notify(\n            title = title ,\n            message = random.choice(ls) * 3 + ' ' + message,\n            timeout = timeout # seconds\n        )\n\n\nif __name__ == '__main__':\n    DesktopNotification.notify()\nYou could add this simple code block to notify you when the program is done! A desktop notification will be prompt on the bottom right corner in Window."
  },
  {
    "objectID": "posts/2021-04-16-full-stack-deep-learning-lecture-03.html",
    "href": "posts/2021-04-16-full-stack-deep-learning-lecture-03.html",
    "title": "Full Stack Deep Learning Notes - Lecture 03 - Recurrent Neural Network",
    "section": "",
    "text": "LSTM\nReference: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\nThe diagrams are from Chris Colah’s blog.\n\n\n\nRNN\nLSTM\n\n\n\n\n\n\n\n\n\n Forget Gate - Control the magnitude of cell state should be kept. Sigmoid range from (0 to 1). If 0, it means we should throw away the state cell, if 1 we keep everything.  * Input Gate - Control what relevant information can be added from the current step. It takes hidden step from last step and the current input into consideration.  * Output Gate - finalize the next hidden state\n\n\n# Google Neurl Machine Translation (GNMT)\nIt more or less follow the attention mechanism described here.\nhttps://blog.floydhub.com/attention-mechanism/#luong-att-step6\n\n\n\nattention_gnmt\n\n\n1.If you take the dot product of 1 encoder vector (at t_i) and decoder, you get a scalar. (Alignment Score) (1,h) * (h,1) -&gt; (1,1) 2. If encoder have 5 time_step, repeat the above steps -&gt; You get a vector with length of 5 (A vector of Alignment Scores) (5,h) (h,1) -&gt; (5,1) 3. Take softmax of the alignments scores -&gt; (attention weights which sum to 1) (5,1) 4. Take dot product of encoders state with attention weights (h, 5)  (5, 1) -&gt; (h, 1), where h stands for dimension of hidden state. The result is a “Context Vector”"
  },
  {
    "objectID": "posts/2021-11-18-what-can-we-learn-from-shipping-crisis-as-a-data-scientist.html",
    "href": "posts/2021-11-18-what-can-we-learn-from-shipping-crisis-as-a-data-scientist.html",
    "title": "What can we learn from Shipping Crisis as a Data Scientist?",
    "section": "",
    "text": "Even if you are not working in shipping industry, you probably heard about shipping cost is skyrocking for the last year. COVID is clearly the initial disruption, but the story does not end there. Recently, Long Beach’s port congestion is at a historcial scale, there are now more than 70+ ships waiting outside the port, the typical number is 1 or 2.\nYou may think the terminal must be busy as hell, so did I, but it is actualy far from the truth. In fact, the port is actually paralyzed. The reason surprised me a lot, it is not because of lacking of driver or empty containers, but yard space. Container are being unloaded from ships, then they are being put at the container yard before they go into depot or being stuffed again.\nOn a high level, it is caused by a negative feedback loop which COVID probably contributed a lot, as it caused a lot of disruption to the supply chain.\n\nPort Congestion -&gt; Containers pilled up at container yard since it is waiting to be loaded on ship\nContainer yard space is taken up by cotnainers, less space is available\nA container need to be put on a chassis before it is loaded, but as the container yard is full, empty containers stuck on the chassis and they need to be unloaded before you put a stuffed container.\nLess Chassis is available to load stuff, so it further slow down the process\nThe loop complete and it starts from 1 again\n\n\n\n\nPort Congestion Feedback Loop\n\n\nThis is a simplified story, you can find more details from this twitter thread from flexport’s CEO Ryan. There are more constraints that making this load/unload process inefficient, so the whole process is jammed. Think about a restaurant with limited amount of trays, you need to get a tray if you want to get food. But because there are too many customers, it jammed the door . So there are many customers holding an empty tray while many food are waiting to be served.\nRyan point out a very important lesson here, that is, you need to choose your bottleneck, and it should really be the capital intensive assets. Going back to our restaurant’s analogy, chef and space is probably the most expensive assets, so we should try to keep the utilization high. A simple solution is to buy more trays, so that it won’t be jammed. Ofcourse, you can also find a larger space, build a bigger door, but that will cost you more money too.\nFor shipping, the terminal’s crane should be the most capital intensive, so we should try our best to keep it working 24/7 to digest the terminal queue.\nThis is a simple idea yet it is powerful and it strikes me hard. As a data scientist, I work on optimization problem. To maximize the output of a system, we can use linear programming. When we are solving this problem, we are asking question like this.\n\nGiven x1 Terminals, x2 drivers, x3 containers, x4 ships, what is the maximize output of this system and how do you arrange them to achieve so?\n\nHowever, if you are a product/business analyst, a better question may be &gt; What is the output of this system if I add more container yard space?\nBy changing the input of the system, you may achieve much better result. But as a data scientist, we often stuck in a mode that how do we optimize x metrics with these features. So we may end up spending months and try to schedule ships and driver perfectly to load 10% more container, but you can actually increase loading efficiency by 50% simply by adding more yard space. It feels like cheating as a scientific question, since this is not we asked originally, but this happened a lot in a business context.\nWe are not trying to find the best algorithm to solve a problem, the algorithm is just one way of doing it. We may get surprising result by just tweaking the asked question a little bit.\nI am curious about what is the limiting factor in our current supply chain system, and how sensitive it is to the environment. Is forecasting & optimization the right way to do it? Do we actually need a precise forecast or we can have a bit of redundancy (like in this case, having extra yard space which could be a waste but improve the system robustness)? This is questions that we need to ask ourselves constantly, as the true question is often not asked, but explored after lots of iterations. We need to, and we have to ask the right question, and that is an art more elegant than an algorithm in my opinion.\nI do not know if Ryan’s word are 100% true, but it reminds me an important lesson. The right solution (question) may be simple, but it may not be obvious. Have we exploited all the simple solution before we went nuts with fancy algorithms?\np.s. Apologised as I don’t have time to proofread but simply try to write down the snapshot of my current mind [2021-11-18]\n\nReference\n{% twitter https://twitter.com/typesfast/status/1451543776992845834?s=20 %} https://twitter.com/typesfast/status/1451543776992845834?s=20 https://www.facebook.com/669645890/posts/10159859049175891/ unroll version: https://threadreaderapp.com/thread/1451543776992845834.html"
  },
  {
    "objectID": "posts/python_warning/2023-07-08-python-warning-test-version-switch-mock.html",
    "href": "posts/python_warning/2023-07-08-python-warning-test-version-switch-mock.html",
    "title": "Mocking Python version for testing, use of Python Warning to raise error for specific Python Version",
    "section": "",
    "text": "Background\nTo release a new open bound version of library so it can be instsalled in any Python versio, while making sure that users are aware this is not supported yet. This article explains well Why setting upper bound for version is a bad idea because it is not supported yet . TL;DR, if you cannot install it, you cannot even test if it works or not. In Python ecosystem, you relies on many libraries and it will take long time until all depedencies are updated, which in reality most likely it has not break anything at all.\n\n\nThe Approach\nWe use warnings and simplefilter. To your surprise, warnings can be triggered as an Exception with the flag -W, i.e. python my_program.py -W UserWarnings or using the Python environment variable PYTHONWARNINGS\nSee the standard Python Docs about warnings.warn yourself &gt; warnings.warn(message, category=None, stacklevel=1, source=None) Issue a warning, or maybe ignore it or raise an exception.\n(TBD, I am still working on the solution but I just need to write it down to document my state of mind lol)"
  },
  {
    "objectID": "posts/2022-05-30-mocking-with-pytest-patch.html#mock",
    "href": "posts/2022-05-30-mocking-with-pytest-patch.html#mock",
    "title": "Testing with Mocking",
    "section": "Mock",
    "text": "Mock\n\nmock = Mock()\n\nWith the Mock object, you can treat it like a magic object that have any attributes or methods.\n\nmock.super_method(), mock.attribute_that_does_not_exist_at_all\n\n(&lt;Mock name='mock.super_method()' id='1587554283232'&gt;,\n &lt;Mock name='mock.attribute_that_does_not_exist_at_all' id='1587554282512'&gt;)\n\n\n\nstr(mock)\n\n\"&lt;Mock id='1587554282848'&gt;\""
  },
  {
    "objectID": "posts/2022-05-30-mocking-with-pytest-patch.html#magicmock",
    "href": "posts/2022-05-30-mocking-with-pytest-patch.html#magicmock",
    "title": "Testing with Mocking",
    "section": "MagicMock",
    "text": "MagicMock\nThe “magic” comes from the magic methods of python object, for example, when you add two object together, it is calling the __add__ magic method under the hook.\n\nmock + mock\n\nTypeError: unsupported operand type(s) for +: 'Mock' and 'Mock'\n\n\n\nmagic_mock = MagicMock()\n\n\nmagic_mock + magic_mock\n\n&lt;MagicMock name='mock.__add__()' id='1587563722784'&gt;\n\n\nWith MagicMock, you get these magic methods for free, this is why adding two mock will not throw an error but adding two Mock will result in a TypeError\nLet say we want to mock the pandas.read_csv function, because we don’t actually want it to read a data, but just return some mock data whenever it is called. It’s easier to explain with an example."
  },
  {
    "objectID": "posts/2022-05-30-mocking-with-pytest-patch.html#mocker.patch-with-createtrue",
    "href": "posts/2022-05-30-mocking-with-pytest-patch.html#mocker.patch-with-createtrue",
    "title": "Testing with Mocking",
    "section": "mocker.patch with create=True",
    "text": "mocker.patch with create=True\n\n%%pytest\nimport pandas as pd\n\ndef test_read_csv(mocker):  # mocker is a special pytest fixture, so even though we haven't define it here but pytest understands it.\n    mocker.patch(\"pandas.read_special_csv\", return_value = \"fake_data\", create=False)\n    assert pd.read_special_csv(\"some_data\") == \"fake_data\"\n\n============================= test session starts =============================\nplatform win32 -- Python 3.8.5, pytest-6.2.5, py-1.11.0, pluggy-1.0.0\nrootdir: C:\\Users\\lrcno\\AppData\\Local\\Temp\\tmpzbddlxxg\nplugins: anyio-3.5.0, cov-3.0.0, mock-1.13.0\ncollected 1 item\n\n_ipytesttmp.py F                                                         [100%]\n\n================================== FAILURES ===================================\n________________________________ test_read_csv ________________________________\n\nmocker = &lt;pytest_mock.plugin.MockFixture object at 0x00000171B28B1820&gt;\n\n    def test_read_csv(mocker):  # mocker is a special pytest fixture, so even though we haven't define it here but pytest understands it.\n&gt;       mocker.patch(\"pandas.read_special_csv\", return_value = \"fake_data\", create=False)\n\n_ipytesttmp.py:4: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\..\\..\\..\\miniconda3\\lib\\site-packages\\pytest_mock\\plugin.py:193: in __call__\n    return self._start_patch(self.mock_module.patch, *args, **kwargs)\n..\\..\\..\\..\\miniconda3\\lib\\site-packages\\pytest_mock\\plugin.py:157: in _start_patch\n    mocked = p.start()\n..\\..\\..\\..\\miniconda3\\lib\\unittest\\mock.py:1529: in start\n    result = self.__enter__()\n..\\..\\..\\..\\miniconda3\\lib\\unittest\\mock.py:1393: in __enter__\n    original, local = self.get_original()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &lt;unittest.mock._patch object at 0x00000171B28B10D0&gt;\n\n    def get_original(self):\n        target = self.getter()\n        name = self.attribute\n    \n        original = DEFAULT\n        local = False\n    \n        try:\n            original = target.__dict__[name]\n        except (AttributeError, KeyError):\n            original = getattr(target, name, DEFAULT)\n        else:\n            local = True\n    \n        if name in _builtins and isinstance(target, ModuleType):\n            self.create = True\n    \n        if not self.create and original is DEFAULT:\n&gt;           raise AttributeError(\n                \"%s does not have the attribute %r\" % (target, name)\n            )\nE           AttributeError: &lt;module 'pandas' from 'c:\\\\users\\\\lrcno\\\\miniconda3\\\\lib\\\\site-packages\\\\pandas\\\\__init__.py'&gt; does not have the attribute 'read_special_csv'\n\n..\\..\\..\\..\\miniconda3\\lib\\unittest\\mock.py:1366: AttributeError\n=========================== short test summary info ===========================\nFAILED _ipytesttmp.py::test_read_csv - AttributeError: &lt;module 'pandas' from ...\n============================== 1 failed in 0.43s ==============================\n\n\nNow we fail the test because pandas.read_special_csv does not exist. However, with create=True you can make the test pass again. Normally you won’t want to do this, but it is an option that available.\n\n%%pytest\nimport pandas as pd\n\ndef test_read_csv(mocker):  # mocker is a special pytest fixture, so even though we haven't define it here but pytest understands it.\n    mocker.patch(\"pandas.read_special_csv\", return_value = \"fake_data\", create=True)\n    assert pd.read_special_csv(\"some_data\") == \"fake_data\"\n\n============================= test session starts =============================\nplatform win32 -- Python 3.8.5, pytest-6.2.5, py-1.11.0, pluggy-1.0.0\nrootdir: C:\\Users\\lrcno\\AppData\\Local\\Temp\\tmphqbckliw\nplugins: anyio-3.5.0, cov-3.0.0, mock-1.13.0\ncollected 1 item\n\n_ipytesttmp.py .                                                         [100%]\n\n============================== 1 passed in 0.10s ==============================\n\n\nMore often, you would want your mock resemble your real object, which means it has the same attributes and method, but it should fails when the method being called isn’t valid. You may specify the return_value with the mock type\n\n%%pytest -vvv\nimport pandas as pd\nfrom unittest.mock import Mock\nimport pytest\n\ndef test_read_csv_valid_method(mocker):  # mocker is a special pytest fixture, so even though we haven't define it here but pytest understands it.\n    mocker.patch(\"pandas.read_csv\", return_value = Mock(pd.DataFrame))\n    df =  pd.read_csv(\"some_data\")\n    df.mean()  # A DataFrame method\n\ndef test_read_csv_invalid_method(mocker):  # mocker is a special pytest fixture, so even though we haven't define it here but pytest understands it.\n    mocker.patch(\"pandas.read_csv\", return_value = Mock(pd.DataFrame))\n    df =  pd.read_csv(\"some_data\")\n    with pytest.raises(Exception):\n        df.not_a_dataframe_method()\n\n============================= test session starts =============================\nplatform win32 -- Python 3.8.5, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- c:\\users\\lrcno\\miniconda3\\python.exe\ncachedir: .pytest_cache\nrootdir: C:\\Users\\lrcno\\AppData\\Local\\Temp\\tmpyfiqtkoy\nplugins: anyio-3.5.0, cov-3.0.0, mock-1.13.0\ncollecting ... collected 2 items\n\n_ipytesttmp.py::test_read_csv_valid_method PASSED                        [ 50%]\n_ipytesttmp.py::test_read_csv_invalid_method PASSED                      [100%]\n\n============================== 2 passed in 0.16s =============================="
  },
  {
    "objectID": "posts/2020-02-08-Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html",
    "href": "posts/2020-02-08-Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html",
    "title": "Hydra - Config Composition for Machine Learning Project",
    "section": "",
    "text": "GitHub: https://github.com/noklam/notadatascientist/tree/master/demo/hydra-example\nMachine learning project involves large number of hyperparmeters. In many case you could have multiple config, e.g. differnet dataset, database connection, train/test mode. hydra provide a simple Command Line Interface that is useful for composing different experiment configs. In essence, it compose different files to a large config setting. It offers you the common Object Oriented Programming with YAML file. Allow you to have clear structure of configurations.\nAssume you have a config.yaml like this, where run_mode and hyperparmeter are separate folder to hold different choice of parameters. You can set defaults for them with the following structure."
  },
  {
    "objectID": "posts/2020-02-08-Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html#folder-structure",
    "href": "posts/2020-02-08-Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html#folder-structure",
    "title": "Hydra - Config Composition for Machine Learning Project",
    "section": "Folder Structure",
    "text": "Folder Structure\nconfig.yaml\ndemo.py\nrun_mode\n  - train.yaml\n  - test.yaml\nhyperparmeter\n  - base.yaml"
  },
  {
    "objectID": "posts/2020-02-08-Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html#config.yaml",
    "href": "posts/2020-02-08-Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html#config.yaml",
    "title": "Hydra - Config Composition for Machine Learning Project",
    "section": "config.yaml",
    "text": "config.yaml\ndefaults:\n - run_mode: train\n - hyperparameter: base\nThe benefit of using such approach is that it makes comparsion of experiments much easier. Instead of going through the parameters list, you only focus on the argument(the difference). It helps organize machine learning results and ease a lot of pain in tracking the model performance.\nimport hydra\nfrom omegaconf import DictConfig\n@hydra.main(config_path=\"config.yaml\")\ndef my_app(cfg : DictConfig) -&gt; None:\n    print(cfg.pretty())\nif __name__ == \"__main__\":\n    my_app()\npython demo.py \ngamma: 0.01\nlearning_rate: 0.01\nrun_mode: train\nweek: 8\nFor example, with a simple example with 4 parameters only, you can simply run the experiment with default"
  },
  {
    "objectID": "posts/2022-02-10-journey-of-understanding-python-and-programming-langauge.html",
    "href": "posts/2022-02-10-journey-of-understanding-python-and-programming-langauge.html",
    "title": "Journey of understanding Python and programming language",
    "section": "",
    "text": "To be written… # What is Python Interpreter?\n\nWhat is Bytecode?\n\n\nPython Virtual Machine\n\n\nCompiler\n\n\nEBNF Grammar\n\n\nLLVM"
  },
  {
    "objectID": "posts/2022-04-22-python-dataclass-partial-immutable.html",
    "href": "posts/2022-04-22-python-dataclass-partial-immutable.html",
    "title": "How to achieve Partial Immutability with Python’s dataclass?",
    "section": "",
    "text": "from dataclasses import dataclass, field, astuple\nWith dataclass, you can set frozen=True to ensure immutablilty.\n@dataclass(frozen=True)\nclass FrozenDataClass:\n    a: int\n    b: int\n\nfrozen = FrozenDataClass(1,2)\nfrozen.c = 3\n\nFrozenInstanceError: cannot assign to field 'c'\nMutating a frozen dataclass is not possible, but what if I need to compose some logic? __post_init__() method is how you can customize logic."
  },
  {
    "objectID": "posts/2022-04-22-python-dataclass-partial-immutable.html#post_init-assignment",
    "href": "posts/2022-04-22-python-dataclass-partial-immutable.html#post_init-assignment",
    "title": "How to achieve Partial Immutability with Python’s dataclass?",
    "section": "post_init assignment",
    "text": "post_init assignment\n\n@dataclass\nclass FrozenDataClass:\n    a: int\n    b: int\n    \n    def __post_init__(self):\n        self.c = self.a + self.b\nfrozen = FrozenDataClass(1,2)\n\n\nfrozen.a, frozen.b, frozen.c\n\n(1, 2, 3)\n\n\nDo notice that I removed the frozen=True flag, see what happen if I put it back."
  },
  {
    "objectID": "posts/2022-04-22-python-dataclass-partial-immutable.html#the-good-old-property",
    "href": "posts/2022-04-22-python-dataclass-partial-immutable.html#the-good-old-property",
    "title": "How to achieve Partial Immutability with Python’s dataclass?",
    "section": "The good old @property?",
    "text": "The good old @property?\n\n@dataclass\nclass PartialFrozenDataClass:\n    a: int # a should be frozen \n    b: int # Should be mutable\n    \n    @property\n    def b(self):\n        return self.b\n    \np = PartialFrozenDataClass(1,2)\n\nAttributeError: can't set attribute\n\n\nIt doesn’t work!"
  },
  {
    "objectID": "posts/2022-04-22-python-dataclass-partial-immutable.html#post_init-assignment-in-a-frozen-dataclass",
    "href": "posts/2022-04-22-python-dataclass-partial-immutable.html#post_init-assignment-in-a-frozen-dataclass",
    "title": "How to achieve Partial Immutability with Python’s dataclass?",
    "section": "post_init assignment in a frozen dataclass ✾",
    "text": "post_init assignment in a frozen dataclass ✾\n\n@dataclass(frozen=True)\nclass FrozenDataClass:\n    a: int\n    b: int\n    \n    def __post_init__(self):\n        self.c = self.a + self.b\nfrozen = FrozenDataClass(1,2)\n\nFrozenInstanceError: cannot assign to field 'c'\n\n\nIt doesn’t work! Because the frozen flag will block any assignment even in the __post_init__ method."
  },
  {
    "objectID": "posts/2022-04-22-python-dataclass-partial-immutable.html#workaround",
    "href": "posts/2022-04-22-python-dataclass-partial-immutable.html#workaround",
    "title": "How to achieve Partial Immutability with Python’s dataclass?",
    "section": "workaround",
    "text": "workaround\n\n@dataclass(frozen=True)\nclass FrozenDataClass:\n    a: int\n    b: int\n    \n    def __post_init__(self):\n        object.__setattr__(self, 'c', self.a + self.b)\n        \nfrozen = FrozenDataClass(1,2)\nfrozen.a, frozen.b, frozen.c\n\n(1, 2, 3)\n\n\n\n@dataclass(frozen=True)\nclass FrozenDataClass:\n    a: int\n    b: int\n    \n    def __post_init__(self):\n        super().__setattr__('c', self.a + self.b)\n        \nfrozen = FrozenDataClass(1,2)\nfrozen.a, frozen.b, frozen.c\n\n(1, 2, 3)\n\n\n\nfrozen.c = 3\n\nFrozenInstanceError: cannot assign to field 'c'\n\n\nIt works as expected, the workaround here is using object.__setattr__. The way dataclass achieve immutability is by blocking assignment in the __setattr__ method. This trick works because we are using the object class method instead of the cls method, thus it won’t stop us assign new attribute. More details can be found in Python Standard Doc."
  },
  {
    "objectID": "posts/kedro-debug-runner.html",
    "href": "posts/kedro-debug-runner.html",
    "title": "Quick implementation of Kedro DebugRunner",
    "section": "",
    "text": "core\n\nFill in a module description here\n\n::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\n%load_ext autoreload\n%autoreload 2\n\nfrom nbdev.showdoc import *\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nfrom collections import Counter\nfrom itertools import chain\nfrom typing import Any, Dict, Iterable, List, Set\n\nfrom kedro.framework.hooks.manager import _NullPluginManager\nfrom kedro.io import AbstractDataSet, DataCatalog, MemoryDataSet\nfrom kedro.pipeline import Pipeline\nfrom kedro.pipeline.node import Node\nfrom kedro.runner import SequentialRunner\nfrom kedro.runner.runner import AbstractRunner, run_node\nfrom pluggy import PluginManager\n\n\nclass DebugRunner(SequentialRunner):\n    def run(\n        self,\n        pipeline: Pipeline,\n        catalog: DataCatalog,\n        dataset_names: List[str] = None,\n        hook_manager: PluginManager = None,\n        session_id: str = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Run the ``Pipeline`` using the datasets provided by ``catalog``\n        and save results back to the same objects.\n\n        Args:\n            pipeline: The ``Pipeline`` to run.\n            catalog: The ``DataCatalog`` from which to fetch data.\n            hook_manager: The ``PluginManager`` to activate hooks.\n            session_id: The id of the session.\n\n        Raises:\n            ValueError: Raised when ``Pipeline`` inputs cannot be satisfied.\n\n        Returns:\n            Any node outputs that cannot be processed by the ``DataCatalog``.\n            These are returned in a dictionary, where the keys are defined\n            by the node outputs.\n\n        \"\"\"\n        if not dataset_names:\n            dataset_names = []\n        hook_manager = hook_manager or _NullPluginManager()\n        catalog = catalog.shallow_copy()\n\n        unsatisfied = pipeline.inputs() - set(catalog.list())\n        if unsatisfied:\n            raise ValueError(\n                f\"Pipeline input(s) {unsatisfied} not found in the DataCatalog\"\n            )\n\n        free_outputs = (\n            pipeline.outputs()\n        )  # Return everything regardless if it it's in catalog\n        unregistered_ds = pipeline.data_sets() - set(catalog.list())\n        for ds_name in unregistered_ds:\n            catalog.add(ds_name, self.create_default_data_set(ds_name))\n\n        if self._is_async:\n            self._logger.info(\n                \"Asynchronous mode is enabled for loading and saving data\"\n            )\n        self._run(pipeline, catalog, dataset_names, hook_manager, session_id)\n\n        self._logger.info(\"Pipeline execution completed successfully.\")\n        \n        free_outputs = free_outputs | set(dataset_names)  # Union\n\n        return {ds_name: catalog.load(ds_name) for ds_name in free_outputs}\n\n    def _run(\n        self,\n        pipeline: Pipeline,\n        catalog: DataCatalog,\n        dataset_names: List[str],\n        hook_manager: PluginManager,\n        session_id: str = None,\n    ) -&gt; None:\n        \"\"\"The method implementing sequential pipeline running.\n\n        Args:\n            pipeline: The ``Pipeline`` to run.\n            catalog: The ``DataCatalog`` from which to fetch data.\n            hook_manager: The ``PluginManager`` to activate hooks.\n            session_id: The id of the session.\n\n        Raises:\n            Exception: in case of any downstream node failure.\n        \"\"\"\n        nodes = pipeline.nodes\n        done_nodes = set()\n\n        load_counts = Counter(chain.from_iterable(n.inputs for n in nodes))\n\n        for exec_index, node in enumerate(nodes):\n            try:\n                run_node(node, catalog, hook_manager, self._is_async, session_id)\n                done_nodes.add(node)\n            except Exception:\n                self._suggest_resume_scenario(pipeline, done_nodes, catalog)\n                raise\n\n            # decrement load counts and release any data sets we've finished with\n            for data_set in node.inputs:\n                load_counts[data_set] -= 1\n                if load_counts[data_set] &lt; 1 and data_set not in pipeline.inputs():\n                    if data_set not in dataset_names:\n                        catalog.release(data_set)\n            for data_set in node.outputs:\n                if load_counts[data_set] &lt; 1 and data_set not in pipeline.outputs():\n                    if data_set not in dataset_names:\n                        catalog.release(data_set)\n\n            self._logger.info(\n                \"Completed %d out of %d tasks\", exec_index + 1, len(nodes)\n            )\n:::\n\n# `DebugRunner` has to be used in a different way since `session.run` don't support additional argument, so we are going to use a lower level approach and construct `Runner` and `Pipeline` and `DataCatalog` ourselves.\n\n# Testing Kedro Project: https://github.com/noklam/kedro_gallery/tree/master/kedro-debug-runner-demo\n%load_ext kedro.ipython\n%reload_kedro ~/dev/kedro_gallery/kedro-debug-runner-demo\n\nThe kedro.ipython extension is already loaded. To reload it, use:\n  %reload_ext kedro.ipython\n[10/06/22 14:45:20] INFO     Updated path to Kedro project:       __init__.py:54\n                             /Users/Nok_Lam_Chan/dev/kedro_galler               \n                             y/kedro-debug-runner-demo                          \n[10/06/22 14:45:22] INFO     Kedro project                        __init__.py:77\n                             kedro_debug_runner_demo                            \n                    INFO     Defined global variable 'context',   __init__.py:78\n                             'session', 'catalog' and 'pipelines'               \n\n\n\n%reload_kedro ~/dev/kedro_gallery/kedro-debug-runner-demo\nrunner = DebugRunner()\ndefault_pipeline = pipelines[\"__default__\"]\nrun_1 = runner.run(default_pipeline, catalog)\n\n                    INFO     Updated path to Kedro project:       __init__.py:54\n                             /Users/Nok_Lam_Chan/dev/kedro_galler               \n                             y/kedro-debug-runner-demo                          \n[10/06/22 14:45:24] INFO     Kedro project                        __init__.py:77\n                             kedro_debug_runner_demo                            \n                    INFO     Defined global variable 'context',   __init__.py:78\n                             'session', 'catalog' and 'pipelines'               \n                    INFO     Loading data from               data_catalog.py:343\n                             'example_iris_data'                                \n                             (CSVDataSet)...                                    \n                    INFO     Loading data from 'parameters'  data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: split:                    node.py:327\n                             split_data([example_iris_data,parameter            \n                             s]) -&gt; [X_train,X_test,y_train,y_test]             \n                    INFO     Saving data to 'X_train'        data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'X_test'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'y_train'        data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'y_test'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'X_train'     data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'X_test'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_train'     data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: make_predictions:         node.py:327\n                             make_predictions([X_train,X_test,y_trai            \n                             n]) -&gt; [y_pred]                                    \n                    INFO     Saving data to 'y_pred'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_pred'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_test'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: report_accuracy:          node.py:327\n                             report_accuracy([y_pred,y_test]) -&gt;                \n                             None                                               \n                    INFO     Model has accuracy of 0.933 on test     nodes.py:74\n                             data.                                              \n\n\n\nrunner = DebugRunner()\ndefault_pipeline = pipelines[\"__default__\"]\nrun_2 = runner.run(default_pipeline, catalog, dataset_names=[\"example_iris_data\"])\n\n[10/06/22 14:45:27] INFO     Loading data from               data_catalog.py:343\n                             'example_iris_data'                                \n                             (CSVDataSet)...                                    \n                    INFO     Loading data from 'parameters'  data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: split:                    node.py:327\n                             split_data([example_iris_data,parameter            \n                             s]) -&gt; [X_train,X_test,y_train,y_test]             \n                    INFO     Saving data to 'X_train'        data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'X_test'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'y_train'        data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'y_test'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'X_train'     data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'X_test'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_train'     data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: make_predictions:         node.py:327\n                             make_predictions([X_train,X_test,y_trai            \n                             n]) -&gt; [y_pred]                                    \n                    INFO     Saving data to 'y_pred'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_pred'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_test'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: report_accuracy:          node.py:327\n                             report_accuracy([y_pred,y_test]) -&gt;                \n                             None                                               \n                    INFO     Model has accuracy of 0.933 on test     nodes.py:74\n                             data.                                              \n                    INFO     Loading data from               data_catalog.py:343\n                             'example_iris_data'                                \n                             (CSVDataSet)...                                    \n\n\n\nrunner = DebugRunner()\ndefault_pipeline = pipelines[\"__default__\"]\nrun_3 = runner.run(default_pipeline, catalog, dataset_names=[\"X_train\"]) # Input datasets\n\n[10/06/22 14:46:01] INFO     Loading data from               data_catalog.py:343\n                             'example_iris_data'                                \n                             (CSVDataSet)...                                    \n                    INFO     Loading data from 'parameters'  data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: split:                    node.py:327\n                             split_data([example_iris_data,parameter            \n                             s]) -&gt; [X_train,X_test,y_train,y_test]             \n                    INFO     Saving data to 'X_train'        data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'X_test'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'y_train'        data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'y_test'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'X_train'     data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'X_test'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_train'     data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: make_predictions:         node.py:327\n                             make_predictions([X_train,X_test,y_trai            \n                             n]) -&gt; [y_pred]                                    \n                    INFO     Saving data to 'y_pred'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_pred'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_test'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: report_accuracy:          node.py:327\n                             report_accuracy([y_pred,y_test]) -&gt;                \n                             None                                               \n                    INFO     Model has accuracy of 0.933 on test     nodes.py:74\n                             data.                                              \n                    INFO     Loading data from 'X_train'     data_catalog.py:343\n                             (MemoryDataSet)...                                 \n\n\n\nrun_1\n\n{}\n\n\n\nrun_2\n\n{'example_iris_data':      sepal_length  sepal_width  petal_length  petal_width    species\n 0             5.1          3.5           1.4          0.2     setosa\n 1             4.9          3.0           1.4          0.2     setosa\n 2             4.7          3.2           1.3          0.2     setosa\n 3             4.6          3.1           1.5          0.2     setosa\n 4             5.0          3.6           1.4          0.2     setosa\n ..            ...          ...           ...          ...        ...\n 145           6.7          3.0           5.2          2.3  virginica\n 146           6.3          2.5           5.0          1.9  virginica\n 147           6.5          3.0           5.2          2.0  virginica\n 148           6.2          3.4           5.4          2.3  virginica\n 149           5.9          3.0           5.1          1.8  virginica\n \n [150 rows x 5 columns]}\n\n\n\nrun_3\n\n{'X_train':      sepal_length  sepal_width  petal_length  petal_width\n 47            4.6          3.2           1.4          0.2\n 3             4.6          3.1           1.5          0.2\n 31            5.4          3.4           1.5          0.4\n 25            5.0          3.0           1.6          0.2\n 15            5.7          4.4           1.5          0.4\n ..            ...          ...           ...          ...\n 28            5.2          3.4           1.4          0.2\n 78            6.0          2.9           4.5          1.5\n 146           6.3          2.5           5.0          1.9\n 49            5.0          3.3           1.4          0.2\n 94            5.6          2.7           4.2          1.3\n \n [120 rows x 4 columns]}\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass GreedySequentialRunner(SequentialRunner):\n    def run(\n        self,\n        pipeline: Pipeline,\n        catalog: DataCatalog,\n        hook_manager: PluginManager = None,\n        session_id: str = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Run the ``Pipeline`` using the datasets provided by ``catalog``\n        and save results back to the same objects.\n\n        Args:\n            pipeline: The ``Pipeline`` to run.\n            catalog: The ``DataCatalog`` from which to fetch data.\n            hook_manager: The ``PluginManager`` to activate hooks.\n            session_id: The id of the session.\n\n        Raises:\n            ValueError: Raised when ``Pipeline`` inputs cannot be satisfied.\n\n        Returns:\n            Any node outputs that cannot be processed by the ``DataCatalog``.\n            These are returned in a dictionary, where the keys are defined\n            by the node outputs.\n\n        \"\"\"\n\n        hook_manager = hook_manager or _NullPluginManager()\n        catalog = catalog.shallow_copy()\n\n        unsatisfied = pipeline.inputs() - set(catalog.list())\n        if unsatisfied:\n            raise ValueError(\n                f\"Pipeline input(s) {unsatisfied} not found in the DataCatalog\"\n            )\n\n        free_outputs = pipeline.outputs() # Return everything regardless if it it's in catalog\n        unregistered_ds = pipeline.data_sets() - set(catalog.list())\n        for ds_name in unregistered_ds:\n            catalog.add(ds_name, self.create_default_data_set(ds_name))\n\n        if self._is_async:\n            self._logger.info(\n                \"Asynchronous mode is enabled for loading and saving data\"\n            )\n        self._run(pipeline, catalog, hook_manager, session_id)\n\n        self._logger.info(\"Pipeline execution completed successfully.\")\n\n        return {ds_name: catalog.load(ds_name) for ds_name in free_outputs}\n:::\n::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\nimport nbdev; nbdev.nbdev_export()\n:::"
  },
  {
    "objectID": "posts/2021-07-02-kedro-datacatalog.html",
    "href": "posts/2021-07-02-kedro-datacatalog.html",
    "title": "Advance Kedro Series - Digging into Dataset Memory Management and CacheDataSet",
    "section": "",
    "text": "Today I am gonna explain some kedro internals to understnad how kedor manage your dataset. If you always write imperative python code, you may find that writing nodes and pipeline is a little bti awkward. They may seems less intuitive, however, it also enable some interesting featrue.\nThis article assumes you have basic understanding of kedro, I will focus on CacheDataSet and the auto-release dataset feature of kedro pipeline. It is useful to reduce your memory footprint without encountering the infamous Out of Memory (OOM) issue.\nTo start with, we have the default iris dataset. Normally we would do it in a YAML file, but to make things easier in Notebook, I’ll keep everything compact in a notebook.\n\nimport kedro\nkedro.__version__\n\n'0.17.4'\n\n\n\nfrom kedro.io import DataCatalog, MemoryDataSet, CachedDataSet\nfrom kedro.extras.datasets.pandas import CSVDataSet\nfrom kedro.pipeline import node, Pipeline\nfrom kedro.runner import SequentialRunner\n\n# Prepare a data catalog\ndata_catalog = DataCatalog({\"iris\": CSVDataSet('data/01_raw/iris.csv')})\n\nNext, we have a pipeline follows this execution order: A -&gt; B -&gt; C\n\nfrom kedro.pipeline import Pipeline, node\nimport pandas as pd\n\n\ndef A(df):\n    print('Loading the Iris Dataset')\n    return 'Step1'\n\n\ndef B(dummy):\n    return 'Step2'\n\n\ndef C(dummy):\n    return 'Step3'\n\n\npipeline = Pipeline([node(A, \"iris\", \"A\"),\n                     node(B, \"A\", \"B\"),\n                     node(C, \"B\", \"C\"),\n                    ])\n\nc:\\programdata\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n\n\nTo zoom in to the pipeline, we can use Hook to print out the catalog after every node’s run.\n\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.framework.hooks import get_hook_manager\nfrom pprint import pprint\n\ndef apply_dict(d):\n    new_dict = {}\n    for k, v in d.items():\n        if isinstance(v, CachedDataSet):\n            if v._cache.exists():\n                print(v._cache._data)\n                new_dict[k] = 'In Memory'\n            else:\n                new_dict[k] ='Cache Deleted'\n        elif v.exists():\n            new_dict[k] = 'In Memory'\n    return new_dict\n\n\nclass DebugHook:\n    \"\"\"A hook class for creating a post mortem debugging with the PDB debugger\n    whenever an error is triggered within a pipeline. The local scope from when the\n    exception occured is available within this debugging session.\n    \"\"\"\n    @hook_impl\n    def after_node_run(self, node, catalog):\n        # adding extra behaviour to a single node\n        print(f\"Finish node {node.name}\")\n        pprint(f\"Print Catalog {apply_dict(catalog._data_sets)}\")\n#         pprint(f\"Print Catalog {apply_dict2(lambda x:x.exists(), catalog._data_sets)}\")\n        print(\"*****************************\")\n        \nhook_manager = get_hook_manager()\ndebug_hook = hook_manager.register(DebugHook());\n\nThis hook will print out dataset that exist in data catalog. It is a bit tricky because kedro did not delete the dataset, it marked the underlying data as _EMPTY object instead.\n\n# Create a runner to run the pipeline\nrunner = SequentialRunner()\n\n# Run the pipeline\nrunner.run(pipeline, data_catalog);\n\nLoading the Iris Dataset\nFinish node A([iris]) -&gt; [A]\n\"Print Catalog {'iris': 'In Memory'}\"\n*****************************\nFinish node B([A]) -&gt; [B]\n\"Print Catalog {'iris': 'In Memory', 'A': 'In Memory'}\"\n*****************************\nFinish node C([B]) -&gt; [C]\n\"Print Catalog {'iris': 'In Memory', 'B': 'In Memory'}\"\n*****************************\n\n\nLet’s have a look at what happened when a SequentialRunner runs a pipeline.\nIt is interesting to note that kedro takes a similar approach to Python, it uses reference counting to control the dataset life cycle. If you are interested, I have another post to dive into Python Memory Management.\n            # decrement load counts and release any data sets we've finished with\n            for data_set in node.inputs:\n                load_counts[data_set] -= 1\n                if load_counts[data_set] &lt; 1 and data_set not in pipeline.inputs():\n                    catalog.release(data_set)\n            for data_set in node.outputs:\n                if load_counts[data_set] &lt; 1 and data_set not in pipeline.outputs():\n                    catalog.release(data_set)\n\nCacheDataSet\nWhat does release do? It will remove the underlying data if this data is stored in memory.\n# In CSVDataSet\nhttps://github.com/quantumblacklabs/kedro/blob/master/kedro/extras/datasets/pandas/csv_dataset.py#L176-L178\n```python\ndef _release(self) -&gt; None:\n    super()._release()\n    self._invalidate_cache()\n# In CacheDataSet\ndef _release(self) -&gt; None:\n    self._cache.release()\n    self._dataset.release()\n# In MemoryDataSet\ndef _release(self) -&gt; None:\n    self._data = _EMPTY\nFirst, we can test if it works as expected.\n\nd = CachedDataSet(CSVDataSet('data/01_raw/iris.csv'))\nd.load()\nd._cache._data.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\nd.exists()\n\nc:\\programdata\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n\n\nTrue\n\n\n\nd.release()\n\n\nd._cache.exists()\n\nFalse\n\n\nThis is the expected behavior, where the cache should be released. However, it seems not to be the case when I run the pipeline.\n\ndata_catalog = DataCatalog({\"iris\": CachedDataSet(CSVDataSet('data/01_raw/iris.csv'))})\nrunner.run(pipeline, data_catalog)\n\nLoading the Iris Dataset\nFinish node A([iris]) -&gt; [A]\n     sepal_length  sepal_width  petal_length  petal_width    species\n0             5.1          3.5           1.4          0.2     setosa\n1             4.9          3.0           1.4          0.2     setosa\n2             4.7          3.2           1.3          0.2     setosa\n3             4.6          3.1           1.5          0.2     setosa\n4             5.0          3.6           1.4          0.2     setosa\n..            ...          ...           ...          ...        ...\n145           6.7          3.0           5.2          2.3  virginica\n146           6.3          2.5           5.0          1.9  virginica\n147           6.5          3.0           5.2          2.0  virginica\n148           6.2          3.4           5.4          2.3  virginica\n149           5.9          3.0           5.1          1.8  virginica\n\n[150 rows x 5 columns]\n\"Print Catalog {'iris': 'In Memory'}\"\n*****************************\nFinish node B([A]) -&gt; [B]\n     sepal_length  sepal_width  petal_length  petal_width    species\n0             5.1          3.5           1.4          0.2     setosa\n1             4.9          3.0           1.4          0.2     setosa\n2             4.7          3.2           1.3          0.2     setosa\n3             4.6          3.1           1.5          0.2     setosa\n4             5.0          3.6           1.4          0.2     setosa\n..            ...          ...           ...          ...        ...\n145           6.7          3.0           5.2          2.3  virginica\n146           6.3          2.5           5.0          1.9  virginica\n147           6.5          3.0           5.2          2.0  virginica\n148           6.2          3.4           5.4          2.3  virginica\n149           5.9          3.0           5.1          1.8  virginica\n\n[150 rows x 5 columns]\n\"Print Catalog {'iris': 'In Memory', 'A': 'In Memory'}\"\n*****************************\nFinish node C([B]) -&gt; [C]\n     sepal_length  sepal_width  petal_length  petal_width    species\n0             5.1          3.5           1.4          0.2     setosa\n1             4.9          3.0           1.4          0.2     setosa\n2             4.7          3.2           1.3          0.2     setosa\n3             4.6          3.1           1.5          0.2     setosa\n4             5.0          3.6           1.4          0.2     setosa\n..            ...          ...           ...          ...        ...\n145           6.7          3.0           5.2          2.3  virginica\n146           6.3          2.5           5.0          1.9  virginica\n147           6.5          3.0           5.2          2.0  virginica\n148           6.2          3.4           5.4          2.3  virginica\n149           5.9          3.0           5.1          1.8  virginica\n\n[150 rows x 5 columns]\n\"Print Catalog {'iris': 'In Memory', 'B': 'In Memory'}\"\n*****************************\n\n\n{'C': 'Step3'}\n\n\nThe dataset is persisted throughout the entire pipeline, why? We can monkey patch the SequentialRunner to check why is this happening.\n\n\nA potential bug or undesired beahvior?\n\nfrom collections import Counter\nfrom itertools import chain\nfrom kedro.runner.runner import AbstractRunner, run_node\n\ndef _run(\n    self, pipeline, catalog, run_id = None\n) -&gt; None:\n    \"\"\"The method implementing sequential pipeline running.\n\n    Args:\n        pipeline: The ``Pipeline`` to run.\n        catalog: The ``DataCatalog`` from which to fetch data.\n        run_id: The id of the run.\n\n    Raises:\n        Exception: in case of any downstream node failure.\n    \"\"\"\n    nodes = pipeline.nodes\n    done_nodes = set()\n\n    load_counts = Counter(chain.from_iterable(n.inputs for n in nodes))\n\n    for exec_index, node in enumerate(nodes):\n        try:\n            run_node(node, catalog, self._is_async, run_id)\n            done_nodes.add(node)\n        except Exception:\n            self._suggest_resume_scenario(pipeline, done_nodes)\n            raise\n            \n        # print load counts for every node run\n        pprint(f\"{load_counts}\")\n        print(\"pipeline input: \", pipeline.inputs())\n        print(\"pipeline output: \", pipeline.outputs())\n\n        # decrement load counts and release any data sets we've finished with\n        for data_set in node.inputs:\n            load_counts[data_set] -= 1\n            if load_counts[data_set] &lt; 1 and data_set not in pipeline.inputs():\n                catalog.release(data_set)\n        for data_set in node.outputs:\n            if load_counts[data_set] &lt; 1 and data_set not in pipeline.outputs():\n                catalog.release(data_set)\n\n        self._logger.info(\n            \"Completed %d out of %d tasks\", exec_index + 1, len(nodes)\n        )\n        \nSequentialRunner._run = _run\n\nc:\\programdata\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n\n\nNow we re-run the pipeline. Let’s reset the hook to only print related information.\n\nclass PrintHook:\n    @hook_impl\n    def after_node_run(self, node, catalog):\n        # adding extra behaviour to a single node\n        print(f\"Finish node {node.name}\")\n        print(\"*****************************\")\n        \n\nhook_manager.set_blocked(debug_hook); # I tried hook_manger.unregister(), but it is not working.\nprint_hook = hook_manager.register(PrintHook())\n\n\n# Create a runner to run the pipeline\nrunner = SequentialRunner()\n\n# Run the pipeline\nrunner.run(pipeline, data_catalog);\n\nLoading the Iris Dataset\nFinish node A([iris]) -&gt; [A]\n*****************************\n\"Counter({'iris': 1, 'A': 1, 'B': 1})\"\npipeline input:  {'iris'}\npipeline output:  {'C'}\nFinish node B([A]) -&gt; [B]\n*****************************\n\"Counter({'A': 1, 'B': 1, 'iris': 0})\"\npipeline input:  {'iris'}\npipeline output:  {'C'}\nFinish node C([B]) -&gt; [C]\n*****************************\n\"Counter({'B': 1, 'iris': 0, 'A': 0})\"\npipeline input:  {'iris'}\npipeline output:  {'C'}\n\n\n\n\nConclusion\nSo the reason why the iris data is kept becasue it is always in pipeline.inputs(), which I think is not what we wanted."
  },
  {
    "objectID": "posts/kedro-with-databricks-asset-bundle/2024-02-21-kedro-with-databricks-asset.html",
    "href": "posts/kedro-with-databricks-asset-bundle/2024-02-21-kedro-with-databricks-asset.html",
    "title": "Kedro with Databricks Assets Bundle",
    "section": "",
    "text": "(Disclaimer: This is not an official documentation).\nThis post describe the process of using Databricks Assets bundle, deploy the notebook to a Databricks workspace and runs the notebook as a Databricks Job."
  },
  {
    "objectID": "posts/kedro-with-databricks-asset-bundle/2024-02-21-kedro-with-databricks-asset.html#requirements",
    "href": "posts/kedro-with-databricks-asset-bundle/2024-02-21-kedro-with-databricks-asset.html#requirements",
    "title": "Kedro with Databricks Assets Bundle",
    "section": "Requirements",
    "text": "Requirements\nI test this with the following dependencies: - kedro==0.19.2 - databricks-cli==0.214.0 # Installation guide: https://docs.databricks.com/en/dev-tools/cli/install.html"
  },
  {
    "objectID": "posts/kedro-with-databricks-asset-bundle/2024-02-21-kedro-with-databricks-asset.html#create-a-project-with-kedro-and-databricks-asset-bundles",
    "href": "posts/kedro-with-databricks-asset-bundle/2024-02-21-kedro-with-databricks-asset.html#create-a-project-with-kedro-and-databricks-asset-bundles",
    "title": "Kedro with Databricks Assets Bundle",
    "section": "Create a project with Kedro and Databricks Asset Bundles",
    "text": "Create a project with Kedro and Databricks Asset Bundles\nFirst, I create a new kedro project which contains a spark pipeline that are ready to run in databricks with kedro new -s databricks-iris. The workflow of using Databricks worksapce to develop Kedro project is documented here.\nNext, you need to have the Databricks CLI installed, run this command to create a Databricks Assets Bundle template.\ndatabricks bundle init, it will prompt you for a few things. For testing purpose, I answer yes for all questions. The resulting folder look like this: \nThis create yet another project template. Both kedro new and databricks bundle init assume you are creating new project. Since we have a Kedro project already, you don’t need the project related files. i.e. requirements-dev.txt, setup.py and src/&lt;project_name&gt;. You can then move everythin from a Kedro project inside the bundle_example folder so that they share the same root level. i.e. pyproject.toml (create by Kedro) should be in the same level as databricks.yml\n`databricks bundle deploy -t dev`\nError: terraform apply: exit status 1\n\nError: cannot create job: default auth: cannot configure default credentials, please check https://docs.databricks.com/en/dev-tools/auth.html#databricks-client-unified-authentication to configure credentials for your preferred authentication method. Config: host=https://adb-4127266075722018.18.azuredatabricks.net. Env: DATABRICKS_HOST\n\n  with databricks_job.my_project_job,\n  on bundle.tf.json line 77, in resource.databricks_job.my_project_job:\n  77:       }"
  },
  {
    "objectID": "posts/kedro-with-databricks-asset-bundle/2024-02-21-kedro-with-databricks-asset.html#create-the-developer-token",
    "href": "posts/kedro-with-databricks-asset-bundle/2024-02-21-kedro-with-databricks-asset.html#create-the-developer-token",
    "title": "Kedro with Databricks Assets Bundle",
    "section": "Create the Developer Token",
    "text": "Create the Developer Token\nI try to submit a job immediately after creating the bundles, the error suggests that I need to create a developer token to submit job to Databricks locally. Go to Databricks workspace -&gt; User Settings -&gt; Developer -&gt; Generate New\nRun in terminal: export DATABRICKS_TOKEN=&lt;your-token&gt;"
  },
  {
    "objectID": "posts/kedro-with-databricks-asset-bundle/2024-02-21-kedro-with-databricks-asset.html#submit-the-job",
    "href": "posts/kedro-with-databricks-asset-bundle/2024-02-21-kedro-with-databricks-asset.html#submit-the-job",
    "title": "Kedro with Databricks Assets Bundle",
    "section": "Submit the job",
    "text": "Submit the job\nAfter this, I run databricks bundle deploy -t dev again and I see this in my workspace.\n\n\n\nworkspace-budnle\n\n\nIf you name your bundle my_project, you should see a my_project_job.yml. You will need to update the tasks and the notebook_path to the targeted notebook. databricks bundle run -t dev my_project_job\nFor example:\n      tasks:\n        - task_key: notebook_task\n          job_cluster_key: job_cluster\n          notebook_task:\n            notebook_path: ../src/notebook.ipynb\nAfter this, I can submit a job and see this on Databricks. Unfortunately I cannot get it running because I have permission issue to create a Databricks Job, but I can see the job request on the UI.\n\n## Summary\nAlthough there are not much documentation on the Internet yet, it's fairly easy to combine the two. It would be nice to automate some of the manual steps, but it is challenging because both takes a template approach and it's always hard to merge them automatically."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Kedro Pipeline (1) - Slicing Pipeline Effortlessly 🍕\n\n\n\n\n\n\n\npython\n\n\nkedro\n\n\ndatabricks\n\n\n\n\nKedro Pipeline provides options allow you to slice and compose pipelines effortlessly\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nKedro with Databricks Assets Bundle\n\n\n\n\n\n\n\npython\n\n\nkedro\n\n\ndatabricks\n\n\n\n\nUsing Kedro with Databricks Assets Bundle\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nEnhancing Debugging experience with Jupyter Magic\n\n\n\n\n\n\n\npython\n\n\nkedro\n\n\njupyter\n\n\n\n\nJupyter Notebook has been thed default tooling for many. Despite many people trying to get rid of it, it integrates even more deeply with the data ecosystem and it is going to stay. By introducing a Jupyter magig command, the debugging experience has been improve and lower the barrier without learning how to use a debugger.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nInvestigation of the Kedro default node names\n\n\n\n\n\n\n\npython\n\n\nkedro\n\n\n\n\nhttps://github.com/kedro-org/kedro/issues/3575\n\n\n\n\n\n\nFeb 8, 2024\n\n\n\n\n\n\n  \n\n\n\n\nHow Kedro Config Loader works\n\n\n\n\n\n\n\npython\n\n\nkedro\n\n\n\n\nKedro Config Loader, resolution order\n\n\n\n\n\n\nNov 16, 2023\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nAdvance Pytest - parameterize your fixture\n\n\n\n\n\n\n\npytest\n\n\n\n\nUsing pytest fixture with params combination\n\n\n\n\n\n\nNov 15, 2023\n\n\n\n\n\n\n  \n\n\n\n\nExperimenting Rust with Python\n\n\n\n\n\n\n\nrust\n\n\n\n\nLearnin Rust, one step at a time\n\n\n\n\n\n\nNov 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\nA Guide to Kedro Namespace Pipelines for Time Series Forecasting\n\n\n\n\n\n\n\nkedro\n\n\n\n\nLearn how to use Kedro’s Namespace Pipelines for efficient time series forecasting.\n\n\n\n\n\n\nSep 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\nFunction overloading - singledispatch in Python with type hint\n\n\n\n\n\n\n\npython\n\n\n\n\nUsing singledispatch with type hint\n\n\n\n\n\n\nNov 16, 2022\n\n\n\n\n\n\n  \n\n\n\n\nMocking Python version for testing, use of Python Warning to raise error for specific Python Version\n\n\n\n\n\n\n\npython\n\n\n\n\nMocking Python version for testing, use of Python Warning to raise error for specific Python Version\n\n\n\n\n\n\nNov 16, 2022\n\n\n\n\n\n\n  \n\n\n\n\nKedro DuckDB\n\n\n\n\n\n\n\npython\n\n\n\n\nKedro Meet the Duck\n\n\n\n\n\n\nNov 15, 2022\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding the Kedro codebase - A quick dirty meta-analysis - (Part I)\n\n\n\n\n\n\n\nkedro\n\n\n\n\nMeta-analysis of the kedro codebase\n\n\n\n\n\n\nNov 13, 2022\n\n\n\n\n\n\n  \n\n\n\n\nBeing Python Expert\n\n\n\n\n\n\n\npython\n\n\n\n\nNotes for the talk - James Powell: So you want to be a Python expert? | PyData Seattle 2017\n\n\n\n\n\n\nNov 10, 2022\n\n\n\n\n\n\n  \n\n\n\n\nDemo of debugging Kedro pipeline with noetebook\n\n\n\n\n\n\n\npython\n\n\nkedro\n\n\n\n\nDemo of debugging Kedro pipeline with noetebook\n\n\n\n\n\n\nNov 8, 2022\n\n\n\n\n\n\n  \n\n\n\n\nQuick implementation of Kedro DebugRunner\n\n\n\n\n\n\n\neuropython\n\n\n\n\nQuick implementation of a debug runner\n\n\n\n\n\n\nNov 1, 2022\n\n\n\n\n\n\n  \n\n\n\n\nEuroPython 2022 - Conference Notes & Summary\n\n\n\n\n\n\n\neuropython\n\n\n\n\nNotes for EuroPython2022 (Update daily)\n\n\n\n\n\n\nJul 11, 2022\n\n\n\n\n\n\n  \n\n\n\n\nTesting with Mocking\n\n\n\n\n\n\n\npython\n\n\n\n\nI have been working with on some unit tests recently with mocking. There are some traps that I falled into and I want to document it here.\n\n\n\n\n\n\nMay 30, 2022\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nHow to achieve Partial Immutability with Python’s dataclass?\n\n\n\n\n\n\n\npython\n\n\n\n\nExploring the Python’s dataclass\n\n\n\n\n\n\nApr 22, 2022\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nJourney of understanding Python and programming language\n\n\n\n\n\n\n\npython\n\n\n\n\nSome unfinished notes about Python lower level details.\n\n\n\n\n\n\nFeb 10, 2022\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nWhat can we learn from Shipping Crisis as a Data Scientist?\n\n\n\n\n\n\n\nproduct\n\n\n\n\nThe Long Beach port congestion could teach us a lot about data science in real world.\n\n\n\n\n\n\nNov 18, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nPython FileNotFoundError or You have a really long file path?\n\n\n\n\n\n\n\npython\n\n\n\n\nToday I encountered an interesting bug that I think it is worth to write it down for my future self.\n\n\n\n\n\n\nAug 18, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\n5 Minutes Data Science Design Patterns I - Callback\n\n\n\n\n\n\n\npython\n\n\ndesign pattern\n\n\nsoftware\n\n\n\n\nA mini collections of design pattern for Data Science - Starting with callbacks.\n\n\n\n\n\n\nJul 10, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nAdvance Kedro Series - Digging into Dataset Memory Management and CacheDataSet\n\n\n\n\n\n\n\npython\n\n\nkedro\n\n\n\n\nKedro pipeline offers some nice feature like automatically release data in memory that is no longer need. How is this possible? Let’s dive deep into the code.\n\n\n\n\n\n\nJul 2, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nJupyter Superpower - Extend SQL analysis with Python\n\n\n\n\n\n\n\npython\n\n\nreviewnb\n\n\nsql\n\n\n\n\nMaking collboration with Notebook possible and share perfect SQL analysis with Notebook.\n\n\n\n\n\n\nJun 26, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nA logging.config.dictConfig() issue in python\n\n\n\n\n\n\n\npython\n\n\n\n\nlogging.config.dictConfig()\n\n\n\n\n\n\nJun 20, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nPython Internal Series - Global Interpreter Lock (GIL) and Memory Management\n\n\n\n\n\n\n\npython-internal\n\n\n\n\nDespite the bad reputation of GIL, it was arguably designed on purpose. The GIL actually comes with a lot of benefit.\n\n\n\n\n\n\nMay 29, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nFull Stack Deep Learning Notes - Lecture 03 - Recurrent Neural Network\n\n\n\n\n\n\n\nfsdl\n\n\n\n\nLecture & Lab notes - This lecture is about Recurrent Neural Network. Key concetps included input gate, forget gate, cell state, and output gate. It also explains how attention mechanism works for a encoder-decoder based architecture.\n\n\n\n\n\n\nApr 16, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nMicrosoft Azure - DP100\n\n\n\n\n\n\n\nazure\n\n\n\n\nThis note helps you to prepare the Azure Assoicate Data Scientist DP-100 exam. I took DP100 in Mar 2021 and includes some important notes for study. Particularly, syntax types questions are very common. You need to study the lab and make sure you understand and remember some syntax to pass this exam.\n\n\n\n\n\n\nMar 27, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nFull Stack Deep Learning Notes - Lecture 01\n\n\n\n\n\n\n\nfsdl\n\n\n\n\nLecture & Lab notes, explain DataModules, Trainer LightningModule.\n\n\n\n\n\n\nMar 21, 2021\n\n\n\n\n\n\n  \n\n\n\n\ndeepcopy, LGBM and pickle\n\n\n\n\n\n\n\npython\n\n\npickle\n\n\ndeepcopy\n\n\n\n\nAt first sight, these 3 things may not sounds related at all. I am writing this article to share a bug with lightgbm that I encountered and it eventually leads to deeper understanding of what pickle really are.\n\n\n\n\n\n\nMar 19, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nData Test as CI\n\n\n\n\n\n\n\npython\n\n\n\n\nUnit Test with Pytest. I want to display a full information rich string into my CI Log, but it is trimmed by Python. Here is a simple fix to make it display full string in the log file.\n\n\n\n\n\n\nMar 17, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nSetting up pyodbc for Impala connection, works on both Linux and Window\n\n\n\n\n\n\n\npyodbc\n\n\nimpala\n\n\n\n\nEasiest way to connect with Impala in Windows\n\n\n\n\n\n\nMar 5, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\ndata augmentation - Understand MixUp and Beta Distribution\n\n\n\n\n\n\n\nML\n\n\n\n\nMixup and Beta Distribution\n\n\n\n\n\n\nFeb 9, 2020\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nHydra - Config Composition for Machine Learning Project\n\n\n\n\n\n\n\npython\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2020\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nplyer - Desktop Notification with Python\n\n\n\n\n\n\n\npython\n\n\n\n\nDesktop notifiaction with Python\n\n\n\n\n\n\nOct 19, 2019\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nnbdev + GitHub Codespaces: A New Literate Programming Environment\n\n\n\n\n\n\n\ncodespaces\n\n\nnbdev\n\n\n\n\nHow a new GitHub feature makes literate programming easier than ever before.\n\n\n\n\n\n\nJan 1, 2019\n\n\nHamel Husain & Jeremy Howard\n\n\n\n\n\n\nNo matching items"
  }
]