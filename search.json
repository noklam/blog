[
  {
    "objectID": "posts/2021-06-20-logging-config-dict-issue-kedro.html",
    "href": "posts/2021-06-20-logging-config-dict-issue-kedro.html",
    "title": "A logging.config.dictConfig() issue in python",
    "section": "",
    "text": "import logging\nfrom clearml import Task\nconf_logging = {\"version\":1, \n                \"formatters\":{\n                      \"simple\":{\n                             \"format\":\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"}\n                      }\n                  }\nt = Task.init(project_name=\"test\")\nlogging.config.dictConfig(conf_logging)\nlogging.info(\"INFO!\")\nlogging.debug(\"DEBUG!\")\nlogging.warning(\"WARN!\")\nprint(\"PRINT!\")\nWith this code block, you will find no print() or logging is sent to ClearML logging Console. Turns out kedro use logging.config.dictConfig(conf_logging) as the default and causing this issue.\nA quick fix is to add \"incremental\": True in the config dict. In the standard documentation, the default is False, which means the configuration will replace existing one, thus removing the clearml handlers, and causing the issue I had.\nconf_logging = {\"version\":1, \n                \"incremental\": True\n                \"formatters\":{\n                      \"simple\":{\n                             \"format\":\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"}\n                      }\n                  }"
  },
  {
    "objectID": "posts/2021-03-17-pytest-data-test-truncated-error.html",
    "href": "posts/2021-03-17-pytest-data-test-truncated-error.html",
    "title": "Data Test as CI",
    "section": "",
    "text": "I am running test with great_expectations that validate data in UAT and production server with CI, so it would be nice if the log can capture this.\nI created a custom error class that would do the job, however, pytest truncated my AssertionError since it is quite long.\nI am using pytest magic from https://github.com/akaihola/ipython_pytest which allow me to run pytest in a Jupyter notebook cell.\nIt is quite simple with a few tens of lines.\n\n%%writefile ipython_pytest.py\nimport os\nimport shlex\nimport sys\nfrom pathlib import Path\n\nimport tempfile\nfrom IPython.core import magic\nfrom pytest import main as pytest_main\n\n\nTEST_MODULE_NAME = '_ipytesttmp'\n\ndef pytest(line, cell):\n    with tempfile.TemporaryDirectory() as root:\n        oldcwd = os.getcwd()\n        os.chdir(root)\n        tests_module_path = '{}.py'.format(TEST_MODULE_NAME)\n        try:\n            Path(tests_module_path).write_text(cell)\n            args = shlex.split(line)\n            os.environ['COLUMNS'] = '80'\n            pytest_main(args + [tests_module_path])\n            if TEST_MODULE_NAME in sys.modules:\n                del sys.modules[TEST_MODULE_NAME]\n        finally:\n            os.chdir(oldcwd)\n\ndef load_ipython_extension(ipython):\n    magic.register_cell_magic(pytest)\n\nWriting ipython_pytest.py\n\n\n\n# !pip install pytest\n%load_ext ipython_pytest\n\nThe ipython_pytest extension is already loaded. To reload it, use:\n  %reload_ext ipython_pytest\n\n\n\n%%pytest\n\ndef test_long_assertion_error():\n    x = \"placeholder\"\n    expect = \"abcdefg\\n\"*20 # Long string\n    assert x == expect\n\n============================= test session starts =============================\nplatform win32 -- Python 3.8.3, pytest-6.2.2, py-1.10.0, pluggy-0.13.1\nrootdir: C:\\Users\\channo\\AppData\\Local\\Temp\\tmpohw9e_9w\ncollected 1 item\n\n_ipytesttmp.py F                                                         [100%]\n\n================================== FAILURES ===================================\n__________________________ test_long_assertion_error __________________________\n\n    def test_long_assertion_error():\n        x = \"placeholder\"\n        expect = \"abcdefg\\n\"*20 # Long string\n>       assert x == expect\nE       AssertionError: assert 'placeholder' == 'abcdefg\\nabc...fg\\nabcdefg\\n'\nE         + placeholder\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg...\nE         \nE         ...Full output truncated (15 lines hidden), use '-vv' to show\n\n_ipytesttmp.py:5: AssertionError\n=========================== short test summary info ===========================\nFAILED _ipytesttmp.py::test_long_assertion_error - AssertionError: assert 'pl...\n============================== 1 failed in 0.06s ==============================\n\n\nYou can see that pytest truncated my error with ... Here is how I solve ths issue\n\n%%pytest -vv\n\ndef test_long_assertion_error():\n    x = \"placeholder\"\n    expect = \"abcdefg\\n\"*20 # Long string\n    assert x == expect\n\n============================= test session starts =============================\nplatform win32 -- Python 3.8.3, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- c:\\programdata\\miniconda3\\python.exe\ncachedir: .pytest_cache\nrootdir: C:\\Users\\channo\\AppData\\Local\\Temp\\tmpyic4vcra\ncollecting ... collected 1 item\n\n_ipytesttmp.py::test_long_assertion_error FAILED                         [100%]\n\n================================== FAILURES ===================================\n__________________________ test_long_assertion_error __________________________\n\n    def test_long_assertion_error():\n        x = \"placeholder\"\n        expect = \"abcdefg\\n\"*20 # Long string\n>       assert x == expect\nE       AssertionError: assert 'placeholder' == ('abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n')\nE         + placeholder\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\n\n_ipytesttmp.py:5: AssertionError\n=========================== short test summary info ===========================\nFAILED _ipytesttmp.py::test_long_assertion_error - AssertionError: assert 'pl...\n============================== 1 failed in 0.06s =============================="
  },
  {
    "objectID": "posts/2021-05-29-python-internal-series-python-gil-and-memory.html",
    "href": "posts/2021-05-29-python-internal-series-python-gil-and-memory.html",
    "title": "Python Internal Series - Global Interpreter Lock (GIL) and Memory Management",
    "section": "",
    "text": "Most people first learn about GIL because of how it slows down Python program and prevent multi-threading running efficiently, however, the GIL is one of the reason why Python survive 30 years and still growing healthyly.\nGIL is nothing like the stereotype people think, legacy, slow. There are multiple benefits GIL provide:\n\nIt speed ups single thread program.\nIt is compatible with many C Program thanks to the C API of CPysthon."
  },
  {
    "objectID": "posts/2021-05-29-python-internal-series-python-gil-and-memory.html#global-interpreter-lock-a.k.a-mutex-lock",
    "href": "posts/2021-05-29-python-internal-series-python-gil-and-memory.html#global-interpreter-lock-a.k.a-mutex-lock",
    "title": "Python Internal Series - Global Interpreter Lock (GIL) and Memory Management",
    "section": "Global Interpreter Lock a.k.a Mutex Lock",
    "text": "Global Interpreter Lock a.k.a Mutex Lock\nTo start with, GIL is a mutex lock."
  },
  {
    "objectID": "posts/2021-05-29-python-internal-series-python-gil-and-memory.html#why-gil-is-needed-in-the-first-place",
    "href": "posts/2021-05-29-python-internal-series-python-gil-and-memory.html#why-gil-is-needed-in-the-first-place",
    "title": "Python Internal Series - Global Interpreter Lock (GIL) and Memory Management",
    "section": "Why GIL is needed in the first place?",
    "text": "Why GIL is needed in the first place?\nMemory management. Python use something called “reference counting”, which make it different from many modern programming lanaguage. It is what allow Python programmer to lay back and let Python take care when to release memory. Precisely, it is actually the C program controlling the memory life cycle for Python (Cpython). Cpython is known as the default Python interpreter. It first compiles Python to intermediate bytecode (.pyc files). These bytecode then being interpreted by a virtual machine ane executed. It is worth to mention that other variants of Python exist, i.e. IronPython(C#), Jython(Java), Pypy(Python) and they have different memory management mechanisms.\n\nPython Memory Management - Reference Count & Garbage Collection (gc)\n\nimport sys\n\n\nsys.getrefcount(a)\n\n3\n\n\nReference counting is a simple idea. The intuition is that if a particular object is not referenced by anything, it can be recycled since it will not be used anymore.\nFor example, the list [1] is now referenced by the variable a, so the reference count is incremented by 1.\n\nimport sys\na = [1]\nsys.getrefcount(a)\n\n2\n\n\nNote that the reference count is 2 instead of 1. 1. The first reference is a = [1] 2. When the variable a is passed to sys.getrefcount(a) as an argument, it also increases the reference count.\n\ndel a\n\nWhen del a is called, the list [1] have 0 reference count, and it is collected by Python automatically behind the scene.\n\n\nLock & Deadlock\n\n\nMemory Management"
  },
  {
    "objectID": "posts/2021-07-10-5minutes-data-science-design-pattern-callback.html",
    "href": "posts/2021-07-10-5minutes-data-science-design-pattern-callback.html",
    "title": "5 Minutes Data Science Design Patterns I - Callback",
    "section": "",
    "text": "Note\n\n\n\nThese series are written as a quick introduction to software design for data scientists, something that is lightweight than the Design Pattern Bible - Clean Code I wish exists when I first started to learn. Design patterns refer to reusable solutions to some common problems, and some happen to be useful for data science. There is a good chance that someone else has solved your problem before. When used wisely, it helps to reduce the complexity of your code."
  },
  {
    "objectID": "posts/2021-07-10-5minutes-data-science-design-pattern-callback.html#so-what-is-callback-after-all",
    "href": "posts/2021-07-10-5minutes-data-science-design-pattern-callback.html#so-what-is-callback-after-all",
    "title": "5 Minutes Data Science Design Patterns I - Callback",
    "section": "So, What is Callback after all?",
    "text": "So, What is Callback after all?\nCallback function, or call after, simply means a function will be called after another function. It is a piece of executable code (function) that passed as an argument to another function. [1]\n\ndef foo(x, callback=None):\n    print('foo!')\n    if callback:\n        callback(x)\n\n\nfoo('123')\n\nfoo!\n\n\n\nfoo('123', print)\n\nfoo!\n123\n\n\nHere I pass the function print as a callback, hence the string 123 get printed after foo!."
  },
  {
    "objectID": "posts/2021-07-10-5minutes-data-science-design-pattern-callback.html#why-do-i-need-to-use-callback",
    "href": "posts/2021-07-10-5minutes-data-science-design-pattern-callback.html#why-do-i-need-to-use-callback",
    "title": "5 Minutes Data Science Design Patterns I - Callback",
    "section": "Why do I need to use Callback?",
    "text": "Why do I need to use Callback?\nCallback is very common in high-level deep learning libraries, most likely you will find them in the training loop. * fastai - fastai provide high-level API for PyTorch * Keras - the high-level API for Tensorflow * ignite - they use event & handler, which provides more flexibility in their opinion\n\nimport numpy as np\n\n# A boring training Loop\ndef train(x):\n    n_epochs = 3\n    n_batches = 2\n    loss = 20\n\n    for epoch in range(n_epochs):\n        for batch in range(n_batches):\n            loss = loss - 1  # Pretend we are training the model\n\n\nx = np.ones(10)\ntrain(x);\n\nSo, let’s say you now want to print the loss at the end of an epoch. You can just add 1 lines of code.\n\nThe simple approach\n\ndef train_with_print(x):\n    n_epochs = 3\n    n_batches = 2\n    loss = 20\n\n    for epoch in range(n_epochs):\n        for batch in range(n_batches):\n            loss = loss - 1 # Pretend we are training the model\n        print(f'End of Epoch. Epoch: {epoch}, Loss: {loss}')\n    return loss\n\n\ntrain_with_print(x);\n\nEnd of Epoch. Epoch: 0, Loss: 18\nEnd of Epoch. Epoch: 1, Loss: 16\nEnd of Epoch. Epoch: 2, Loss: 14\n\n\n\n\nCallback approach\nOr you call add a PrintCallback, which does the same thing but with a bit more code.\n\nclass Callback:\n    def on_epoch_start(self, x):\n        pass\n\n    def on_epoch_end(self, x):\n        pass\n\n    def on_batch_start(self, x):\n        pass\n\n    def on_batch_end(self, x):\n        pass\n\n\nclass PrintCallback(Callback):\n    def on_epoch_end(self, x):\n        print(f'End of Epoch. Loss: {x}')\n\n\ndef train_with_callback(x, callback=None):\n    n_epochs = 3\n    n_batches = 2\n    loss = 20\n\n    for epoch in range(n_epochs):\n\n        callback.on_epoch_start(loss)\n\n        for batch in range(n_batches):\n            callback.on_batch_start(loss)\n            loss = loss - 1  # Pretend we are training the model\n            callback.on_batch_end(loss)\n\n        callback.on_epoch_end(loss)\n\n\ntrain_with_callback(x, callback=PrintCallback());\n\nEnd of Epoch. Loss: 18\nEnd of Epoch. Loss: 16\nEnd of Epoch. Loss: 14\n\n\nUsually, a callback defines a few particular events on_xxx_xxx, which indicate that the function will be executed according to the corresponding condition. So all callbacks will inherit the base class Callback, and override the desired function, here we only implemented the on_epoch_end method because we only want to show the loss at the end.\nIt may seem awkward to write so many more code to do one simple thing, but there are good reasons. Consider now you need to add more features, how would you do it?\n\nModelCheckpoint\nEarly Stopping\nLearningRateScheduler\n\nYou can just add code in the loop, but it will start growing into a really giant function. It is impossible to test this function because it does 10 things at the same time. In addition, the extra code may not even be related to the training logic, they are just there to save the model or plot a chart. So, it is best to separate the logic. A function should only do 1 thing according to the Single Responsibility Principle. It helps you to reduce the complexity as it provides a nice abstraction, you are only modifying code within the specific callback you are interested.\n\n\nAdd some more sauce!\nWhen using the Callback Pattern, I can just implement a few more classes and the training loop is barely touched. Here we introduce a new class Callbacks because we need to execute more than 1 callback, it is used for holding all callbacks and executed them sequentially.\n\nclass Callbacks:\n    \"\"\"\n    It is the container for callback\n    \"\"\"\n\n    def __init__(self, callbacks):\n        self.callbacks = callbacks\n\n    def on_epoch_start(self, x):\n        for callback in self.callbacks:\n            callback.on_epoch_start(x)\n\n    def on_epoch_end(self, x):\n        for callback in self.callbacks:\n            callback.on_epoch_end(x)\n\n    def on_batch_start(self, x):\n        for callback in self.callbacks:\n            callback.on_batch_start(x)\n\n    def on_batch_end(self, x):\n        for callback in self.callbacks:\n            callback.on_batch_end(x)\n\nThen we implement the new Callback one by one, here we only have the pseudocode, but you should get the gist. For example, we only need to save the model at the end of an epoch, thus we implement the method on_epoch_end with a ModelCheckPoint callback.\n\nclass PrintCallback(Callback):\n    def on_epoch_end(self, x):\n        print(f'[{type(self).__name__}]: End of Epoch. Loss: {x}')\n\n\nclass ModelCheckPoint(Callback):\n    def on_epoch_end(self, x):\n        print(f'[{type(self).__name__}]: Save Model')\n\n\nclass EarlyStoppingCallback(Callback):\n    def on_epoch_end(self, x):\n        if x < 16:\n            print(f'[{type(self).__name__}]: Early Stopped')\n\n\nclass LearningRateScheduler(Callback):\n    def on_batch_end(self, x):\n        print(f'    [{type(self).__name__}]: Reduce learning rate')\n\nAnd we also modify the training loop a bit, the argument now takes a Callbacks which contain zero to many callbacks.\n\ndef train_with_callbacks(x, callbacks=None):\n    n_epochs = 2\n    n_batches = 3\n    loss = 20\n\n    for epoch in range(n_epochs):\n\n        callbacks.on_epoch_start(loss)                             # on_epoch_start\n        for batch in range(n_batches):\n            callbacks.on_batch_start(loss)                         # on_batch_start\n            loss = loss - 1  # Pretend we are training the model\n            callbacks.on_batch_end(loss)                           # on_batch_end\n        callbacks.on_epoch_end(loss)                               # on_epoch_end\n\n\ncallbacks = Callbacks([PrintCallback(), ModelCheckPoint(),\n                      EarlyStoppingCallback(), LearningRateScheduler()])\ntrain_with_callbacks(x, callbacks=callbacks)\n\n    [LearningRateScheduler]: Reduce learning rate\n    [LearningRateScheduler]: Reduce learning rate\n    [LearningRateScheduler]: Reduce learning rate\n[PrintCallback]: End of Epoch. Loss: 17\n[ModelCheckPoint]: Save Model\n    [LearningRateScheduler]: Reduce learning rate\n    [LearningRateScheduler]: Reduce learning rate\n    [LearningRateScheduler]: Reduce learning rate\n[PrintCallback]: End of Epoch. Loss: 14\n[ModelCheckPoint]: Save Model\n[EarlyStoppingCallback]: Early Stopped\n\n\nHopefully, it convinces you Callback makes the code cleaner and easier to maintain. If you just use plain if-else statements, you may end up with a big chunk of if-else clauses.\n\nfastai - fastai provide high-level API for PyTorch\nKeras - the high-level API for Tensorflow\nignite - they use event & handler, which provides more flexibility in their opinion"
  },
  {
    "objectID": "posts/2021-07-10-5minutes-data-science-design-pattern-callback.html#reference",
    "href": "posts/2021-07-10-5minutes-data-science-design-pattern-callback.html#reference",
    "title": "5 Minutes Data Science Design Patterns I - Callback",
    "section": "Reference",
    "text": "Reference\n\nhttps://stackoverflow.com/questions/824234/what-is-a-callback-function"
  },
  {
    "objectID": "posts/2020-02-09-MixUp-and-Beta-Distribution.html",
    "href": "posts/2020-02-09-MixUp-and-Beta-Distribution.html",
    "title": "data augmentation - Understand MixUp and Beta Distribution",
    "section": "",
    "text": "GitHub: https://github.com/noklam/notadatascientist/tree/master/demo/mixup-beta"
  },
  {
    "objectID": "posts/2020-02-09-MixUp-and-Beta-Distribution.html#beta-distribution",
    "href": "posts/2020-02-09-MixUp-and-Beta-Distribution.html#beta-distribution",
    "title": "data augmentation - Understand MixUp and Beta Distribution",
    "section": "Beta Distribution",
    "text": "Beta Distribution\nBeta distribution is control by two parameters, α and β with interval [0, 1], which make it useful for Mixup. Mixup is basically a superposition of two image with a parameter t. Instead of using a dog image, with Mixup, you may end up have a image which is 0.7 dog + 0.3 cat\nTo get some sense of what a beta distribution is, let plot beta distribution with different alpha and beta to see its effect\nimport math\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch import tensor\n# PyTorch has a log-gamma but not a gamma, so we'll create one\nΓ = lambda x: x.lgamma().exp()\nfacts = [math.factorial(i) for i in range(7)]\n\nplt.plot(range(7), facts, 'ro')\nplt.plot(torch.linspace(0,6), Γ(torch.linspace(0,6)+1))\nplt.legend(['factorial','Γ']);\n\n\n\npng"
  },
  {
    "objectID": "posts/2021-03-05-pyodbc-linux.html",
    "href": "posts/2021-03-05-pyodbc-linux.html",
    "title": "Setting up pyodbc for Impala connection, works on both Linux and Window",
    "section": "",
    "text": "Introduction\nLong story short, connect with Impala is a big headache in Windows. pyhive, impyla are both buggy. At the end, I stick with pyodbc as it works on both Linux and Windows, and seems to have better performance. There are not many steps, but it would be tricky if you try to Google as there are not much guide that just work out of the box\n\n\nSetup\nFirst, you need to download the ODBC driver from Cloudera.\nThen you need to instsall the driver properly.\ndpkg -i docker/clouderaimpalaodbc_2.6.10.1010-2_amd64.deb\nAdd this file to the directory /etc/odbcinst.ini, if you already have add, append this to the file.\n# /etc/odbcinst.ini\n[ODBC Drivers]\nCloudera Impala ODBC Driver 32-bit=Installed\nCloudera Impala ODBC Driver 64-bit=Installed\n[Cloudera Impala ODBC Driver 32-bit]\nDescription=Cloudera Impala ODBC Driver (32-bit)\nDriver=/opt/cloudera/impalaodbc/lib/32/libclouderaimpalaodbc32.so\n[Cloudera Impala ODBC Driver 64-bit]\nDescription=Cloudera Impala ODBC Driver (64-bit)\nDriver=/opt/cloudera/impalaodbc/lib/64/libclouderaimpalaodbc64.so\nThen install some additional package.\napt-get update && apt-get -y install gnupg apt-transport-https\napt-get update && apt-get -y install libssl1.0.0 unixodbc unixodbc-dev \\\n&& ACCEPT_EULA=Y apt-get -y install msodbcsql17\napt-get install unixodbc-dev -y\nLast, pip install pyodbc and have fun.\nTo read a database table, you can simply do this.\nimport pyodbc\nimport pandas as pd\n\nconn = pyodbc.connect(f\"\"\"\nDriver=Cloudera ODBC Driver for Impala 64-bit;\nPWD=password;\nUID=username;\nDatabase=database\n\"\"\")\nThere are multiple way to connect, but I found using a connection string is the most straight forward solution that does not require any additional enviornment variable setup."
  },
  {
    "objectID": "posts/2021-03-19-deepcopy-lightgbm-and-🥒pickles.html",
    "href": "posts/2021-03-19-deepcopy-lightgbm-and-🥒pickles.html",
    "title": "deepcopy, LGBM and pickle",
    "section": "",
    "text": "To start with, let’s look at some code to get some context."
  },
  {
    "objectID": "posts/2021-03-19-deepcopy-lightgbm-and-🥒pickles.html#deepcopy-or-no-copy",
    "href": "posts/2021-03-19-deepcopy-lightgbm-and-🥒pickles.html#deepcopy-or-no-copy",
    "title": "deepcopy, LGBM and pickle",
    "section": "deepcopy or no copy?",
    "text": "deepcopy or no copy?\n\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom copy import deepcopy\n\nparams = {\n'objective': 'regression',\n'verbose': -1,\n'num_leaves': 3\n}\n\nX = np.random.rand(100,2)\nY = np.ravel(np.random.rand(100,1))\nlgbm = lgb.train(params, lgb.Dataset(X,label=Y),num_boost_round=1)\nprint(\"Parameters of the model: \", lgbm.params)\n\nParameters of the model:  {'objective': 'regression', 'verbose': -1, 'num_leaves': 3, 'num_iterations': 1, 'early_stopping_round': None}\n\n\n\n## Deep copy will missing params\nnew_model = deepcopy(lgbm)\n\nFinished loading model, total used 1 iterations\n\n\nYou would expect new_model.parameters return the same dict right? Not quite.\n\nprint(\"Parameters of the copied model: \", new_model.params)\n\nParameters of the copied model:  {}\n\n\nSurprise, surprise. It’s an empty dict, where did the parameters go? To dive deep into the issue, let’s have a look at the source code of deepcopy to understand how does it work.\nreference: https://github.com/python/cpython/blob/e8e341993e3f80a3c456fb8e0219530c93c13151/Lib/copy.py#L128\ndef deepcopy(x, memo=None, _nil=[]):\n    \"\"\"Deep copy operation on arbitrary Python objects.\n    See the module's __doc__ string for more info.\n    \"\"\"\n\n    ... # skip some irrelevant code  \n\n    cls = type(x)\n\n    copier = _deepcopy_dispatch.get(cls)\n    if copier is not None:\n        y = copier(x, memo)\n    else:\n        if issubclass(cls, type):\n            y = _deepcopy_atomic(x, memo)\n        else:\n            copier = getattr(x, \"__deepcopy__\", None)\n            if copier is not None:\n                y = copier(memo)\n            else:\n                ... # skip irrelevant code\n\n    # If is its own copy, don't memoize.\n    if y is not x:\n        memo[d] = y\n        _keep_alive(x, memo) # Make sure x lives at least as long as d\n    return y\nIn particular, line 17 is what we care.\ncopier = getattr(x, \"__deepcopy__\", None)\nIf a particular class has implement the __deepcopy__ method, deepcopy will try to invoke that instead of the standard copy. The following dummy class should illustrate this clearly.\n\nclass DummyClass():\n    def __deepcopy__(self, _):\n        print('Just hanging around and not copying.')\n\n\no = DummyClass()\ndeepcopy(o)\n\nJust hanging around and not copying.\n\n\na lightgbm model is actually a Booster object and implement its own __deepcopy__. It only copy the model string but nothing else, this explains why deepcopy(lgbm).paramters is an empty dictionary.\n def __deepcopy__(self, _): \n     model_str = self.model_to_string(num_iteration=-1) \n     booster = Booster(model_str=model_str) \n     return booster \nReference: https://github.com/microsoft/LightGBM/blob/d6ebd063fff7ff9ed557c3f2bcacc8f9456583e6/python-package/lightgbm/basic.py#L2279-L2282\nOkay, so why lightgbm need to have an custom implementation? I thought this is a bug, but turns out there are some deeper reason behind this. I created an issue on GitHub.\nhttps://github.com/microsoft/LightGBM/issues/4085 Their response is > Custom deepcopy is needed to make Booster class picklable."
  },
  {
    "objectID": "posts/2021-03-19-deepcopy-lightgbm-and-🥒pickles.html#italian-bmt-lettuce-tomato-and-some-pickles-please",
    "href": "posts/2021-03-19-deepcopy-lightgbm-and-🥒pickles.html#italian-bmt-lettuce-tomato-and-some-pickles-please",
    "title": "deepcopy, LGBM and pickle",
    "section": "🥖Italian BMT, 🥬Lettuce 🍅 tomato and some 🥒pickles please",
    "text": "🥖Italian BMT, 🥬Lettuce 🍅 tomato and some 🥒pickles please\nWhat does pickle really is? and what makes an object pickable?\n\nPython Pickle is used to serialize and deserialize a python object structure. Any object on python can be pickled so that it can be saved on disk.\n\nSerialization roughly means translating the data in memory into a format that can be stored on disk or sent over network. It’s like ordering a chair from Ikea, they will send you a box, but not a chair.\nThe process of decomposing the chair and put it into a box is serialization, while putting it together is deserialization. With pickle terms, we called it Pickling and Unpickling.\n\n\n\n\n\ndeserialize and serialize\n\n\n\nWhat is Pickle\nPickle is a protocol for Python, you and either pickling a Python object to memory or to file.\n\nimport pickle\n\n\nd = {'a': 1}\npickle_d = pickle.dumps(d)\npickle_d\n\nb'\\x80\\x04\\x95\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00}\\x94\\x8c\\x01a\\x94K\\x01s.'\n\n\nThe python dict is now transfrom into a series of binary str, this string can be only understand by Python. We can also deserialize a binary string back to a python dict.\n\nbinary_str = b'\\x80\\x04\\x95\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00}\\x94\\x8c\\x01a\\x94K\\x01s.'\npickle.loads(binary_str)\n\n{'a': 1}\n\n\nReference: https://www.python.org/dev/peps/pep-0574/#:~:text=The%20pickle%20protocol%20was%20originally%20designed%20in%201995,copying%20temporary%20data%20before%20writing%20it%20to%20disk.\n\n\nWhat makes something picklable\nFinally, we come back to our initial questions. > What makes something picklable? Why lightgbm need to have deepcopy to make the Booster class picklable?\n\nWhat can be pickled and unpickled? The following types can be pickled:\n* None, True, and False\n* integers, floating point numbers, complex numbers\n* strings, bytes, bytearrays\n* tuples, lists, sets, and dictionaries containing only picklable objects\n* functions defined at the top level of a module (using def, not lambda)\n* built-in functions defined at the top level of a module\n* classes that are defined at the top level of a module\n\nSo pretty much common datatype, functions and classes are picklable. Let’s see without __deepcopy__, the Booster class is not serializable as it claims.\n\nimport lightgbm\nfrom lightgbm import Booster\ndel Booster.__deepcopy__\n\nparams = {\n'objective': 'regression',\n'verbose': -1,\n'num_leaves': 3\n}\n\nX = np.random.rand(100,2)\nY = np.ravel(np.random.rand(100,1))\nlgbm = lgb.train(params, lgb.Dataset(X,label=Y),num_boost_round=1)\n\n\ndeepcopy_lgbm = deepcopy(lgbm)\nlgbm.params, deepcopy_lgbm.params\n\n({'objective': 'regression',\n  'verbose': -1,\n  'num_leaves': 3,\n  'num_iterations': 1,\n  'early_stopping_round': None},\n {'objective': 'regression',\n  'verbose': -1,\n  'num_leaves': 3,\n  'num_iterations': 1,\n  'early_stopping_round': None})\n\n\n\npickle.dumps(deepcopy_lgbm) == pickle.dumps(lgbm)\n\nTrue\n\n\n\nunpickle_model = pickle.loads(pickle.dumps(deepcopy_lgbm))\nunpickle_deepcopy_model = pickle.loads(pickle.dumps(lgbm))\n\n\nunpickle_model.params, unpickle_deepcopy_model.params\n\n({'objective': 'regression',\n  'verbose': -1,\n  'num_leaves': 3,\n  'num_iterations': 1,\n  'early_stopping_round': None},\n {'objective': 'regression',\n  'verbose': -1,\n  'num_leaves': 3,\n  'num_iterations': 1,\n  'early_stopping_round': None})\n\n\n\nunpickle_model.model_to_string() == unpickle_deepcopy_model.model_to_string()\n\nTrue\n\n\n\nunpickle_deepcopy_model.predict(X)\n\narray([0.48439803, 0.48439803, 0.50141491, 0.48439803, 0.48439803,\n       0.48439803, 0.50141491, 0.48439803, 0.48439803, 0.48439803,\n       0.49029787, 0.49029787, 0.48439803, 0.48439803, 0.48439803,\n       0.49029787, 0.48439803, 0.50141491, 0.50141491, 0.50141491,\n       0.48439803, 0.50141491, 0.48439803, 0.49029787, 0.50141491,\n       0.50141491, 0.48439803, 0.49029787, 0.49029787, 0.49029787,\n       0.49029787, 0.50141491, 0.48439803, 0.50141491, 0.48439803,\n       0.49029787, 0.50141491, 0.48439803, 0.48439803, 0.48439803,\n       0.48439803, 0.50141491, 0.50141491, 0.48439803, 0.49029787,\n       0.48439803, 0.48439803, 0.50141491, 0.48439803, 0.48439803,\n       0.48439803, 0.48439803, 0.48439803, 0.48439803, 0.50141491,\n       0.49029787, 0.48439803, 0.50141491, 0.49029787, 0.49029787,\n       0.50141491, 0.50141491, 0.48439803, 0.50141491, 0.48439803,\n       0.48439803, 0.48439803, 0.48439803, 0.50141491, 0.48439803,\n       0.48439803, 0.50141491, 0.50141491, 0.49029787, 0.50141491,\n       0.48439803, 0.49029787, 0.48439803, 0.48439803, 0.50141491,\n       0.50141491, 0.48439803, 0.49029787, 0.48439803, 0.48439803,\n       0.50141491, 0.49029787, 0.50141491, 0.50141491, 0.49029787,\n       0.48439803, 0.49029787, 0.48439803, 0.48439803, 0.48439803,\n       0.48439803, 0.48439803, 0.48439803, 0.50141491, 0.49029787])\n\n\n\n\nLast Word\nWell…. It seems actually picklable? I may need to investigate the issue a bit more. For now, the __deepcopy__ does not seems to be necessary.\nI tried to dig into lightgbm source code and find this potential related issue. https://github.com/microsoft/LightGBM/blame/dc1bc23adf1137ef78722176e2da69f8411b1feb/python-package/lightgbm/basic.py#L2298"
  },
  {
    "objectID": "posts/2019-01-01-codespace-template.html",
    "href": "posts/2019-01-01-codespace-template.html",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "",
    "text": "Today, we are going to show you how to set up a literate programming environment, allowing you to use an IDE (VS Code) and an interactive computing environment (Jupyter), without leaving your browser, for free, in under 5 minutes. You’ll even see how VSCode and Jupyter work together automatically! But first, what is literate programming? And how did I go from skeptic to a zealot of literate programming?"
  },
  {
    "objectID": "posts/2019-01-01-codespace-template.html#introduction",
    "href": "posts/2019-01-01-codespace-template.html#introduction",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "Introduction",
    "text": "Introduction\n\nLiterate programming is a programming paradigm introduced by Donald Knuth in which a computer program is given an explanation of its logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which compilable source code can be generated. According to Knuth, literate programming provides higher-quality programs by forcing programmers to explicitly state the thoughts behind the program. This process makes poorly thought-out design decisions more obvious. Knuth also claims that literate programming provides a first-rate documentation system, which is not an add-on, but is grown naturally in the process of exposition of one’s thoughts during a program’s creation. 1\n\nWhen I first learned about literate programming, I was quite skeptical. For the longest time, I had wrongly equated Jupyter notebooks with literate programming. Indeed, Jupyter is a brilliant interactive computing system, which was awarded the Association of Computing Machinery (ACM) Software System Award, and is loved by many developers. However, Jupyter falls short of the literate programming paradigm for the following reasons:2\n\nIt can be difficult to compile source code from notebooks.\nIt can be difficult to diff and use version control with notebooks because they are not stored in plain text.\nIt is not clear how to automatically generate documentation from notebooks.\nIt is not clear how to properly run tests suites when writing code in notebooks.\n\nMy skepticism quickly evaporated when I began using nbdev, a project that extends notebooks to complete the literate programming ideal. I spent a month, full time, using nbdev while contributing to the python library fastcore, and can report that Donald Knuth was definitely onto something. The process of writing prose and tests alongside code forced me to deeply understand why the code does what it does, and to think deeply about its design. Furthermore, the reduced cognitive load and speed of iteration of having documentation, code, and tests in one location boosted my productivity to levels I have never before experienced as a software developer. Furthermore, I found that developing this way bolstered collaboration such that code reviews not only happened faster but were more meaningful. In short, nbdev may be the most profound productivity tool I have ever used.\nAs a teaser, look how easy it is to instantiate this literate programming environment, which includes a notebook, a docs site and an IDE with all dependencies pre-installed! :point_down:\n{% include video.html url=“https://github.com/machine-learning-apps/demo-videos/raw/master/codespaces-nbdev/e2e_small.mp4” %}"
  },
  {
    "objectID": "posts/2019-01-01-codespace-template.html#features-of-nbdev",
    "href": "posts/2019-01-01-codespace-template.html#features-of-nbdev",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "Features of nbdev",
    "text": "Features of nbdev\nAs discussed in the docs, nbdev provides the following features:\n\nSearchable, hyperlinked documentation, which can be automatically hosted on GitHub Pages for free.\nPython modules, following best practices such as automatically defining __all__ with your exported functions, classes, and variables.\nPip and Conda installers.\nTests defined directly in notebooks which run in parallel. This testing system has been thoroughly tested with GitHub Actions.\nNavigate and edit your code in a standard text editor or IDE, and export any changes automatically back into your notebooks.\n\nSince you are in a notebook, you can also add charts, text, links, images, videos, etc, that are included automatically in the documentation of your library, along with standardized documentation generated automatically from your code. This site is an example of docs generated automatically by nbdev."
  },
  {
    "objectID": "posts/2019-01-01-codespace-template.html#github-codespaces",
    "href": "posts/2019-01-01-codespace-template.html#github-codespaces",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "GitHub Codespaces",
    "text": "GitHub Codespaces\nThanks to Conda and nbdev_template, setting up a development environment with nbdev is far easier than it used to be. However, we realized it could be even easier, thanks to a new GitHub product called Codespaces. Codespaces is a fully functional development environment in your browser, accessible directly from GitHub, that provides the following features:\n\nA full VS Code IDE.\nAn environment that has files from the repository mounted into the environment, along with your GitHub credentials.\nA development environment with dependencies pre-installed, backed by Docker.\nThe ability to serve additional applications on arbitrary ports. For nbdev, we serve a Jupyter notebook server as well as a Jekyll based documentation site.\nA shared file system, which facilitates editing code in one browser tab and rendering the results in another.\n… and more.\n\nCodespaces enables developers to immediately participate in a project without wasting time on DevOps or complicated setup steps. Most importantly, CodeSpaces with nbdev allows developers to quickly get started with creating their own software with literate programming."
  },
  {
    "objectID": "posts/2019-01-01-codespace-template.html#a-demo-of-nbdev-codespaces",
    "href": "posts/2019-01-01-codespace-template.html#a-demo-of-nbdev-codespaces",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "A demo of nbdev + Codespaces",
    "text": "A demo of nbdev + Codespaces\nThis demo uses the project fastai/fastcore, which was built with nbdev, as an example. First, we can navigate to this repo and launch a Codespace:\n{% include video.html url=“https://github.com/machine-learning-apps/demo-videos/raw/master/codespaces-nbdev/1_open.mp4” %}\n\n\n\nIf you are launching a fresh Codespace, it may take several minutes to set up. Once the environment is ready, we can verify that all dependencies we want are installed (in this case fastcore and nbdev):\n{% include video.html url=“https://github.com/machine-learning-apps/demo-videos/raw/master/codespaces-nbdev/2_verify.mp4” %}\n\n\n\nAdditionally, we can serve an arbitrary number of applications on user-specified ports, which we can open through VSCode as shown below:\n{% include video.html url=“https://github.com/machine-learning-apps/demo-videos/raw/master/codespaces-nbdev/3_nb_small.mp4” %}\n\n\n\nIn this case, these applications are a notebook and docs site. Changes to a notebook are reflected immediately in the data docs. Furthermore, we can use the cli command nbdev_build_lib to sync our notebooks with python modules. This functionality is shown below:\n{% include video.html url=“https://github.com/machine-learning-apps/demo-videos/raw/master/codespaces-nbdev/4_reload_small.mp4” %}\n\n\n\nThis is amazing! With a click of a button, I was able to:\n\nLaunch an IDE with all dependencies pre-installed.\nLaunch two additional applications: a Jupyter Notebook server on port 8080 and a docs site on port 4000.\nAutomatically update the docs and modules every time I make a change to a Jupyter notebook.\n\nThis is just the tip of the iceberg. There are additional utilities for writing and executing tests, diffing notebooks, special flags for hiding, showing, and collapsing cells in the generated docs, as well as git hooks for automation. This and more functionality is covered in the nbdev docs."
  },
  {
    "objectID": "posts/2019-01-01-codespace-template.html#give-it-a-try-for-yourself",
    "href": "posts/2019-01-01-codespace-template.html#give-it-a-try-for-yourself",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "Give It A Try For Yourself",
    "text": "Give It A Try For Yourself\nTo try out nbdev yourself, take this tutorial, which will walk you through everything you need to know. The tutorial also shows you how to use a repository template with the configuration files necessary to enable Codespaces with nbdev."
  },
  {
    "objectID": "posts/2019-01-01-codespace-template.html#you-can-write-blogs-with-notebooks-too",
    "href": "posts/2019-01-01-codespace-template.html#you-can-write-blogs-with-notebooks-too",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "You Can Write Blogs With Notebooks, Too!",
    "text": "You Can Write Blogs With Notebooks, Too!\nThis blog post was written in fastpages which is also built on nbdev! We recommend fastpages if you want an easy way to blog with Jupyter notebooks."
  },
  {
    "objectID": "posts/2019-01-01-codespace-template.html#additional-resources",
    "href": "posts/2019-01-01-codespace-template.html#additional-resources",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nThe GitHub Codepaces site.\nThe official docs for Codespaces.\nThe nbdev docs.\nThe nbdev GitHub repo.\nfastpages: The project used to write this blog.\nThe GitHub repo fastai/fastcore, which is what we used in this blog post as an example."
  },
  {
    "objectID": "posts/2019-10-19-Deskto-Notification.html",
    "href": "posts/2019-10-19-Deskto-Notification.html",
    "title": "plyer - Desktop Notification with Python",
    "section": "",
    "text": "from plyer import notification\nimport random\n\nclass DesktopNotification:\n    @staticmethod\n    def notify(title='Hey~', message='Done!', timeout=10):\n        ls = ['👍','✔','✌','👌','👍','😎']\n        notification.notify(\n            title = title ,\n            message = random.choice(ls) * 3 + ' ' + message,\n            timeout = timeout # seconds\n        )\n\n\nif __name__ == '__main__':\n    DesktopNotification.notify()\nYou could add this simple code block to notify you when the program is done! A desktop notification will be prompt on the bottom right corner in Window."
  },
  {
    "objectID": "posts/2021-03-21-full-stack-deep-learning-lecture-01.html",
    "href": "posts/2021-03-21-full-stack-deep-learning-lecture-01.html",
    "title": "Full Stack Deep Learning Notes - Lecture 01",
    "section": "",
    "text": "Models become hardware agnostic\nCode is clear to read because engineering code is abstracted away\nEasier to reproduce\nMake fewer mistakes because lightning handles the tricky engineering\nKeeps all the flexibility (LightningModules are still PyTorch modules), but removes a ton of boilerplate\nLightning has dozens of integrations with popular machine learning tools.\nTested rigorously with every new PR. We test every combination of PyTorch and Python supported versions, every OS, multi GPUs and even TPUs.\nMinimal running speed overhead (about 300 ms per epoch compared with pure PyTorch)."
  },
  {
    "objectID": "posts/2021-04-16-full-stack-deep-learning-lecture-03.html",
    "href": "posts/2021-04-16-full-stack-deep-learning-lecture-03.html",
    "title": "Full Stack Deep Learning Notes - Lecture 03 - Recurrent Neural Network",
    "section": "",
    "text": "LSTM\nReference: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\nThe diagrams are from Chris Colah’s blog.\n\n\n\nRNN\nLSTM\n\n\n\n\n\n\n\n\n\n Forget Gate - Control the magnitude of cell state should be kept. Sigmoid range from (0 to 1). If 0, it means we should throw away the state cell, if 1 we keep everything.  * Input Gate - Control what relevant information can be added from the current step. It takes hidden step from last step and the current input into consideration.  * Output Gate - finalize the next hidden state\n\n\n# Google Neurl Machine Translation (GNMT)\nIt more or less follow the attention mechanism described here.\nhttps://blog.floydhub.com/attention-mechanism/#luong-att-step6\n\n\n\nattention_gnmt\n\n\n1.If you take the dot product of 1 encoder vector (at t_i) and decoder, you get a scalar. (Alignment Score) (1,h) * (h,1) -> (1,1) 2. If encoder have 5 time_step, repeat the above steps -> You get a vector with length of 5 (A vector of Alignment Scores) (5,h) (h,1) -> (5,1) 3. Take softmax of the alignments scores -> (attention weights which sum to 1) (5,1) 4. Take dot product of encoders state with attention weights (h, 5)  (5, 1) -> (h, 1), where h stands for dimension of hidden state. The result is a “Context Vector”"
  },
  {
    "objectID": "posts/python_single_dispatch/python-dispatch-typehint.html",
    "href": "posts/python_single_dispatch/python-dispatch-typehint.html",
    "title": "Function overloading - singledispatch in Python with type hint",
    "section": "",
    "text": "With Python>=3.7, the @singledispatch method can now understand the type hints. It behaves like function overloading but it’s more dynamic than the static langauge.\nHere is a quick example to demonstrate it.\n\nfrom functools import singledispatch\n\n@singledispatch\ndef foo(x):\n    print(\"foo\")\n\n\n@foo.register\ndef _(x: float):\n    print(\"It's a float\")\n\n\n@foo.register\ndef _(x: str):\n    print(\"It's a string now!\")\n\nLet’s see how it works.\n\nfoo(1)\n\nfoo\n\n\n\nfoo(1.0)\n\nIt's a float\n\n\n\nfoo(\"1\")\n\nIt's a string now!\n\n\nThe function foo now understand the type of the argument and dispatch the corresponding functions. This is nicer than a big chunk of if/else statement since it’s less couple. It’s also easy to extend this. Imagine the foo function is import from a package, it’s easy to extend it.\n\n# Imagine `foo` was imported from a package\n# Now that you have a special type and you want to extend it from your own library, you don't need to touch the source code at all.\n\n# from bar import foo\nclass Nok:\n    ...\n\n\n@foo.register\ndef _(x: Nok):\n    print(\"Nok\")\n\n\nnok = Nok()\nfoo(nok)\n\nNok\n\n\nThis is only possible because Python is a dynamic language. In contrast, to achieve the same functionalities with monkey patching, you would need to copy the source code of the function and extend the if/else block.\nLet’s dive a bit deeper to the decorator.\n\nprint([attr for attr in dir(foo) if not attr.startswith(\"_\")])\n\n['dispatch', 'register', 'registry']\n\n\n\nfoo.dispatch\n\n<function functools.singledispatch.<locals>.dispatch(cls)>\n\n\n\nfoo.register\n\n<function functools.singledispatch.<locals>.register(cls, func=None)>\n\n\n\nfoo.registry\n\nmappingproxy({object: <function __main__.foo(x)>,\n              float: <function __main__._(x: float)>,\n              str: <function __main__._(x: str)>,\n              __main__.Nok: <function __main__._(x: __main__.Nok)>,\n              __main__.Nok: <function __main__._(x: __main__.Nok)>})\n\n\n\nfrom collections import abc\nisinstance(foo.registry, abc.Mapping)\n\nThe foo.registry is the most interesting part. Basically, it’s a dictionary-like object which store the types. It behaves like\nif type(x) == \"int\":\n    do_something()\nelif type(x) == \"float\":\n    do_somthing_else()\nelse:\n    do_this_instead()"
  },
  {
    "objectID": "posts/2021-11-18-what-can-we-learn-from-shipping-crisis-as-a-data-scientist.html",
    "href": "posts/2021-11-18-what-can-we-learn-from-shipping-crisis-as-a-data-scientist.html",
    "title": "What can we learn from Shipping Crisis as a Data Scientist?",
    "section": "",
    "text": "Even if you are not working in shipping industry, you probably heard about shipping cost is skyrocking for the last year. COVID is clearly the initial disruption, but the story does not end there. Recently, Long Beach’s port congestion is at a historcial scale, there are now more than 70+ ships waiting outside the port, the typical number is 1 or 2.\nYou may think the terminal must be busy as hell, so did I, but it is actualy far from the truth. In fact, the port is actually paralyzed. The reason surprised me a lot, it is not because of lacking of driver or empty containers, but yard space. Container are being unloaded from ships, then they are being put at the container yard before they go into depot or being stuffed again.\nOn a high level, it is caused by a negative feedback loop which COVID probably contributed a lot, as it caused a lot of disruption to the supply chain.\n\nPort Congestion -> Containers pilled up at container yard since it is waiting to be loaded on ship\nContainer yard space is taken up by cotnainers, less space is available\nA container need to be put on a chassis before it is loaded, but as the container yard is full, empty containers stuck on the chassis and they need to be unloaded before you put a stuffed container.\nLess Chassis is available to load stuff, so it further slow down the process\nThe loop complete and it starts from 1 again\n\n\n\n\nPort Congestion Feedback Loop\n\n\nThis is a simplified story, you can find more details from this twitter thread from flexport’s CEO Ryan. There are more constraints that making this load/unload process inefficient, so the whole process is jammed. Think about a restaurant with limited amount of trays, you need to get a tray if you want to get food. But because there are too many customers, it jammed the door . So there are many customers holding an empty tray while many food are waiting to be served.\nRyan point out a very important lesson here, that is, you need to choose your bottleneck, and it should really be the capital intensive assets. Going back to our restaurant’s analogy, chef and space is probably the most expensive assets, so we should try to keep the utilization high. A simple solution is to buy more trays, so that it won’t be jammed. Ofcourse, you can also find a larger space, build a bigger door, but that will cost you more money too.\nFor shipping, the terminal’s crane should be the most capital intensive, so we should try our best to keep it working 24/7 to digest the terminal queue.\nThis is a simple idea yet it is powerful and it strikes me hard. As a data scientist, I work on optimization problem. To maximize the output of a system, we can use linear programming. When we are solving this problem, we are asking question like this.\n\nGiven x1 Terminals, x2 drivers, x3 containers, x4 ships, what is the maximize output of this system and how do you arrange them to achieve so?\n\nHowever, if you are a product/business analyst, a better question may be > What is the output of this system if I add more container yard space?\nBy changing the input of the system, you may achieve much better result. But as a data scientist, we often stuck in a mode that how do we optimize x metrics with these features. So we may end up spending months and try to schedule ships and driver perfectly to load 10% more container, but you can actually increase loading efficiency by 50% simply by adding more yard space. It feels like cheating as a scientific question, since this is not we asked originally, but this happened a lot in a business context.\nWe are not trying to find the best algorithm to solve a problem, the algorithm is just one way of doing it. We may get surprising result by just tweaking the asked question a little bit.\nI am curious about what is the limiting factor in our current supply chain system, and how sensitive it is to the environment. Is forecasting & optimization the right way to do it? Do we actually need a precise forecast or we can have a bit of redundancy (like in this case, having extra yard space which could be a waste but improve the system robustness)? This is questions that we need to ask ourselves constantly, as the true question is often not asked, but explored after lots of iterations. We need to, and we have to ask the right question, and that is an art more elegant than an algorithm in my opinion.\nI do not know if Ryan’s word are 100% true, but it reminds me an important lesson. The right solution (question) may be simple, but it may not be obvious. Have we exploited all the simple solution before we went nuts with fancy algorithms?\np.s. Apologised as I don’t have time to proofread but simply try to write down the snapshot of my current mind [2021-11-18]\n\nReference\n{% twitter https://twitter.com/typesfast/status/1451543776992845834?s=20 %} https://twitter.com/typesfast/status/1451543776992845834?s=20 https://www.facebook.com/669645890/posts/10159859049175891/ unroll version: https://threadreaderapp.com/thread/1451543776992845834.html"
  },
  {
    "objectID": "posts/2021-06-26-jupyter-sql-notebook.html",
    "href": "posts/2021-06-26-jupyter-sql-notebook.html",
    "title": "Jupyter Superpower - Extend SQL analysis with Python",
    "section": "",
    "text": "If you have ever written SQL queries to extract data from a database, chances are you are familiar with an IDE like the screenshot below. The IDE offers features like auto-completion, visualize the query output, display the table schema and the ER diagram. Whenever you need to write a query, this is your go-to tool. However, you may want to add Jupyter Notebook into your toolkit. It improves my productivity by complementing some missing features in IDE."
  },
  {
    "objectID": "posts/2021-06-26-jupyter-sql-notebook.html#notebook-as-a-self-contained-report",
    "href": "posts/2021-06-26-jupyter-sql-notebook.html#notebook-as-a-self-contained-report",
    "title": "Jupyter Superpower - Extend SQL analysis with Python",
    "section": "Notebook as a self-contained report",
    "text": "Notebook as a self-contained report\nAs a data scientist/data analyst, you write SQL queries for ad-hoc analyses all the time. After getting the right data, you make nice-looking charts and put them in a PowerPoint and you are ready to present your findings. Unlike a well-defined ETL job, you are exploring the data and testing your hypotheses all the time. You make assumptions, which is often wrong but you only realized it after a few weeks. But all you got is a CSV that you cannot recall how it was generated in the first place.\nData is not stationary, why should your analysis be? I have seen many screenshots, fragmented scripts flying around in organizations. As a data scientist, I learned that you need to be cautious about what you heard. Don’t trust peoples’ words easily, verify the result! To achieve that, we need to know exactly how the data was extracted, what kind of assumptions have been made? Unfortunately, this information usually is not available. As a result, people are redoing the same analysis over and over. You will be surprised that this is very common in organizations. In fact, numbers often do not align because every department has its own definition for a given metric. It is not shared among the organization, and verbal communication is inaccurate and error-prone. It would be really nice if anyone in the organization can reproduce the same result with just a single click. Jupyter Notebook can achieve that reproducibility and keep your entire analysis (documentation, data, and code) in the same place."
  },
  {
    "objectID": "posts/2021-06-26-jupyter-sql-notebook.html#notebook-as-an-extension-of-ide",
    "href": "posts/2021-06-26-jupyter-sql-notebook.html#notebook-as-an-extension-of-ide",
    "title": "Jupyter Superpower - Extend SQL analysis with Python",
    "section": "Notebook as an extension of IDE",
    "text": "Notebook as an extension of IDE\nWriting SQL queries in a notebook gives you extra flexibility of a full programming language alongside SQL. For example:\n\nWrite complex processing logic that is not easy in pure SQL\nCreate visualizations directly from SQL results without exporting to an intermediate CSV\n\nFor instance, you can pipe your SQL query with pandas and then make a plot. It allows you to generate analysis with richer content. If you find bugs in your code, you can modify the code and re-run the analysis. This reduces the hustles to reproduce an analysis greatly. In contrast, if your analysis is reading data from an anonymous exported CSV, it is almost guaranteed that the definition of the data will be lost. No one will be able to reproduce the dataset.\nYou can make use of the ipython_sql library to make queries in a notebook. To do this, you need to use the magic function with the inline magic % or cell magic %%.\n\nsales = %sql SELECT * from sales LIMIT 3\nsales\n\n\n\n    \n        ProductId\n        Unit\n        IsDeleted\n    \n    \n        1\n        10\n        1\n    \n    \n        1\n        10\n        1\n    \n    \n        2\n        10\n        0\n    \n\n\n\nTo make it fancier, you can even parameterize your query with variables. Tools like papermill allows you to parameterize your notebook. If you execute the notebook regularly with a scheduler, you can get a updated dashboard. To reference the python variable, the $ sign is used.\n\ntable = \"sales\"\nquery = f\"SELECT * from {table} LIMIT 3\"\nsales = %sql $query\nsales\n\n\n\n    \n        ProductId\n        Unit\n        IsDeleted\n    \n    \n        1\n        10\n        1\n    \n    \n        1\n        10\n        1\n    \n    \n        2\n        10\n        0\n    \n\n\n\nWith a little bit of python code, you can make a nice plot to summarize your finding. You can even make an interactive plot if you want. This is a very powerful way to extend your analysis.\n\nimport seaborn as sns\nsales = %sql SELECT * FROM SALES\nsales_df = sales.DataFrame()\nsales_df = sales_df.groupby('ProductId', as_index=False).sum()\nax = sns.barplot(x='ProductId', y='Unit', data=sales_df)\nax.set_title('Sales by ProductId');"
  },
  {
    "objectID": "posts/2021-06-26-jupyter-sql-notebook.html#notebook-as-a-collaboration-tool",
    "href": "posts/2021-06-26-jupyter-sql-notebook.html#notebook-as-a-collaboration-tool",
    "title": "Jupyter Superpower - Extend SQL analysis with Python",
    "section": "Notebook as a collaboration tool",
    "text": "Notebook as a collaboration tool\nJupyter Notebook is flexible and it fits extremely well with exploratory data analysis. To share to a non-coder, you can share the notebook or export it as an HTML file. They can read the report or any cached executed result. If they need to verify the data or add some extra plots, they can do it easily themselves.\nIt is true that Jupyter Notebook has an infamous reputation. It is not friendly to version control, it’s hard to collaborate with notebooks. Luckily, there are efforts that make collaboration in notebook a lot easier now.\nHere what I did not show you is that the table has an isDeleted column. Some of the records are invalid and we should exclude them. In reality, this happens frequently when you are dealing with hundreds of tables that you are not familiar with. These tables are made for applications, transactions, and they do not have analytic in mind. Data Analytic is usually an afterthought. Therefore, you need to consult the SME or the maintainer of that tables. It takes many iterations to get the correct data that can be used to produce useful insight.\nWith ReviewNB, you can publish your result and invite some domain expert to review your analysis. This is where notebook shine, this kind of workflow is not possible with just the SQL script or a screenshot of your finding. The notebook itself is a useful documentation and collaboration tool.\n\nStep 1 - Review PR online\n\n\n\n\n\nStep1\n\n\nYou can view your notebook and add comments on a particular cell on ReviewNB. This lowers the technical barrier as your analysts do not have to understand Git. He can review changes and make comments on the web without the need to pull code at all. As soon as your analyst makes a suggestion, you can make changes.\n\n\nStep 2 - Review Changes\n\n\n\n\n\nStep2\n\n\nOnce you have made changes to the notebook, you can review it side by side. This is very trivial to do it in your local machine. Without ReviewNB, you have to pull both notebooks separately. As Git tracks line-level changes, you can’t really read the changes as it consists of a lot of confusing noise. It would also be impossible to view changes about the chart with git.\n\n\nStep 3 - Resolve Discussion\n\n\n\n\n\nStep3\n\n\nOnce the changes are reviewed, you can resolve the discussion and share your insight with the team. You can publish the notebook to internal sharing platform like knowledge-repo to organize the analysis.\nI hope this convince you that Notebook is a good choice for adhoc analytics. It is possible to collaborate with notebook with proper software in place. Regarless if you use notebook or not, you should try your best to document the process. Let’s make more reproducible analyses!"
  },
  {
    "objectID": "posts/python_warning/2023-07-08-python-warning-test-version-switch-mock.html",
    "href": "posts/python_warning/2023-07-08-python-warning-test-version-switch-mock.html",
    "title": "Mocking Python version for testing, use of Python Warning to raise error for specific Python Version",
    "section": "",
    "text": "Background\nTo release a new open bound version of library so it can be instsalled in any Python versio, while making sure that users are aware this is not supported yet. This article explains well Why setting upper bound for version is a bad idea because it is not supported yet . TL;DR, if you cannot install it, you cannot even test if it works or not. In Python ecosystem, you relies on many libraries and it will take long time until all depedencies are updated, which in reality most likely it has not break anything at all.\n\n\nThe Approach\nWe use warnings and simplefilter. To your surprise, warnings can be triggered as an Exception with the flag -W, i.e. python my_program.py -W UserWarnings or using the Python environment variable PYTHONWARNINGS\nSee the standard Python Docs about warnings.warn yourself > warnings.warn(message, category=None, stacklevel=1, source=None) Issue a warning, or maybe ignore it or raise an exception.\n(TBD, I am still working on the solution but I just need to write it down to document my state of mind lol)"
  },
  {
    "objectID": "posts/2021-03-27-microsoft-azure-dp100.html",
    "href": "posts/2021-03-27-microsoft-azure-dp100.html",
    "title": "Microsoft Azure - DP100",
    "section": "",
    "text": "Last Updated: 2021-04-22"
  },
  {
    "objectID": "posts/2021-03-27-microsoft-azure-dp100.html#official-suggested-materials",
    "href": "posts/2021-03-27-microsoft-azure-dp100.html#official-suggested-materials",
    "title": "Microsoft Azure - DP100",
    "section": "Official Suggested Materials",
    "text": "Official Suggested Materials\n\n❗ https://docs.microsoft.com/en-us/learn/paths/build-ai-solutions-with-azure-ml-service/ - This should be your main focus, try to finish the labs and read the tutorials. You need to understand the use case of different products and familiar yourself with the syntax of Azure ML SDK etc.\nhttps://docs.microsoft.com/en-us/learn/paths/create-no-code-predictive-models-azure-machine-learning/ - You should at least finish 1 of the lab to get some sense of the UI. It would be included in the exam for sure (2-3 questions maybe)\nhttps://docs.microsoft.com/en-us/learn/paths/create-machine-learn-models/ - I didn’t spend much time on it as most of them are baisc data science concepts. You would need to how to apply different types of models (Regression/Classification/Time Series) & AutoML for given scenario."
  },
  {
    "objectID": "posts/2021-03-27-microsoft-azure-dp100.html#compute-target",
    "href": "posts/2021-03-27-microsoft-azure-dp100.html#compute-target",
    "title": "Microsoft Azure - DP100",
    "section": "Compute Target",
    "text": "Compute Target\nMachine Learning Studio - single/multi - Development/Experiment - Local Machine/Cloud VM. - Scale up to larger data/distributed - training compute target - Azure Machine Learning compute cluster - Azure Machine Learning compute instance - Deploy Compute Target (You need to able to judge the most appropiate option based on the context.) - Local web service - Azure Kubernetes Service (AKS) - Azure Container Instances - Azure Machine Learning compute clusters (Batch Inference)"
  },
  {
    "objectID": "posts/2021-03-27-microsoft-azure-dp100.html#datastore",
    "href": "posts/2021-03-27-microsoft-azure-dp100.html#datastore",
    "title": "Microsoft Azure - DP100",
    "section": "DataStore",
    "text": "DataStore\n\nAzure Storage (blob and file containers)\nAzure Data Lake stores\nAzure SQL Database\nAzure Databricks file system (DBFS)"
  },
  {
    "objectID": "posts/2020-02-08-Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html",
    "href": "posts/2020-02-08-Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html",
    "title": "Hydra - Config Composition for Machine Learning Project",
    "section": "",
    "text": "GitHub: https://github.com/noklam/notadatascientist/tree/master/demo/hydra-example\nMachine learning project involves large number of hyperparmeters. In many case you could have multiple config, e.g. differnet dataset, database connection, train/test mode. hydra provide a simple Command Line Interface that is useful for composing different experiment configs. In essence, it compose different files to a large config setting. It offers you the common Object Oriented Programming with YAML file. Allow you to have clear structure of configurations.\nAssume you have a config.yaml like this, where run_mode and hyperparmeter are separate folder to hold different choice of parameters. You can set defaults for them with the following structure."
  },
  {
    "objectID": "posts/2020-02-08-Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html#folder-structure",
    "href": "posts/2020-02-08-Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html#folder-structure",
    "title": "Hydra - Config Composition for Machine Learning Project",
    "section": "Folder Structure",
    "text": "Folder Structure\nconfig.yaml\ndemo.py\nrun_mode\n  - train.yaml\n  - test.yaml\nhyperparmeter\n  - base.yaml"
  },
  {
    "objectID": "posts/2020-02-08-Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html#config.yaml",
    "href": "posts/2020-02-08-Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html#config.yaml",
    "title": "Hydra - Config Composition for Machine Learning Project",
    "section": "config.yaml",
    "text": "config.yaml\ndefaults:\n - run_mode: train\n - hyperparameter: base\nThe benefit of using such approach is that it makes comparsion of experiments much easier. Instead of going through the parameters list, you only focus on the argument(the difference). It helps organize machine learning results and ease a lot of pain in tracking the model performance.\nimport hydra\nfrom omegaconf import DictConfig\n@hydra.main(config_path=\"config.yaml\")\ndef my_app(cfg : DictConfig) -> None:\n    print(cfg.pretty())\nif __name__ == \"__main__\":\n    my_app()\npython demo.py \ngamma: 0.01\nlearning_rate: 0.01\nrun_mode: train\nweek: 8\nFor example, with a simple example with 4 parameters only, you can simply run the experiment with default"
  },
  {
    "objectID": "posts/2022-02-10-journey-of-understanding-python-and-programming-langauge.html",
    "href": "posts/2022-02-10-journey-of-understanding-python-and-programming-langauge.html",
    "title": "Journey of understanding Python and programming language",
    "section": "",
    "text": "To be written… # What is Python Interpreter?\n\nWhat is Bytecode?\n\n\nPython Virtual Machine\n\n\nCompiler\n\n\nEBNF Grammar\n\n\nLLVM"
  },
  {
    "objectID": "posts/2021-08-18-python-file-not-found-long-file-path-window.html",
    "href": "posts/2021-08-18-python-file-not-found-long-file-path-window.html",
    "title": "Python FileNotFoundError or You have a really long file path?",
    "section": "",
    "text": "FileNotFoundError? Not so quick\n\n\n\n\n\nscreenshot\n\n\nTo illustrate the issue, I perpared some fake file. The script is simple, it just read a file with plain text, except that the filename is really long.\n\n\n\n\n\nerror\n\n\nUnforuntately, even though the file exists, Python gives me a FileNotFoundError, how come? However long debugging, I found out that this is related to the filename that exist only on Windows.\nThis StackOverflow thread explain this issue.\n\nMaximum Path Length Limitation\nIn the Windows API (with some exceptions discussed in the following paragraphs), the maximum length for a path is MAX_PATH, which is defined as 260 characters. A local path is structured in the following order: drive letter, colon, backslash, name components separated by backslashes, and a terminating null character. For example, the maximum path on drive D is “D:*some 256-character path string*” where “” represents the invisible terminating null character for the current system codepage. (The characters < > are used here for visual clarity and cannot be part of a valid path string.)\n\n\n\nSolution - Registry\nUpdating your Registry can solve this problem.\n\n\n\n\n\nHello World\n\n\nAfter applying the config, I can finally read the file. :)\n\n\nSummary (TLDR version)\n\nWindow filesystem only allow 256 characters, beyond that you will have trouble to open the file.\nPython will not be able to see this file and throw FileNotFoundError (I have no idea, anyone know why is that?)\nYou can update registry to enable long file path in Window to fix this issue.\n\n(Bonus: Window actually has weird behavior for long filepath, you can try to break it with different ways.)"
  },
  {
    "objectID": "posts/pandas_expert/kedro-meta-analysis.html",
    "href": "posts/pandas_expert/kedro-meta-analysis.html",
    "title": "Understanding the Kedro codebase - A quick dirty meta-analysis - (Part I)",
    "section": "",
    "text": "Inspired by this talk\n\nHow many lines of code in Kedro?\n\nfrom pathlib import Path\nimport pandas as pd\nfrom collections import Counter\n\n\nREPO_PATH = Path(\"/Users/Nok_Lam_Chan/GitHub/kedro\")\nlist(REPO_PATH.iterdir())\n\n[PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test_requirements.txt'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/CODE_OF_CONDUCT.md'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/LICENSE.md'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tools'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro_technical_charter.pdf'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.DS_Store'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.pytest_cache'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/derby.log'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro.egg-info'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.pre-commit-config.yaml'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.coverage'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/Makefile'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/CITATION.cff'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/CODEOWNERS'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/pyproject.toml'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/trufflehog-ignore.txt'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/dependency'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/MANIFEST.in'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/docs'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.readthedocs.yml'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/dep_tree.log'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/README.md'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/RELEASE.md'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/setup.py'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/demo-project'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/logs'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.mypy_cache'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.gitignore'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/static'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/CONTRIBUTING.md'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/behave.ini'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.github'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.gitpod.yml'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/info.log'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/coverage.xml'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/errors.log'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.git'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/htmlcov'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.vscode'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/data'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/conf'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.circleci'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/import.log'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/notebooks'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.run'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/.idea'),\n PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/src')]\n\n\n\ndef count_effective_line(counter, fn):\n    with open (fn) as f:\n        for line in f:\n            counter[fn] += 1\n\n\nlines_count = Counter()\nfor fn in REPO_PATH.rglob(\"*/*.py\"):\n#     print(fn)\n    count_effective_line(lines_count, fn)\nprint(lines_count)\n            \n\nCounter({PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/spark/test_spark_dataset.py'): 984, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/pipeline/test_pipeline.py'): 940, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/pipeline/pipeline.py'): 926, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/session/test_session.py'): 891, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/micropkg.py'): 854, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/micropkg/test_micropkg_pull.py'): 846, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/io/core.py'): 748, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/test_cli.py'): 730, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/io/test_data_catalog.py'): 685, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/test_starters.py'): 639, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/cli_steps.py'): 623, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/pipeline/node.py'): 612, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/docs/conf.py'): 598, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/io/data_catalog.py'): 594, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/docs/build/conf.py'): 587, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/micropkg/test_micropkg_package.py'): 581, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/session/test_session_extension_hooks.py'): 576, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/io/test_partitioned_dataset.py'): 565, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/starters.py'): 552, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/io/partitioned_dataset.py'): 551, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/pipeline/test_pipeline.py'): 522, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/io/test_incremental_dataset.py'): 503, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/context/test_context.py'): 485, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/config/test_templated_config.py'): 482, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/test_project.py'): 479, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/test_jupyter.py'): 470, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/utils.py'): 469, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/runner.py'): 456, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/tensorflow/test_tensorflow_model_dataset.py'): 441, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/sql_dataset.py'): 438, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/matplotlib/test_matplotlib_writer.py'): 436, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/pipeline/test_node.py'): 434, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/session/session.py'): 423, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/spark/spark_dataset.py'): 422, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/pipeline/test_modular_pipeline.py'): 418, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/runner/test_parallel_runner.py'): 401, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/project.py'): 392, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/pipeline/test_pipeline_with_transcoding.py'): 391, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_generic_dataset.py'): 383, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/session/conftest.py'): 381, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_sql_dataset.py'): 374, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/project/__init__.py'): 369, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/config/test_config.py'): 354, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/parallel_runner.py'): 353, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/context/context.py'): 345, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_parquet_dataset.py'): 344, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/pipeline.py'): 336, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_gbq_dataset.py'): 315, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/spark/test_spark_hive_dataset.py'): 314, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/gbq_dataset.py'): 314, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/test_catalog.py'): 305, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/ipython/test_ipython.py'): 304, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_csv_dataset.py'): 300, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/hooks/specs.py'): 296, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/pipeline/modular_pipeline.py'): 290, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/jupyter.py'): 282, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_excel_dataset.py'): 281, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/config/templated_config.py'): 281, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/build/lib/resume_kedro/dagascii.py'): 275, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/runner/test_sequential_runner.py'): 273, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pickle/test_pickle_dataset.py'): 269, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/micropkg/test_micropkg_requirements.py'): 266, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/project/test_pipeline_discovery.py'): 260, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/excel_dataset.py'): 254, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/test_startup.py'): 250, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/config/common.py'): 248, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/generic_dataset.py'): 246, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_hdf_dataset.py'): 245, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/matplotlib/matplotlib_writer.py'): 243, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pickle/pickle_dataset.py'): 243, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_xml_dataset.py'): 241, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_json_dataset.py'): 241, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/geojson/test_geojson_dataset.py'): 232, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pillow/test_image_dataset.py'): 231, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/parquet_dataset.py'): 230, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/pipeline/test_pipeline_from_missing.py'): 227, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/io/test_memory_dataset.py'): 226, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/networkx/test_json_dataset.py'): 226, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/email/test_message_dataset.py'): 226, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/docs/source/conf.py'): 225, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/docs/source/conf.py'): 225, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/docs/source/conf.py'): 225, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test/docs/source/conf.py'): 224, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/project/{{ cookiecutter.repo_name }}/docs/source/conf.py'): 224, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/docs/source/conf.py'): 222, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/holoviews/test_holoviews_writer.py'): 220, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/pandas/test_feather_dataset.py'): 220, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/spark/spark_hive_dataset.py'): 220, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/runner/test_thread_runner.py'): 213, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/cli.py'): 211, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/yaml/test_yaml_dataset.py'): 210, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/hdf_dataset.py'): 204, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/json/test_json_dataset.py'): 200, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/io/test_lambda_dataset.py'): 194, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/tracking/test_metrics_dataset.py'): 194, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/csv_dataset.py'): 194, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/feather_dataset.py'): 191, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/redis/redis_dataset.py'): 189, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/networkx/test_gml_dataset.py'): 188, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/networkx/test_graphml_dataset.py'): 188, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/tensorflow/tensorflow_model_dataset.py'): 188, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/email/message_dataset.py'): 188, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/text/test_text_dataset.py'): 187, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/json_dataset.py'): 187, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/tracking/test_json_dataset.py'): 185, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/catalog.py'): 176, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/spark/spark_jdbc_dataset.py'): 175, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/xml_dataset.py'): 171, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/api/test_api_dataset.py'): 170, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/runner/conftest.py'): 168, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/redis/test_redis_dataset.py'): 165, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/ipython/__init__.py'): 164, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/dask/test_parquet_dataset.py'): 162, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/json/json_dataset.py'): 160, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/conftest.py'): 159, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/geopandas/geojson_dataset.py'): 157, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/startup.py'): 156, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/thread_runner.py'): 156, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/yaml/yaml_dataset.py'): 155, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/tools/test_cli.py'): 154, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/plotly/json_dataset.py'): 154, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/networkx/json_dataset.py'): 150, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/networkx/gml_dataset.py'): 145, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/networkx/graphml_dataset.py'): 143, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/io/test_cached_dataset.py'): 142, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pillow/image_dataset.py'): 142, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/api/api_dataset.py'): 142, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/pipeline/test_node_run.py'): 141, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/biosequence/biosequence_dataset.py'): 137, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/holoviews/holoviews_writer.py'): 137, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/dask/parquet_dataset.py'): 136, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/config/config.py'): 134, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/io/memory_dataset.py'): 132, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/text/text_dataset.py'): 131, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/environment.py'): 128, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/test_cli_hooks.py'): 128, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/session/test_session_hook_manager.py'): 126, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/spark/test_spark_jdbc_dataset.py'): 121, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/plotly/plotly_dataset.py'): 117, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/io/cached_dataset.py'): 113, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/io/lambda_dataset.py'): 113, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/plotly/test_plotly_dataset.py'): 108, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/spark/deltatable_dataset.py'): 108, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/bioinformatics/test_biosequence_dataset.py'): 107, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/hooks/manager.py'): 106, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/sh_run.py'): 105, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/project/test_settings.py'): 102, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/plotly/test_json_dataset.py'): 101, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/io/test_core.py'): 96, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/logging/color_logger.py'): 95, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/conftest.py'): 89, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/session/test_store.py'): 89, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/spark/test_deltatable_dataset.py'): 89, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/test_registry.py'): 88, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/sequential_runner.py'): 87, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/util.py'): 84, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/pipeline/test_pipeline_integration.py'): 84, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/pipeline/conftest.py'): 84, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipelines/data_science/nodes.py'): 80, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/project/test_pipeline_registry.py'): 79, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/micropkg/conftest.py'): 79, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/hooks/test_manager.py'): 75, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/iris/nodes.py'): 74, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/iris_demo/nodes.py'): 74, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/resume_kedro/nodes.py'): 74, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/build/lib/resume_kedro/nodes.py'): 74, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/tracking/metrics_dataset.py'): 68, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/spark/test_memory_dataset.py'): 67, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tools/cli.py'): 62, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/iris_demo/settings.py'): 62, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/project/test_logging.py'): 58, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/build/lib/resume_kedro/settings.py'): 56, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipelines/data_engineering/nodes.py'): 51, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/registry.py'): 50, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/hooks/manager.py'): 49, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test/src/test/__main__.py'): 47, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/iris/__main__.py'): 47, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/iris_demo/__main__.py'): 47, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/__main__.py'): 47, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/resume_kedro/__main__.py'): 47, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/build/lib/resume_kedro/__main__.py'): 47, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/project/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/__main__.py'): 47, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/tracking/json_dataset.py'): 47, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/hooks/specs.py'): 46, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/session/shelvestore.py'): 43, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test/src/tests/test_run.py'): 41, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/spark/conftest.py'): 41, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/project/{{ cookiecutter.repo_name }}/src/tests/test_run.py'): 41, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/tests/test_run.py'): 40, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test/src/setup.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/tests/test_run.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/setup.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/tests/test_run.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/setup.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/settings.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/tests/test_run.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/resume_kedro/pipeline.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/config/abstract_config.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/session/store.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/project/{{ cookiecutter.repo_name }}/src/setup.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/project/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/settings.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pandas/__init__.py'): 39, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/build/lib/resume_kedro/pipeline.py'): 38, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test/src/test/settings.py'): 37, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/setup.py'): 37, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/iris/settings.py'): 37, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/setup.py'): 37, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/datasets/conftest.py'): 35, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/iris/pipeline.py'): 33, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/iris_demo/pipeline.py'): 33, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/io/__init__.py'): 33, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/test_utils.py'): 30, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/utils.py'): 28, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_plugin/plugin.py'): 27, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipelines/data_science/pipeline.py'): 27, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tools/circleci/github_scripts/kedro_version.py'): 26, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipelines/data_engineering/pipeline.py'): 26, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/settings.py'): 24, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/framework/cli/hooks/test_manager.py'): 22, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/extensions/ipython.py'): 22, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipeline_registry.py'): 19, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/config/__init__.py'): 19, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/iris/pipeline_registry.py'): 18, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/iris_demo/pipeline_registry.py'): 18, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/resume_kedro/pipeline_registry.py'): 18, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/build/lib/resume_kedro/pipeline_registry.py'): 18, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/tests/extras/logging/test_color_logger.py'): 16, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/runner/__init__.py'): 16, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/project/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipeline_registry.py'): 16, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/logging/__init__.py'): 15, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/networkx/__init__.py'): 15, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/spark/__init__.py'): 14, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test/src/test/pipeline_registry.py'): 13, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_plugin/setup.py'): 12, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/__init__.py'): 11, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/hooks/markers.py'): 11, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/plotly/__init__.py'): 11, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/api/__init__.py'): 11, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/tracking/__init__.py'): 11, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/__main__.py'): 10, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/hooks/markers.py'): 10, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/pipeline/{{ cookiecutter.pipeline_name }}/__init__.py'): 10, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/pipeline/{{ cookiecutter.pipeline_name }}/pipeline.py'): 10, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/tests/test_pipeline.py'): 9, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/tests/test_pipeline.py'): 9, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/tests/test_pipeline.py'): 9, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/pipeline/__init__.py'): 9, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/pipeline/{{ cookiecutter.pipeline_name }}/tests/test_pipeline.py'): 9, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/dask/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/redis/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/geopandas/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pillow/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/json/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/biosequence/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/tensorflow/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/matplotlib/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/yaml/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/pickle/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/text/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/holoviews/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/email/__init__.py'): 8, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipelines/data_science/__init__.py'): 7, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/pipelines/data_engineering/__init__.py'): 7, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/context/__init__.py'): 7, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/__init__.py'): 7, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/session/__init__.py'): 6, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/hooks/__init__.py'): 5, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/cli/hooks/__init__.py'): 5, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/test/src/test/__init__.py'): 4, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris/src/iris/__init__.py'): 4, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/iris-demo/src/iris_demo/__init__.py'): 4, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/features/steps/test_starter/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/__init__.py'): 4, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/resume_kedro/__init__.py'): 4, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/build/lib/resume_kedro/__init__.py'): 4, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/pipeline/{{ cookiecutter.pipeline_name }}/nodes.py'): 4, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/templates/project/{{ cookiecutter.repo_name }}/src/{{ cookiecutter.python_package }}/__init__.py'): 4, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/resume-kedro/src/resume_kedro/settings.py'): 3, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/datasets/__init__.py'): 3, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/extensions/__init__.py'): 3, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/extras/__init__.py'): 2, PosixPath('/Users/Nok_Lam_Chan/GitHub/kedro/kedro/framework/__init__.py'): 1})\n\n\n\n\nClean up the dictionary a little bit\n\nline_counts_df = pd.DataFrame(lines_count.items(), columns=[\"fullpath\",\"line_of_code\"])\nline_counts_df[\"fullpath\"] = line_counts_df[\"fullpath\"].apply(str)\nline_counts_df[\"fullpath\"] =  line_counts_df[\"fullpath\"].str.replace(\"/Users/Nok_Lam_Chan/GitHub/kedro/\", \"\")\nline_counts_df.head(2)\n\n\n\n\n\n  \n    \n      \n      fullpath\n      line_of_code\n    \n  \n  \n    \n      0\n      tools/cli.py\n      62\n    \n    \n      1\n      features/environment.py\n      128\n    \n  \n\n\n\n\n\nline_counts_df[[\"toplevel\",\"module\",\"submodule\",\"filename\"]] = line_counts_df[\"fullpath\"].str.split(\"/\",expand=True, n=3)\n\n\nline_counts_df\n\n\n\n\n\n  \n    \n      \n      fullpath\n      line_of_code\n      toplevel\n      module\n      submodule\n      filename\n    \n  \n  \n    \n      0\n      tools/cli.py\n      62\n      tools\n      cli.py\n      None\n      None\n    \n    \n      1\n      features/environment.py\n      128\n      features\n      environment.py\n      None\n      None\n    \n    \n      2\n      tests/test_utils.py\n      30\n      tests\n      test_utils.py\n      None\n      None\n    \n    \n      3\n      tests/conftest.py\n      89\n      tests\n      conftest.py\n      None\n      None\n    \n    \n      4\n      docs/conf.py\n      598\n      docs\n      conf.py\n      None\n      None\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      276\n      kedro/extras/datasets/pandas/feather_dataset.py\n      191\n      kedro\n      extras\n      datasets\n      pandas/feather_dataset.py\n    \n    \n      277\n      kedro/extras/datasets/pandas/hdf_dataset.py\n      204\n      kedro\n      extras\n      datasets\n      pandas/hdf_dataset.py\n    \n    \n      278\n      kedro/extras/datasets/pandas/csv_dataset.py\n      194\n      kedro\n      extras\n      datasets\n      pandas/csv_dataset.py\n    \n    \n      279\n      kedro/extras/datasets/pandas/excel_dataset.py\n      254\n      kedro\n      extras\n      datasets\n      pandas/excel_dataset.py\n    \n    \n      280\n      kedro/extras/datasets/pandas/gbq_dataset.py\n      314\n      kedro\n      extras\n      datasets\n      pandas/gbq_dataset.py\n    \n  \n\n281 rows × 6 columns\n\n\n\n\n## Sort by Top level module\nline_counts_df.groupby([\"toplevel\"]).sum().sort_values(ascending=False, by =\"line_of_code\")\n\n\n\n\n\n  \n    \n      \n      line_of_code\n    \n    \n      toplevel\n      \n    \n  \n  \n    \n      tests\n      25341\n    \n    \n      kedro\n      18683\n    \n    \n      features\n      1587\n    \n    \n      docs\n      1185\n    \n    \n      resume-kedro\n      1007\n    \n    \n      iris-demo\n      550\n    \n    \n      iris\n      547\n    \n    \n      test\n      405\n    \n    \n      tools\n      88\n    \n  \n\n\n\n\nInterstingly we have roughly a 1:1 ratio between tests and kedro\n\nline_counts_df.groupby([\"module\",\"submodule\"]).sum().sort_values(ascending=False, by =\"line_of_code\")\n\n\n\n\n\n  \n    \n      \n      \n      line_of_code\n    \n    \n      module\n      submodule\n      \n    \n  \n  \n    \n      extras\n      datasets\n      15775\n    \n    \n      framework\n      cli\n      8837\n    \n    \n      session\n      2574\n    \n    \n      pipeline\n      test_pipeline.py\n      940\n    \n    \n      pipeline.py\n      926\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      config\n      __init__.py\n      19\n    \n    \n      runner\n      __init__.py\n      16\n    \n    \n      pipeline\n      __init__.py\n      9\n    \n    \n      extras\n      __init__.py\n      2\n    \n    \n      framework\n      __init__.py\n      1\n    \n  \n\n74 rows × 1 columns\n\n\n\n\n## Sort by Sub-module\nkedro_line_counts_df = line_counts_df[line_counts_df[\"toplevel\"] == \"kedro\"]\ntmp = kedro_line_counts_df.groupby(\"module\").sum().rename(mapper={\"line_of_code\": \"module_line_of_code\"},axis=1 )\nkedro_line_counts_df_group = kedro_line_counts_df.groupby([\"module\",\"submodule\"]).sum().reset_index().merge(tmp, left_on=\"module\", right_on=\"module\")\n\n\n# .sort_values(ascending=False, by =\"line_of_code\")\n\n\nkedro_line_counts_df.groupby([\"module\"]).sum().sort_values(ascending=False, by =\"line_of_code\")\n\n\n\n\n\n  \n    \n      \n      line_of_code\n    \n    \n      module\n      \n    \n  \n  \n    \n      extras\n      6871\n    \n    \n      framework\n      5246\n    \n    \n      io\n      2284\n    \n    \n      pipeline\n      1837\n    \n    \n      runner\n      1068\n    \n    \n      config\n      721\n    \n    \n      templates\n      443\n    \n    \n      ipython\n      164\n    \n    \n      utils.py\n      28\n    \n    \n      __init__.py\n      11\n    \n    \n      __main__.py\n      10\n    \n  \n\n\n\n\n\n# Sort by file \nkedro_line_counts_df_group.sort_values(ascending=False, by =[\"module_line_of_code\",\"line_of_code\"])\n\n\n\n\n\n  \n    \n      \n      module\n      submodule\n      line_of_code\n      module_line_of_code\n    \n  \n  \n    \n      6\n      extras\n      datasets\n      6734\n      6871\n    \n    \n      8\n      extras\n      logging\n      110\n      6871\n    \n    \n      7\n      extras\n      extensions\n      25\n      6871\n    \n    \n      5\n      extras\n      __init__.py\n      2\n      6871\n    \n    \n      10\n      framework\n      cli\n      3439\n      5246\n    \n    \n      14\n      framework\n      session\n      511\n      5246\n    \n    \n      12\n      framework\n      hooks\n      418\n      5246\n    \n    \n      13\n      framework\n      project\n      369\n      5246\n    \n    \n      11\n      framework\n      context\n      352\n      5246\n    \n    \n      15\n      framework\n      startup.py\n      156\n      5246\n    \n    \n      9\n      framework\n      __init__.py\n      1\n      5246\n    \n    \n      18\n      io\n      core.py\n      748\n      2284\n    \n    \n      19\n      io\n      data_catalog.py\n      594\n      2284\n    \n    \n      22\n      io\n      partitioned_dataset.py\n      551\n      2284\n    \n    \n      21\n      io\n      memory_dataset.py\n      132\n      2284\n    \n    \n      17\n      io\n      cached_dataset.py\n      113\n      2284\n    \n    \n      20\n      io\n      lambda_dataset.py\n      113\n      2284\n    \n    \n      16\n      io\n      __init__.py\n      33\n      2284\n    \n    \n      27\n      pipeline\n      pipeline.py\n      926\n      1837\n    \n    \n      26\n      pipeline\n      node.py\n      612\n      1837\n    \n    \n      25\n      pipeline\n      modular_pipeline.py\n      290\n      1837\n    \n    \n      24\n      pipeline\n      __init__.py\n      9\n      1837\n    \n    \n      30\n      runner\n      runner.py\n      456\n      1068\n    \n    \n      29\n      runner\n      parallel_runner.py\n      353\n      1068\n    \n    \n      32\n      runner\n      thread_runner.py\n      156\n      1068\n    \n    \n      31\n      runner\n      sequential_runner.py\n      87\n      1068\n    \n    \n      28\n      runner\n      __init__.py\n      16\n      1068\n    \n    \n      4\n      config\n      templated_config.py\n      281\n      721\n    \n    \n      2\n      config\n      common.py\n      248\n      721\n    \n    \n      3\n      config\n      config.py\n      134\n      721\n    \n    \n      1\n      config\n      abstract_config.py\n      39\n      721\n    \n    \n      0\n      config\n      __init__.py\n      19\n      721\n    \n    \n      34\n      templates\n      project\n      410\n      443\n    \n    \n      33\n      templates\n      pipeline\n      33\n      443\n    \n    \n      23\n      ipython\n      __init__.py\n      164\n      164\n    \n  \n\n\n\n\n\n# Total number of LOC\nkedro_line_counts_df[\"line_of_code\"].sum()\n\n18683\n\n\n\n\nConclusion\nThe kedro codebase is not huge, roughly 20000 line of code, compare to pandas which has > 250000 of code, 10x smaller. The datasets and framework code is the largest module which isn’t surprise to me. The more surprising is how small config actually is, but it creates huge complexity in terms of a kedro project. The cli is also relatively huge as it takes ~3000 lines of code which I didn’t expected."
  },
  {
    "objectID": "posts/kedro_duckdb/kedro-duckdb.html",
    "href": "posts/kedro_duckdb/kedro-duckdb.html",
    "title": "Kedro DuckDB",
    "section": "",
    "text": "Exploring DuckDB and how can we use it with kedro"
  },
  {
    "objectID": "posts/kedro_duckdb/kedro-duckdb.html#practical-sql-for-data-analysis",
    "href": "posts/kedro_duckdb/kedro-duckdb.html#practical-sql-for-data-analysis",
    "title": "Kedro DuckDB",
    "section": "Practical SQL for Data Analysis",
    "text": "Practical SQL for Data Analysis\n\nWhat you can do together with Pandas\n\n!pip install --quiet duckdb\n\n\nimport pandas as pd\nimport numpy as np\n\nimport sqlite3\nimport duckdb\nimport time\n\nPreparation\nDownload the data and set up the Pandas data frames. We read the data into a Pandas DataFrame using DuckDB’s built-in Parquet reader.\n\n!wget -q https://github.com/cwida/duckdb-data/releases/download/v1.0/lineitemsf1.snappy.parquet\n\n  HTTP/1.1 301 Moved Permanently\n  Server: GitHub.com\n  Date: Tue, 15 Nov 2022 22:34:17 GMT\n  Content-Type: text/html; charset=utf-8\n  Vary: X-PJAX, X-PJAX-Container, Turbo-Visit, Turbo-Frame, Accept-Encoding, Accept, X-Requested-With\n  Location: https://github.com/duckdb/duckdb-data/releases/download/v1.0/lineitemsf1.snappy.parquet\n  Cache-Control: no-cache\n  Strict-Transport-Security: max-age=31536000; includeSubdomains; preload\n  X-Frame-Options: deny\n  X-Content-Type-Options: nosniff\n  X-XSS-Protection: 0\n  Referrer-Policy: origin-when-cross-origin, strict-origin-when-cross-origin\n  Content-Security-Policy: default-src 'none'; base-uri 'self'; block-all-mixed-content; child-src github.com/assets-cdn/worker/ gist.github.com/assets-cdn/worker/; connect-src 'self' uploads.github.com objects-origin.githubusercontent.com www.githubstatus.com collector.github.com raw.githubusercontent.com api.github.com github-cloud.s3.amazonaws.com github-production-repository-file-5c1aeb.s3.amazonaws.com github-production-upload-manifest-file-7fdce7.s3.amazonaws.com github-production-user-asset-6210df.s3.amazonaws.com cdn.optimizely.com logx.optimizely.com/v1/events *.actions.githubusercontent.com wss://*.actions.githubusercontent.com online.visualstudio.com/api/v1/locations github-production-repository-image-32fea6.s3.amazonaws.com github-production-release-asset-2e65be.s3.amazonaws.com insights.github.com wss://alive.github.com; font-src github.githubassets.com; form-action 'self' github.com gist.github.com objects-origin.githubusercontent.com; frame-ancestors 'none'; frame-src viewscreen.githubusercontent.com notebooks.githubusercontent.com; img-src 'self' data: github.githubassets.com media.githubusercontent.com camo.githubusercontent.com identicons.github.com avatars.githubusercontent.com github-cloud.s3.amazonaws.com objects.githubusercontent.com objects-origin.githubusercontent.com secured-user-images.githubusercontent.com/ opengraph.githubassets.com github-production-user-asset-6210df.s3.amazonaws.com customer-stories-feed.github.com spotlights-feed.github.com *.githubusercontent.com; manifest-src 'self'; media-src github.com user-images.githubusercontent.com/ secured-user-images.githubusercontent.com/; script-src github.githubassets.com; style-src 'unsafe-inline' github.githubassets.com; worker-src github.com/assets-cdn/worker/ gist.github.com/assets-cdn/worker/\n  Content-Length: 0\n  X-GitHub-Request-Id: D47F:F2DE:2F3E29B:303AC52:637413E9\n  HTTP/1.1 302 Found\n  Server: GitHub.com\n  Date: Tue, 15 Nov 2022 22:34:17 GMT\n  Content-Type: text/html; charset=utf-8\n  Vary: X-PJAX, X-PJAX-Container, Turbo-Visit, Turbo-Frame, Accept-Encoding, Accept, X-Requested-With\n  Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/263853960/33e88e80-95cb-11ea-8bb7-2dfa0654592c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221115%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221115T223417Z&X-Amz-Expires=300&X-Amz-Signature=07d1673053f9e8676510f46b62993e3b9b2428a17f00a613162f67690318e82f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=263853960&response-content-disposition=attachment%3B%20filename%3Dlineitemsf1.snappy.parquet&response-content-type=application%2Foctet-stream\n  Cache-Control: no-cache\n  Strict-Transport-Security: max-age=31536000; includeSubdomains; preload\n  X-Frame-Options: deny\n  X-Content-Type-Options: nosniff\n  X-XSS-Protection: 0\n  Referrer-Policy: no-referrer-when-downgrade\n  Content-Security-Policy: default-src 'none'; base-uri 'self'; block-all-mixed-content; child-src github.com/assets-cdn/worker/ gist.github.com/assets-cdn/worker/; connect-src 'self' uploads.github.com objects-origin.githubusercontent.com www.githubstatus.com collector.github.com raw.githubusercontent.com api.github.com github-cloud.s3.amazonaws.com github-production-repository-file-5c1aeb.s3.amazonaws.com github-production-upload-manifest-file-7fdce7.s3.amazonaws.com github-production-user-asset-6210df.s3.amazonaws.com cdn.optimizely.com logx.optimizely.com/v1/events *.actions.githubusercontent.com wss://*.actions.githubusercontent.com online.visualstudio.com/api/v1/locations github-production-repository-image-32fea6.s3.amazonaws.com github-production-release-asset-2e65be.s3.amazonaws.com insights.github.com wss://alive.github.com; font-src github.githubassets.com; form-action 'self' github.com gist.github.com objects-origin.githubusercontent.com; frame-ancestors 'none'; frame-src viewscreen.githubusercontent.com notebooks.githubusercontent.com; img-src 'self' data: github.githubassets.com media.githubusercontent.com camo.githubusercontent.com identicons.github.com avatars.githubusercontent.com github-cloud.s3.amazonaws.com objects.githubusercontent.com objects-origin.githubusercontent.com secured-user-images.githubusercontent.com/ opengraph.githubassets.com github-production-user-asset-6210df.s3.amazonaws.com customer-stories-feed.github.com spotlights-feed.github.com *.githubusercontent.com; manifest-src 'self'; media-src github.com user-images.githubusercontent.com/ secured-user-images.githubusercontent.com/; script-src github.githubassets.com; style-src 'unsafe-inline' github.githubassets.com; worker-src github.com/assets-cdn/worker/ gist.github.com/assets-cdn/worker/\n  Content-Length: 0\n  X-GitHub-Request-Id: D47F:F2DE:2F3E323:303ACEF:637413E9\n  HTTP/1.1 200 OK\n  Connection: keep-alive\n  Content-Length: 206368635\n  Content-Type: application/octet-stream\n  Last-Modified: Tue, 07 Dec 2021 13:35:44 GMT\n  ETag: \"0x8D9B986787C89B4\"\n  Server: Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0\n  x-ms-request-id: b588900b-a01e-0060-6d42-f95efa000000\n  x-ms-version: 2020-04-08\n  x-ms-creation-time: Tue, 17 Aug 2021 11:28:44 GMT\n  x-ms-lease-status: unlocked\n  x-ms-lease-state: available\n  x-ms-blob-type: BlockBlob\n  Content-Disposition: attachment; filename=lineitemsf1.snappy.parquet\n  x-ms-server-encrypted: true\n  Fastly-Restarts: 1\n  Accept-Ranges: bytes\n  Age: 0\n  Date: Tue, 15 Nov 2022 22:34:18 GMT\n  Via: 1.1 varnish\n  X-Served-By: cache-lhr7337-LHR\n  X-Cache: MISS\n  X-Cache-Hits: 0\n  X-Timer: S1668551658.108887,VS0,VE259\n\n\n\n!wget -q https://github.com/cwida/duckdb-data/releases/download/v1.0/orders.parquet\n\n\n%%time\nlineitem = duckdb.query(\"SELECT * FROM 'lineitemsf1.snappy.parquet'\").to_df()\norders = duckdb.query(\"SELECT * FROM 'orders.parquet'\").to_df()\n\nCPU times: user 7.62 s, sys: 5.43 s, total: 13 s\nWall time: 14.1 s\n\n\n\n%%time\n_ = pd.read_parquet(\"lineitemsf1.snappy.parquet\")\n_ = pd.read_parquet(\"orders.parquet\")\n\nCPU times: user 6.29 s, sys: 1.5 s, total: 7.78 s\nWall time: 5.81 s\n\n\n\ncon = duckdb.connect()\ncon.execute('PRAGMA threads=2')\n\ndef timeit(fun, name):\n    import time\n    start_time = time.monotonic()\n    fun()\n    return [name, time.monotonic() - start_time]\n\ndef plot_results(results, title):\n  df = pd.DataFrame.from_dict({\n      'name': [x[0] for x in results],\n      'time': [x[1] for x in results]\n  })\n  print(title)\n  print(df)\n\nUngrouped Aggregates\nThis performs a simple set of ungrouped aggregates (sum, min, max, avg) over a column without any filters or other complex operations.\n\nungrouped_aggregate = '''\n    SELECT SUM(l_extendedprice), MIN(l_extendedprice), MAX(l_extendedprice), AVG(l_extendedprice) FROM lineitem\n'''\n\ndef duckdb_ungrouped_aggregate(d_con):\n    print(d_con.query(ungrouped_aggregate).to_df())\n\ndef duckdb_ungrouped_aggregate_1t():\n    duckdb_ungrouped_aggregate(duckdb)\n\ndef duckdb_ungrouped_aggregate_2t():\n    duckdb_ungrouped_aggregate(con)\n\ndef pandas_ungrouped_aggregate():\n  result = lineitem.groupby(\n    ['l_returnflag', 'l_linestatus']\n  ).agg(\n    Sum=('l_extendedprice', 'sum'),\n    Min=('l_extendedprice', 'min'),\n    Max=('l_extendedprice', 'max'),\n    Avg=('l_extendedprice', 'mean')\n  )\n  print(result)\n    # print(lineitem.agg(Sum=('l_extendedprice', 'sum'), Min=('l_extendedprice', 'min'), Max=('l_extendedprice', 'max'), Avg=('l_extendedprice', 'mean')))\n\nua_results = []\nua_results.append(timeit(duckdb_ungrouped_aggregate_1t, 'DuckDB (1T)'))\nua_results.append(timeit(duckdb_ungrouped_aggregate_2t, 'DuckDB (2T)'))\nua_results.append(timeit(pandas_ungrouped_aggregate, 'Pandas'))\nplot_results(ua_results, 'Ungrouped Aggregate')\n\n   sum(l_extendedprice)  min(l_extendedprice)  max(l_extendedprice)  \\\n0          2.295773e+11                 901.0              104949.5   \n\n   avg(l_extendedprice)  \n0          38255.138485  \n   sum(l_extendedprice)  min(l_extendedprice)  max(l_extendedprice)  \\\n0          2.295773e+11                 901.0              104949.5   \n\n   avg(l_extendedprice)  \n0          38255.138485  \n                                    Sum    Min       Max           Avg\nl_returnflag l_linestatus                                             \nA            F             5.658655e+10  904.0  104949.5  38273.129735\nN            F             1.487505e+09  920.0  104049.5  38284.467761\n             O             1.149352e+11  901.0  104749.5  38248.015609\nR            F             5.656804e+10  904.0  104899.5  38250.854626\nUngrouped Aggregate\n          name      time\n0  DuckDB (1T)  0.052544\n1  DuckDB (2T)  0.066239\n2       Pandas  0.801278\n\n\nGrouped Aggregates\nThis performs the same set of aggregates, but this time grouped by two other columns (l_returnflag and l_linestatus).\n\ngrouped_aggregate = '''\nSELECT l_returnflag,\n       l_linestatus,\n       SUM(l_extendedprice),\n       MIN(l_extendedprice),\n       MAX(l_extendedprice),\n       AVG(l_extendedprice)\nFROM lineitem\nGROUP BY l_returnflag,\n         l_linestatus\n'''\n\ndef duckdb_grouped_aggregate(d_con):\n    print(d_con.query(grouped_aggregate).to_df())\n\ndef duckdb_grouped_aggregate_1t():\n    duckdb_grouped_aggregate(duckdb)\n\ndef duckdb_grouped_aggregate_2t():\n    duckdb_grouped_aggregate(con)\n\ndef pandas_grouped_aggregate():\n    print(lineitem.groupby(['l_returnflag', 'l_linestatus']).agg(Sum=('l_extendedprice', 'sum'), Min=('l_extendedprice', 'min'), Max=('l_extendedprice', 'max'), Avg=('l_extendedprice', 'mean')))\n\nresults = []\nresults.append(timeit(duckdb_grouped_aggregate_1t, 'DuckDB (1T)'))\nresults.append(timeit(duckdb_grouped_aggregate_2t, 'DuckDB (2T)'))\nresults.append(timeit(pandas_grouped_aggregate, 'Pandas'))\nplot_results(results, 'Grouped Aggregate')\n\n  l_returnflag l_linestatus  sum(l_extendedprice)  min(l_extendedprice)  \\\n0            N            O          1.149352e+11                 901.0   \n1            R            F          5.656804e+10                 904.0   \n2            A            F          5.658655e+10                 904.0   \n3            N            F          1.487505e+09                 920.0   \n\n   max(l_extendedprice)  avg(l_extendedprice)  \n0              104749.5          38248.015609  \n1              104899.5          38250.854626  \n2              104949.5          38273.129735  \n3              104049.5          38284.467761  \n  l_returnflag l_linestatus  sum(l_extendedprice)  min(l_extendedprice)  \\\n0            N            O          1.149352e+11                 901.0   \n1            R            F          5.656804e+10                 904.0   \n2            A            F          5.658655e+10                 904.0   \n3            N            F          1.487505e+09                 920.0   \n\n   max(l_extendedprice)  avg(l_extendedprice)  \n0              104749.5          38248.015609  \n1              104899.5          38250.854626  \n2              104949.5          38273.129735  \n3              104049.5          38284.467761  \n                                    Sum    Min       Max           Avg\nl_returnflag l_linestatus                                             \nA            F             5.658655e+10  904.0  104949.5  38273.129735\nN            F             1.487505e+09  920.0  104049.5  38284.467761\n             O             1.149352e+11  901.0  104749.5  38248.015609\nR            F             5.656804e+10  904.0  104899.5  38250.854626\nGrouped Aggregate\n          name      time\n0  DuckDB (1T)  0.115463\n1  DuckDB (2T)  0.222520\n2       Pandas  0.708696\n\n\nGrouped Aggregate with a Filter\nThis benchmark performs a grouped aggregate with a filter over the shipdate column.\nAs Pandas does not perform any projection pushdown, we include a version where we manually perform the projection pushdown by filtering only the columns we actually need before running the filter and aggregate.\nThis optimization is performed automatically in DuckDB by the query optimizer.\n\ndef duckdb_grouped_aggregate_filter(d_con):\n    print(d_con.query('''\nSELECT l_returnflag,\n       l_linestatus,\n       SUM(l_extendedprice),\n       MIN(l_extendedprice),\n       MAX(l_extendedprice),\n       AVG(l_extendedprice)\nFROM lineitem\nWHERE\n    l_shipdate <= DATE '1998-09-02'\nGROUP BY l_returnflag,\n         l_linestatus\n''').to_df())\n\ndef duckdb_grouped_aggregate_filter_1t():\n    duckdb_grouped_aggregate_filter(duckdb)\n\ndef duckdb_grouped_aggregate_filter_2t():\n    duckdb_grouped_aggregate_filter(con)\n\ndef pandas_grouped_aggregate_filter():\n  filtered_df = lineitem[lineitem['l_shipdate'] < \"1998-09-02\"]\n  print(filtered_df.groupby(['l_returnflag', 'l_linestatus']).agg(Sum=('l_extendedprice', 'sum'), Min=('l_extendedprice', 'min'), Max=('l_extendedprice', 'max'), Avg=('l_extendedprice', 'mean')))\n\ndef pandas_grouped_aggregate_filter_projection_pushdown():\n  pushed_down_df = lineitem[['l_shipdate', 'l_returnflag', 'l_linestatus', 'l_extendedprice']]\n  filtered_df = pushed_down_df[pushed_down_df['l_shipdate'] < \"1998-09-02\"]\n  print(filtered_df.groupby(['l_returnflag', 'l_linestatus']).agg(Sum=('l_extendedprice', 'sum'), Min=('l_extendedprice', 'min'), Max=('l_extendedprice', 'max'), Avg=('l_extendedprice', 'mean')))\n\nresults = []\nresults.append(timeit(duckdb_grouped_aggregate_filter_1t, 'DuckDB (1T)'))\nresults.append(timeit(duckdb_grouped_aggregate_filter_2t, 'DuckDB (2T)'))\nresults.append(timeit(pandas_grouped_aggregate_filter, 'Pandas'))\nresults.append(timeit(pandas_grouped_aggregate_filter_projection_pushdown, 'Pandas (manual pushdown)'))\nplot_results(results, 'Grouped Aggregate + Filter')\n\n  l_returnflag l_linestatus  sum(l_extendedprice)  min(l_extendedprice)  \\\n0            N            O          1.117017e+11                 901.0   \n1            A            F          5.658655e+10                 904.0   \n2            R            F          5.656804e+10                 904.0   \n3            N            F          1.487505e+09                 920.0   \n\n   max(l_extendedprice)  avg(l_extendedprice)  \n0              104749.5          38249.117989  \n1              104949.5          38273.129735  \n2              104899.5          38250.854626  \n3              104049.5          38284.467761  \n  l_returnflag l_linestatus  sum(l_extendedprice)  min(l_extendedprice)  \\\n0            N            O          1.117017e+11                 901.0   \n1            A            F          5.658655e+10                 904.0   \n2            R            F          5.656804e+10                 904.0   \n3            N            F          1.487505e+09                 920.0   \n\n   max(l_extendedprice)  avg(l_extendedprice)  \n0              104749.5          38249.117989  \n1              104949.5          38273.129735  \n2              104899.5          38250.854626  \n3              104049.5          38284.467761  \n                                    Sum    Min       Max           Avg\nl_returnflag l_linestatus                                             \nA            F             5.658655e+10  904.0  104949.5  38273.129735\nN            F             1.487505e+09  920.0  104049.5  38284.467761\n             O             1.116318e+11  901.0  104749.5  38249.322811\nR            F             5.656804e+10  904.0  104899.5  38250.854626\n                                    Sum    Min       Max           Avg\nl_returnflag l_linestatus                                             \nA            F             5.658655e+10  904.0  104949.5  38273.129735\nN            F             1.487505e+09  920.0  104049.5  38284.467761\n             O             1.116318e+11  901.0  104749.5  38249.322811\nR            F             5.656804e+10  904.0  104899.5  38250.854626\nGrouped Aggregate + Filter\n                       name      time\n0               DuckDB (1T)  0.281653\n1               DuckDB (2T)  0.356302\n2                    Pandas  2.889015\n3  Pandas (manual pushdown)  1.625353\n\n\nGrouped Aggregate with Join and Filter\nIn this benchmark we expand on the previous benchmark by including a join and a filter on the joined-on table.\nNote that the naive version in Pandas is extremely slow, as it performs a full join of the entire table including all the columns that are not used and all the rows that will be filtered out. For that reason we have included a separate benchmark in which we have manually optimized the Pandas code by pushing down the projections and the filters.\nThese optimizations are performed automatically in DuckDB by the query optimizer.\n\n\n# projection & filter on lineitem table\nlineitem_projected = lineitem[\n  ['l_shipdate',\n   'l_orderkey',\n   'l_linestatus',\n   'l_returnflag',\n   'l_extendedprice']\n]\nlineitem_filtered = lineitem_projected[\n  lineitem_projected['l_shipdate'] < \"1998-09-02\"]\n# projection and filter on order table\norders_projected = orders[\n  ['o_orderkey',\n   'o_orderstatus']\n]\norders_filtered = orders_projected[\n  orders_projected['o_orderstatus'] == 'O']\n# perform the join\nmerged = lineitem_filtered.merge(\n  orders_filtered,\n  left_on='l_orderkey',\n  right_on='o_orderkey')\n# perform the aggregate\nresult = merged.groupby(\n  ['l_returnflag', 'l_linestatus']\n).agg(\n  Sum=('l_extendedprice', 'sum'),\n  Min=('l_extendedprice', 'min'),\n  Max=('l_extendedprice', 'max'),\n  Avg=('l_extendedprice', 'mean')\n)\n\n\nresult\n\n\n\n\n\n  \n    \n      \n      \n      Sum\n      Min\n      Max\n      Avg\n    \n    \n      l_returnflag\n      l_linestatus\n      \n      \n      \n      \n    \n  \n  \n    \n      N\n      O\n      1.080448e+11\n      901.0\n      104749.5\n      38250.662806\n    \n  \n\n\n\n\n\ndef duckdb_grouped_aggregate_filter_join(d_con):\n    print(d_con.query('''\nSELECT l_returnflag,\n       l_linestatus,\n       sum(l_extendedprice),\n       min(l_extendedprice),\n       max(l_extendedprice),\n       avg(l_extendedprice)\nFROM lineitem lineitem\nJOIN orders orders ON (l_orderkey=o_orderkey)\nWHERE l_shipdate <= DATE '1998-09-02'\n  AND o_orderstatus='O'\nGROUP BY l_returnflag,\n         l_linestatus\n''').to_df())\n\ndef duckdb_grouped_aggregate_filter_join_1t():\n    duckdb_grouped_aggregate_filter_join(duckdb)\n\ndef duckdb_grouped_aggregate_filter_join_2t():\n    duckdb_grouped_aggregate_filter_join(con)\n\ndef pandas_grouped_aggregate_filter_join():\n    merged = lineitem.merge(orders, left_on='l_orderkey', right_on='o_orderkey')\n    filtered_a = merged[merged['l_shipdate'] < \"1998-09-02\"]\n    filtered_b = filtered_a[filtered_a['o_orderstatus'] == 'O']\n    result = filtered_b.groupby(['l_returnflag', 'l_linestatus']).agg(Sum=('l_extendedprice', 'sum'), Min=('l_extendedprice', 'min'), Max=('l_extendedprice', 'max'), Avg=('l_extendedprice', 'mean'))\n    print(result)\n\ndef pandas_grouped_aggregate_filter_join_manual_pushdown():\n    lineitem_projected = lineitem[['l_shipdate', 'l_orderkey', 'l_linestatus', 'l_returnflag', 'l_extendedprice']]\n    lineitem_filtered = lineitem_projected[lineitem_projected['l_shipdate'] < \"1998-09-02\"]\n    orders_projected = orders[['o_orderkey', 'o_orderstatus']]\n    orders_filtered = orders_projected[orders_projected['o_orderstatus'] == 'O']\n    merged = lineitem_filtered.merge(orders_filtered, left_on='l_orderkey', right_on='o_orderkey')\n    result = merged.groupby(['l_returnflag', 'l_linestatus']).agg(Sum=('l_extendedprice', 'sum'), Min=('l_extendedprice', 'min'), Max=('l_extendedprice', 'max'), Avg=('l_extendedprice', 'mean'))\n    print(result)\n\nresults = []\nresults.append(timeit(duckdb_grouped_aggregate_filter_join_1t, 'DuckDB (1T)'))\nresults.append(timeit(duckdb_grouped_aggregate_filter_join_2t, 'DuckDB (2T)'))\nresults.append(timeit(pandas_grouped_aggregate_filter_join, 'Pandas'))\nresults.append(timeit(pandas_grouped_aggregate_filter_join_manual_pushdown, 'Pandas (manual pushdown)'))\nplot_results(results, 'Grouped Aggregate Join')\n\n\n  l_returnflag l_linestatus  sum(l_extendedprice)  min(l_extendedprice)  \\\n0            N            O          1.081147e+11                 901.0   \n\n   max(l_extendedprice)  avg(l_extendedprice)  \n0              104749.5          38250.450307  \n  l_returnflag l_linestatus  sum(l_extendedprice)  min(l_extendedprice)  \\\n0            N            O          1.081147e+11                 901.0   \n\n   max(l_extendedprice)  avg(l_extendedprice)  \n0              104749.5          38250.450307  \n                                    Sum    Min       Max           Avg\nl_returnflag l_linestatus                                             \nN            O             1.080448e+11  901.0  104749.5  38250.662806\n                                    Sum    Min       Max           Avg\nl_returnflag l_linestatus                                             \nN            O             1.080448e+11  901.0  104749.5  38250.662806\nGrouped Aggregate Join\n                       name       time\n0               DuckDB (1T)   0.218088\n1               DuckDB (2T)   0.376592\n2                    Pandas  11.403579\n3  Pandas (manual pushdown)   2.765103"
  },
  {
    "objectID": "posts/understand_namespace/2023-09-26-understand-kedro-namespace-pipeline.html",
    "href": "posts/understand_namespace/2023-09-26-understand-kedro-namespace-pipeline.html",
    "title": "Understanding the Kedro namespace with time series forecasting pipeline",
    "section": "",
    "text": "Kedro support Namespace Pipeline, which is a flexible way to re-use pipeline. If your pipeline has some recursive structure and parallel fork, namespace pipeline may be a good candidate to achieve this. For example, it is handy for time-series forecasting.\nTypically, time-series forecasts have a recursive structure, where the current prediction will become the input of next prediction. In this example, we will try to build up a recursive pipeline. First, let’s start with create a dummy node make_monthly_predictions and making sense of namespace.\n\n\n\nExample of multi-step forecast\n\n\n\ndef make_monthly_predictions(input_data):\n    output_data = \"dummy\"\n    return output_data\n\n\nfrom kedro.pipeline.modular_pipeline import pipeline\nfrom kedro.pipeline import node\n\n\nbase_pipeline = pipeline(\n    [\n        node(\n            func=make_monthly_predictions,\n            inputs=[\"input_data\"],\n            outputs=[\"output_data\"]\n        )\n    ]\n)\n\nThis is a 1-node pipeline, which takes 1 month of data and make 1 month of prediction. The next step is create many similar pipelines with different input data. i.e. current prediction need to become input for next node. Kedro provide a feature call namespace pipeline, which can be used to override datasets argument easily.\n\n\n\npipeline.png\n\n\n\nnamespace_pipeline = pipeline(\n    [\n        node(\n            func=make_monthly_predictions,\n            inputs=[\"input_data\"],\n            outputs=[\"output_data\"]\n        )\n    ],\n    namespace=\"namespace\"\n)\n\nWhen the namespace argument is specified, it will add a prefix to the inputs and outputs automatically.\n\nnamespace_pipeline\n\nPipeline([\nNode(make_monthly_predictions, ['namespace.input_data'], ['namespace.output_data'], None)\n])\n\n\n\nOptionally, you may also want to keep the dataset without namespace by specifying which dataset you want to keep.\n\nSyntaxError: invalid syntax (677617201.py, line 1)\n\n\n\nnamespace_pipeline = pipeline(\n    [\n        node(\n            func=make_monthly_predictions,\n            inputs=[\"input_data\"],\n            outputs=[\"output_data\"]\n        )\n    ],\n    inputs = [\"input_data\"],\n    namespace=\"namespace\"\n)\n\nnamespace_pipeline\n\nPipeline([\nNode(make_monthly_predictions, ['input_data'], ['namespace.output_data'], None)\n])\n\n\nNotice that now the node take input_data instead of namespace.input_data. For time series data, we want to connect the output to the next input, you can do this by providing a dict\n\nnamespace_pipeline = pipeline(\n    [\n        node(\n            func=make_monthly_predictions,\n            inputs=[\"input_data\"],\n            outputs=[\"output_data\"]\n        )\n    ],\n    inputs = {\"input_data\": \"my_fav_data\"},\n    namespace=\"namespace\"\n)\n\nnamespace_pipeline\n\nPipeline([\nNode(make_monthly_predictions, ['my_fav_data'], ['namespace.output_data'], None)\n])\n\n\n\nBuild the time-series pipeline\n\n\n\nforecast pipeline structure\n\n\nNow we understand better how to use namespace, what we need to do is roughly 1. Create a base_pipeline 2. Iteratively loop through these pipeline and apply namespace 3. Connect these pipeline by updating the inputs or outputs definition\n\nmonths = [\"jan\", \"feb\", \"mar\", \"apr\"]\n\n\nbase_pipeline\n\nPipeline([\nNode(make_monthly_predictions, ['input_data'], ['output_data'], None)\n])\n\n\n\ndef create_pipeline(months):\n    pipelines = []\n    for i in range(len(months)):\n        if i + 1 >= len(months): break\n        curr, next = months[i], months[i+1]\n        pipelines.append(pipeline(base_pipeline, namespace=curr))\n    return pipeline(pipelines) # Aggregate the pipelines\n\npipelines = create_pipeline(months)\npipelines\n\nPipeline([\nNode(make_monthly_predictions, ['feb.input_data'], ['feb.output_data'], None),\nNode(make_monthly_predictions, ['jan.input_data'], ['jan.output_data'], None),\nNode(make_monthly_predictions, ['mar.input_data'], ['mar.output_data'], None)\n])\n\n\n\n\n\ndisconnect pipeline\n\n\nIf we zoom in to focus on the pair of jan and feb, they are still not connected properly, so we need to update the pipeline to output as feb.input_data instead of jan.output_data.\nNode(make_monthly_predictions, ['feb.input_data'], ['feb.output_data'], None),\nNode(make_monthly_predictions, ['jan.input_data'], ['jan.output_data'], None),\n\ndef create_pipeline(months):\n    pipelines = []\n    for i in range(len(months)):\n        if i + 1 >= len(months): break\n        curr, next = months[i], months[i+1]\n        pipelines.append(pipeline(base_pipeline,\n                                  outputs={\"output_data\":f\"{next}.input_data\"}, # Override the input definition\n                                          namespace=curr))\n    return pipeline(pipelines) # Aggregate the pipelines\n\npipelines = create_pipeline(months)\npipelines\n\nPipeline([\nNode(make_monthly_predictions, ['jan.input_data'], ['feb.input_data'], None),\nNode(make_monthly_predictions, ['feb.input_data'], ['mar.input_data'], None),\nNode(make_monthly_predictions, ['mar.input_data'], ['apr.input_data'], None)\n])\n\n\nWe can verify the pipeline with kedro viz\n\n\n\nconnect pipeline"
  },
  {
    "objectID": "posts/2021-07-02-kedro-datacatalog.html",
    "href": "posts/2021-07-02-kedro-datacatalog.html",
    "title": "Advance Kedro Series - Digging into Dataset Memory Management and CacheDataSet",
    "section": "",
    "text": "Today I am gonna explain some kedro internals to understnad how kedor manage your dataset. If you always write imperative python code, you may find that writing nodes and pipeline is a little bti awkward. They may seems less intuitive, however, it also enable some interesting featrue.\nThis article assumes you have basic understanding of kedro, I will focus on CacheDataSet and the auto-release dataset feature of kedro pipeline. It is useful to reduce your memory footprint without encountering the infamous Out of Memory (OOM) issue.\nTo start with, we have the default iris dataset. Normally we would do it in a YAML file, but to make things easier in Notebook, I’ll keep everything compact in a notebook.\n\nimport kedro\nkedro.__version__\n\n'0.17.4'\n\n\n\nfrom kedro.io import DataCatalog, MemoryDataSet, CachedDataSet\nfrom kedro.extras.datasets.pandas import CSVDataSet\nfrom kedro.pipeline import node, Pipeline\nfrom kedro.runner import SequentialRunner\n\n# Prepare a data catalog\ndata_catalog = DataCatalog({\"iris\": CSVDataSet('data/01_raw/iris.csv')})\n\nNext, we have a pipeline follows this execution order: A -> B -> C\n\nfrom kedro.pipeline import Pipeline, node\nimport pandas as pd\n\n\ndef A(df):\n    print('Loading the Iris Dataset')\n    return 'Step1'\n\n\ndef B(dummy):\n    return 'Step2'\n\n\ndef C(dummy):\n    return 'Step3'\n\n\npipeline = Pipeline([node(A, \"iris\", \"A\"),\n                     node(B, \"A\", \"B\"),\n                     node(C, \"B\", \"C\"),\n                    ])\n\nc:\\programdata\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n\n\nTo zoom in to the pipeline, we can use Hook to print out the catalog after every node’s run.\n\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.framework.hooks import get_hook_manager\nfrom pprint import pprint\n\ndef apply_dict(d):\n    new_dict = {}\n    for k, v in d.items():\n        if isinstance(v, CachedDataSet):\n            if v._cache.exists():\n                print(v._cache._data)\n                new_dict[k] = 'In Memory'\n            else:\n                new_dict[k] ='Cache Deleted'\n        elif v.exists():\n            new_dict[k] = 'In Memory'\n    return new_dict\n\n\nclass DebugHook:\n    \"\"\"A hook class for creating a post mortem debugging with the PDB debugger\n    whenever an error is triggered within a pipeline. The local scope from when the\n    exception occured is available within this debugging session.\n    \"\"\"\n    @hook_impl\n    def after_node_run(self, node, catalog):\n        # adding extra behaviour to a single node\n        print(f\"Finish node {node.name}\")\n        pprint(f\"Print Catalog {apply_dict(catalog._data_sets)}\")\n#         pprint(f\"Print Catalog {apply_dict2(lambda x:x.exists(), catalog._data_sets)}\")\n        print(\"*****************************\")\n        \nhook_manager = get_hook_manager()\ndebug_hook = hook_manager.register(DebugHook());\n\nThis hook will print out dataset that exist in data catalog. It is a bit tricky because kedro did not delete the dataset, it marked the underlying data as _EMPTY object instead.\n\n# Create a runner to run the pipeline\nrunner = SequentialRunner()\n\n# Run the pipeline\nrunner.run(pipeline, data_catalog);\n\nLoading the Iris Dataset\nFinish node A([iris]) -> [A]\n\"Print Catalog {'iris': 'In Memory'}\"\n*****************************\nFinish node B([A]) -> [B]\n\"Print Catalog {'iris': 'In Memory', 'A': 'In Memory'}\"\n*****************************\nFinish node C([B]) -> [C]\n\"Print Catalog {'iris': 'In Memory', 'B': 'In Memory'}\"\n*****************************\n\n\nLet’s have a look at what happened when a SequentialRunner runs a pipeline.\nIt is interesting to note that kedro takes a similar approach to Python, it uses reference counting to control the dataset life cycle. If you are interested, I have another post to dive into Python Memory Management.\n            # decrement load counts and release any data sets we've finished with\n            for data_set in node.inputs:\n                load_counts[data_set] -= 1\n                if load_counts[data_set] < 1 and data_set not in pipeline.inputs():\n                    catalog.release(data_set)\n            for data_set in node.outputs:\n                if load_counts[data_set] < 1 and data_set not in pipeline.outputs():\n                    catalog.release(data_set)\n\nCacheDataSet\nWhat does release do? It will remove the underlying data if this data is stored in memory.\n# In CSVDataSet\nhttps://github.com/quantumblacklabs/kedro/blob/master/kedro/extras/datasets/pandas/csv_dataset.py#L176-L178\n```python\ndef _release(self) -> None:\n    super()._release()\n    self._invalidate_cache()\n# In CacheDataSet\ndef _release(self) -> None:\n    self._cache.release()\n    self._dataset.release()\n# In MemoryDataSet\ndef _release(self) -> None:\n    self._data = _EMPTY\nFirst, we can test if it works as expected.\n\nd = CachedDataSet(CSVDataSet('data/01_raw/iris.csv'))\nd.load()\nd._cache._data.head()\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n      species\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      setosa\n    \n  \n\n\n\n\n\nd.exists()\n\nc:\\programdata\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n\n\nTrue\n\n\n\nd.release()\n\n\nd._cache.exists()\n\nFalse\n\n\nThis is the expected behavior, where the cache should be released. However, it seems not to be the case when I run the pipeline.\n\ndata_catalog = DataCatalog({\"iris\": CachedDataSet(CSVDataSet('data/01_raw/iris.csv'))})\nrunner.run(pipeline, data_catalog)\n\nLoading the Iris Dataset\nFinish node A([iris]) -> [A]\n     sepal_length  sepal_width  petal_length  petal_width    species\n0             5.1          3.5           1.4          0.2     setosa\n1             4.9          3.0           1.4          0.2     setosa\n2             4.7          3.2           1.3          0.2     setosa\n3             4.6          3.1           1.5          0.2     setosa\n4             5.0          3.6           1.4          0.2     setosa\n..            ...          ...           ...          ...        ...\n145           6.7          3.0           5.2          2.3  virginica\n146           6.3          2.5           5.0          1.9  virginica\n147           6.5          3.0           5.2          2.0  virginica\n148           6.2          3.4           5.4          2.3  virginica\n149           5.9          3.0           5.1          1.8  virginica\n\n[150 rows x 5 columns]\n\"Print Catalog {'iris': 'In Memory'}\"\n*****************************\nFinish node B([A]) -> [B]\n     sepal_length  sepal_width  petal_length  petal_width    species\n0             5.1          3.5           1.4          0.2     setosa\n1             4.9          3.0           1.4          0.2     setosa\n2             4.7          3.2           1.3          0.2     setosa\n3             4.6          3.1           1.5          0.2     setosa\n4             5.0          3.6           1.4          0.2     setosa\n..            ...          ...           ...          ...        ...\n145           6.7          3.0           5.2          2.3  virginica\n146           6.3          2.5           5.0          1.9  virginica\n147           6.5          3.0           5.2          2.0  virginica\n148           6.2          3.4           5.4          2.3  virginica\n149           5.9          3.0           5.1          1.8  virginica\n\n[150 rows x 5 columns]\n\"Print Catalog {'iris': 'In Memory', 'A': 'In Memory'}\"\n*****************************\nFinish node C([B]) -> [C]\n     sepal_length  sepal_width  petal_length  petal_width    species\n0             5.1          3.5           1.4          0.2     setosa\n1             4.9          3.0           1.4          0.2     setosa\n2             4.7          3.2           1.3          0.2     setosa\n3             4.6          3.1           1.5          0.2     setosa\n4             5.0          3.6           1.4          0.2     setosa\n..            ...          ...           ...          ...        ...\n145           6.7          3.0           5.2          2.3  virginica\n146           6.3          2.5           5.0          1.9  virginica\n147           6.5          3.0           5.2          2.0  virginica\n148           6.2          3.4           5.4          2.3  virginica\n149           5.9          3.0           5.1          1.8  virginica\n\n[150 rows x 5 columns]\n\"Print Catalog {'iris': 'In Memory', 'B': 'In Memory'}\"\n*****************************\n\n\n{'C': 'Step3'}\n\n\nThe dataset is persisted throughout the entire pipeline, why? We can monkey patch the SequentialRunner to check why is this happening.\n\n\nA potential bug or undesired beahvior?\n\nfrom collections import Counter\nfrom itertools import chain\nfrom kedro.runner.runner import AbstractRunner, run_node\n\ndef _run(\n    self, pipeline, catalog, run_id = None\n) -> None:\n    \"\"\"The method implementing sequential pipeline running.\n\n    Args:\n        pipeline: The ``Pipeline`` to run.\n        catalog: The ``DataCatalog`` from which to fetch data.\n        run_id: The id of the run.\n\n    Raises:\n        Exception: in case of any downstream node failure.\n    \"\"\"\n    nodes = pipeline.nodes\n    done_nodes = set()\n\n    load_counts = Counter(chain.from_iterable(n.inputs for n in nodes))\n\n    for exec_index, node in enumerate(nodes):\n        try:\n            run_node(node, catalog, self._is_async, run_id)\n            done_nodes.add(node)\n        except Exception:\n            self._suggest_resume_scenario(pipeline, done_nodes)\n            raise\n            \n        # print load counts for every node run\n        pprint(f\"{load_counts}\")\n        print(\"pipeline input: \", pipeline.inputs())\n        print(\"pipeline output: \", pipeline.outputs())\n\n        # decrement load counts and release any data sets we've finished with\n        for data_set in node.inputs:\n            load_counts[data_set] -= 1\n            if load_counts[data_set] < 1 and data_set not in pipeline.inputs():\n                catalog.release(data_set)\n        for data_set in node.outputs:\n            if load_counts[data_set] < 1 and data_set not in pipeline.outputs():\n                catalog.release(data_set)\n\n        self._logger.info(\n            \"Completed %d out of %d tasks\", exec_index + 1, len(nodes)\n        )\n        \nSequentialRunner._run = _run\n\nc:\\programdata\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n\n\nNow we re-run the pipeline. Let’s reset the hook to only print related information.\n\nclass PrintHook:\n    @hook_impl\n    def after_node_run(self, node, catalog):\n        # adding extra behaviour to a single node\n        print(f\"Finish node {node.name}\")\n        print(\"*****************************\")\n        \n\nhook_manager.set_blocked(debug_hook); # I tried hook_manger.unregister(), but it is not working.\nprint_hook = hook_manager.register(PrintHook())\n\n\n# Create a runner to run the pipeline\nrunner = SequentialRunner()\n\n# Run the pipeline\nrunner.run(pipeline, data_catalog);\n\nLoading the Iris Dataset\nFinish node A([iris]) -> [A]\n*****************************\n\"Counter({'iris': 1, 'A': 1, 'B': 1})\"\npipeline input:  {'iris'}\npipeline output:  {'C'}\nFinish node B([A]) -> [B]\n*****************************\n\"Counter({'A': 1, 'B': 1, 'iris': 0})\"\npipeline input:  {'iris'}\npipeline output:  {'C'}\nFinish node C([B]) -> [C]\n*****************************\n\"Counter({'B': 1, 'iris': 0, 'A': 0})\"\npipeline input:  {'iris'}\npipeline output:  {'C'}\n\n\n\n\nConclusion\nSo the reason why the iris data is kept becasue it is always in pipeline.inputs(), which I think is not what we wanted."
  },
  {
    "objectID": "posts/python_expert/python-expert-decorator-generator-contextmanager.html",
    "href": "posts/python_expert/python-expert-decorator-generator-contextmanager.html",
    "title": "Being Python Expert",
    "section": "",
    "text": "This presentation introduce three concepts, decorator, generator and contextmanager. The presenter explains that Python is a Protocol oriented langauge brilliantly and put these three concepts together to illustrate a story. Ultimately, being a Python expert doesn’t mean that you write advance syntax, but using these pattern wisely with simple code. These three concepts both serves its own puprose and are orthogonal to each other, yet working nicely when they are composed together."
  },
  {
    "objectID": "posts/python_expert/python-expert-decorator-generator-contextmanager.html#meet-the-python-data-model",
    "href": "posts/python_expert/python-expert-decorator-generator-contextmanager.html#meet-the-python-data-model",
    "title": "Being Python Expert",
    "section": "Meet the Python Data Model",
    "text": "Meet the Python Data Model\nWith python, there are almost always a lower level __ method corresponds to a higher-level function/syntax. It’s useful to change these behavior and understand how the Python Data Model work. You can find all the dunder __ method here. https://docs.python.org/3/reference/datamodel.html\n x + y   --> __add__\n repr(s) --> __repr__\n x()     --> __call__\n\n## Metaclasses\n`metaclass` exists to allow library code works nicely with user code. For example, how can a library author ensure that its user will follow its protocol and not using it wrongly? Again you can dive into `__new__` for ensuring that. In practice, the code is already written for `abc` and people use `@abstractmethod` `abc.ABCMeta`\n\n\n# Meet the `__builtins__` library\nimport builtins\n__build_class__\n\nclass Nok:\n    pass\n\nNok() # This is possible because class is a Python keyword\n\n# You can actually construct a class with a function.\nbuiltins.__build_class__(\"Nok\")\n\nTypeError: __build_class__: not enough arguments"
  },
  {
    "objectID": "posts/python_expert/python-expert-decorator-generator-contextmanager.html#generator",
    "href": "posts/python_expert/python-expert-decorator-generator-contextmanager.html#generator",
    "title": "Being Python Expert",
    "section": "Generator",
    "text": "Generator\n\nEager vs Lazy\nProcess when data comes - memory efficient and no wait.\nyield control - interleaving. Idea of executing some code, then passing the output back to user, do something and continue.\n\n\ndef temptable(cur):\n    print(\"Create Table\")\n    yield\n    print(\"Drop Table\")\n\n\nclass T:\n    def __enter__(self):\n        self.gen = temptable(\"123\")\n        next(self.gen)\n        return self\n\n    def __exit__(self, *args):\n        return next(self.gen, None)\n\nwith T():\n    print(\"Finish\")\n\n\nCreate Table\nFinish\nDrop Table"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Understanding the Kedro namespace with time series forecasting pipeline\n\n\n\n\n\n\n\nkedro\n\n\n\n\nUnderstand how namespace works in Kedro with a time series forcasting example\n\n\n\n\n\n\nSep 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction overloading - singledispatch in Python with type hint\n\n\n\n\n\n\n\npython\n\n\n\n\nUsing singledispatch with type hint\n\n\n\n\n\n\nNov 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMocking Python version for testing, use of Python Warning to raise error for specific Python Version\n\n\n\n\n\n\n\npython\n\n\n\n\nMocking Python version for testing, use of Python Warning to raise error for specific Python Version\n\n\n\n\n\n\nNov 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKedro DuckDB\n\n\n\n\n\n\n\npython\n\n\n\n\nKedro Meet the Duck\n\n\n\n\n\n\nNov 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the Kedro codebase - A quick dirty meta-analysis - (Part I)\n\n\n\n\n\n\n\nkedro\n\n\n\n\nMeta-analysis of the kedro codebase\n\n\n\n\n\n\nNov 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeing Python Expert\n\n\n\n\n\n\n\npython\n\n\n\n\nNotes for the talk - James Powell: So you want to be a Python expert? | PyData Seattle 2017\n\n\n\n\n\n\nNov 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJourney of understanding Python and programming language\n\n\n\n\n\n\n\npython\n\n\n\n\nSome unfinished notes about Python lower level details.\n\n\n\n\n\n\nFeb 10, 2022\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nWhat can we learn from Shipping Crisis as a Data Scientist?\n\n\n\n\n\n\n\nproduct\n\n\n\n\nThe Long Beach port congestion could teach us a lot about data science in real world.\n\n\n\n\n\n\nNov 18, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nPython FileNotFoundError or You have a really long file path?\n\n\n\n\n\n\n\npython\n\n\n\n\nToday I encountered an interesting bug that I think it is worth to write it down for my future self.\n\n\n\n\n\n\nAug 18, 2021\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5 Minutes Data Science Design Patterns I - Callback\n\n\n\n\n\n\n\npython\n\n\ndesign pattern\n\n\nsoftware\n\n\n\n\nA mini collections of design pattern for Data Science - Starting with callbacks.\n\n\n\n\n\n\nJul 10, 2021\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvance Kedro Series - Digging into Dataset Memory Management and CacheDataSet\n\n\n\n\n\n\n\npython\n\n\nkedro\n\n\n\n\nKedro pipeline offers some nice feature like automatically release data in memory that is no longer need. How is this possible? Let’s dive deep into the code.\n\n\n\n\n\n\nJul 2, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nJupyter Superpower - Extend SQL analysis with Python\n\n\n\n\n\n\n\npython\n\n\nreviewnb\n\n\nsql\n\n\n\n\nMaking collboration with Notebook possible and share perfect SQL analysis with Notebook.\n\n\n\n\n\n\nJun 26, 2021\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA logging.config.dictConfig() issue in python\n\n\n\n\n\n\n\npython\n\n\n\n\nlogging.config.dictConfig()\n\n\n\n\n\n\nJun 20, 2021\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython Internal Series - Global Interpreter Lock (GIL) and Memory Management\n\n\n\n\n\n\n\npython-internal\n\n\n\n\nDespite the bad reputation of GIL, it was arguably designed on purpose. The GIL actually comes with a lot of benefit.\n\n\n\n\n\n\nMay 29, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nFull Stack Deep Learning Notes - Lecture 03 - Recurrent Neural Network\n\n\n\n\n\n\n\nfsdl\n\n\n\n\nLecture & Lab notes - This lecture is about Recurrent Neural Network. Key concetps included input gate, forget gate, cell state, and output gate. It also explains how attention mechanism works for a encoder-decoder based architecture.\n\n\n\n\n\n\nApr 16, 2021\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMicrosoft Azure - DP100\n\n\n\n\n\n\n\nazure\n\n\n\n\nThis note helps you to prepare the Azure Assoicate Data Scientist DP-100 exam. I took DP100 in Mar 2021 and includes some important notes for study. Particularly, syntax types questions are very common. You need to study the lab and make sure you understand and remember some syntax to pass this exam.\n\n\n\n\n\n\nMar 27, 2021\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFull Stack Deep Learning Notes - Lecture 01\n\n\n\n\n\n\n\nfsdl\n\n\n\n\nLecture & Lab notes, explain DataModules, Trainer LightningModule.\n\n\n\n\n\n\nMar 21, 2021\n\n\n\n\n\n\n  \n\n\n\n\ndeepcopy, LGBM and pickle\n\n\n\n\n\n\n\npython\n\n\npickle\n\n\ndeepcopy\n\n\n\n\nAt first sight, these 3 things may not sounds related at all. I am writing this article to share a bug with lightgbm that I encountered and it eventually leads to deeper understanding of what pickle really are.\n\n\n\n\n\n\nMar 19, 2021\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Test as CI\n\n\n\n\n\n\n\npython\n\n\n\n\nUnit Test with Pytest. I want to display a full information rich string into my CI Log, but it is trimmed by Python. Here is a simple fix to make it display full string in the log file.\n\n\n\n\n\n\nMar 17, 2021\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up pyodbc for Impala connection, works on both Linux and Window\n\n\n\n\n\n\n\npyodbc\n\n\nimpala\n\n\n\n\nEasiest way to connect with Impala in Windows\n\n\n\n\n\n\nMar 5, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\ndata augmentation - Understand MixUp and Beta Distribution\n\n\n\n\n\n\n\nML\n\n\n\n\nMixup and Beta Distribution\n\n\n\n\n\n\nFeb 9, 2020\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHydra - Config Composition for Machine Learning Project\n\n\n\n\n\n\n\npython\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2020\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplyer - Desktop Notification with Python\n\n\n\n\n\n\n\npython\n\n\n\n\nDesktop notifiaction with Python\n\n\n\n\n\n\nOct 19, 2019\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\nnbdev + GitHub Codespaces: A New Literate Programming Environment\n\n\n\n\n\n\n\ncodespaces\n\n\nnbdev\n\n\n\n\nHow a new GitHub feature makes literate programming easier than ever before.\n\n\n\n\n\n\nJan 1, 2019\n\n\nHamel Husain & Jeremy Howard\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]