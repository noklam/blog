{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "toc: true\n",
    "badges: true\n",
    "categories:\n",
    "- python\n",
    "- kedro\n",
    "date: '2024-07-08'\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Achieve (Almost) dependencies-free Kedro Viz Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List\n",
    "\n",
    "from kedro.pipeline.modular_pipeline import pipeline as ModularPipeline\n",
    "from kedro.pipeline.pipeline import Node, Pipeline\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kedro viz is a visualisation tools for Kedro. It creates an interactive flow chart that visualize Kedro's pipeline in a web app. One of the issue is that Kedro-Viz requires loading a Kedro project, this creates frictions as Kedro-Viz is often used for onboarding and installing all dependencies correctly could be a big challenge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution\n",
    "If we focus only on the interactive flowchart of Kedro-Viz, it's possible to get rid of the dependencies, and the key is to use Abstract Syntax Tree (AST) to parse Kedro pipeline instead actually loading the module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How this work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python as an Interpreted Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is often considered as \"interpreted\" rather than \"compiled\". In fact, compilation still happens in Python but it's a lot simpler compare to other language like C++.\n",
    "\n",
    "The things that happens is usually\n",
    "\n",
    "Parsing a text file -> AST -> Bytecode (i.e. the .pyc file) -> Machine code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before this, we need to understand what `AST` is and how can we leverage the `ast` Python library. AST is a data structure that represent your code in a tree-like structure. For example, consider the snippet below:\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "# A simple time program\n",
    "start_time = time.time()\n",
    "time.sleep(1)\n",
    "now = time.time()\n",
    "print(\"Time spent:\", now - start_time)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet = \"\"\"import time\n",
    "\n",
    "# A simple time program\n",
    "start_time = time.time()\n",
    "time.sleep(1)\n",
    "now = time.time()\n",
    "print(\"Time spent:\", now - start_time)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "parsed = ast.parse(snippet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<ast.Import object at 0x127acdea0>, <ast.Assign object at 0x127acd960>, <ast.Expr object at 0x127c2ee00>, <ast.Assign object at 0x127c2ed40>, <ast.Expr object at 0x1270ba020>]\n"
     ]
    }
   ],
   "source": [
    "print(parsed.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a library called `ast.dump` to visualise the tree better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module(\n",
      "    body=[\n",
      "        Import(\n",
      "            names=[\n",
      "                alias(name='time')]),\n",
      "        Assign(\n",
      "            targets=[\n",
      "                Name(id='start_time', ctx=Store())],\n",
      "            value=Call(\n",
      "                func=Attribute(\n",
      "                    value=Name(id='time', ctx=Load()),\n",
      "                    attr='time',\n",
      "                    ctx=Load()),\n",
      "                args=[],\n",
      "                keywords=[])),\n",
      "        Expr(\n",
      "            value=Call(\n",
      "                func=Attribute(\n",
      "                    value=Name(id='time', ctx=Load()),\n",
      "                    attr='sleep',\n",
      "                    ctx=Load()),\n",
      "                args=[\n",
      "                    Constant(value=1)],\n",
      "                keywords=[])),\n",
      "        Assign(\n",
      "            targets=[\n",
      "                Name(id='now', ctx=Store())],\n",
      "            value=Call(\n",
      "                func=Attribute(\n",
      "                    value=Name(id='time', ctx=Load()),\n",
      "                    attr='time',\n",
      "                    ctx=Load()),\n",
      "                args=[],\n",
      "                keywords=[])),\n",
      "        Expr(\n",
      "            value=Call(\n",
      "                func=Name(id='print', ctx=Load()),\n",
      "                args=[\n",
      "                    Constant(value='Time spent:'),\n",
      "                    BinOp(\n",
      "                        left=Name(id='now', ctx=Load()),\n",
      "                        op=Sub(),\n",
      "                        right=Name(id='start_time', ctx=Load()))],\n",
      "                keywords=[]))],\n",
      "    type_ignores=[])\n"
     ]
    }
   ],
   "source": [
    "print(ast.dump(parsed, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, this is corresponding to `start_time = time.time()`\n",
    "\n",
    "```python\n",
    "        Assign(\n",
    "            targets=[\n",
    "                Name(id='start_time', ctx=Store())],\n",
    "            value=Call(\n",
    "                func=Attribute(\n",
    "                    value=Name(id='time', ctx=Load()),\n",
    "                    attr='time',\n",
    "                    ctx=Load()),\n",
    "                args=[],\n",
    "                keywords=[]))\n",
    "    \n",
    "```\n",
    "There is one thing that is missing from the snippet, which is the comment. As the interpreter does not care about this information, so it is thrown away during the process. If you care about preserving comments, you may consider `CST` or other parser which keep the information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem - Create flowchart with missing dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this pipeline which requires `spark` as a dependency.\n",
    "\n",
    "```python\n",
    "# from nodes.py\n",
    "import spark\n",
    "\n",
    "def my_spark_etl_func():\n",
    "    spark...\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "# from pipeline.py\n",
    "from kedro.pipeline import pipeline, node\n",
    "from .nodes import my_spark_etl_func\n",
    "\n",
    "def create_pipeline():\n",
    "    return pipeline(node(my_spark_etl_func,\n",
    "                         inputs=[\"dataset_1\",\"dataset_2\"],\n",
    "                         outputs=[\"output_dataset_1\"]\n",
    "                        )\n",
    "                   )\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing with AST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From kedro viz perspective, this is the problematic part because this will cause a `ImportError`:\n",
    "\n",
    "```python\n",
    "from .nodes import my_spark_etl_func\n",
    "```\n",
    "\n",
    "\n",
    "As Kedro-viz does not execute these function, it would be nice if we can parse the second part out and ignore the rest of the file. This is where `ast` will be useful.\n",
    "\n",
    "```python\n",
    "def create_pipeline():\n",
    "    return pipeline(node(my_spark_etl_func,\n",
    "                         inputs=[\"dataset_1\",\"dataset_2\"],\n",
    "                         outputs=[\"output_dataset_1\"]\n",
    "                        )\n",
    "                   )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a KedroPipelineFinder to find the pipeline defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "kedro_pipeline_text = \"\"\"from kedro.pipeline import pipeline, node\n",
    "from .nodes import my_spark_etl_func\n",
    "\n",
    "def create_pipeline():\n",
    "    return pipeline(node(my_spark_etl_func,\n",
    "                         inputs=[\"dataset_1\",\"dataset_2\"],\n",
    "                         outputs=[\"output_dataset_1\"]\n",
    "                        )\n",
    "                   )\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ast` library provides an useful class `ast.NodeVisitor`, instead of implementing the entire AST, you only need to implement the relevant part that you care. It implement a method called `ast.visit`, and you only need to implement the relevant part in your class, i.e. `visit_<class_name>`. You can find the full list of `<class_name>` in the [AST Grammar](https://docs.python.org/3/library/ast.html#abstract-grammar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunctionDefPrinter(ast.NodeVisitor):\n",
    "#     def generic_visit(self, node):\n",
    "#         print(type(node).__name__)\n",
    "#         super().generic_visit(node)\n",
    "\n",
    "    def visit_Import(self, node):\n",
    "        print(node.names)\n",
    "        print(\"print everytime something is imported\")\n",
    "\n",
    "#         print(dir(node))\n",
    "v = FunctionDefPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<ast.alias object at 0x132fec1c0>]\n",
      "print everytime something is imported\n"
     ]
    }
   ],
   "source": [
    "parsed = ast.parse(snippet)\n",
    "v.visit(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 -  Parsing function that has a name `create_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KedroPipelineFinder(ast.NodeVisitor):\n",
    "    def __init__(self):\n",
    "        self.pipeline_def = []\n",
    "\n",
    "    def generic_visit(self, node):\n",
    "        if ast.NodeVisitor.generic_visit(self,node):\n",
    "            print(\"Got something!\")\n",
    "\n",
    "    def visit_FunctionDef(self, node):\n",
    "        if node.name == \"create_pipeline\":\n",
    "            print(\"found a create_pipeline()\")\n",
    "            self.pipeline_def.append(node)\n",
    "#             return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found a create_pipeline()\n"
     ]
    }
   ],
   "source": [
    "kpf = KedroPipelineFinder()\n",
    "parsed = ast.parse(kedro_pipeline_text)\n",
    "kpf.visit(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pipeline_def = parsed.body[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FunctionDef(\n",
      "    name='create_pipeline',\n",
      "    args=arguments(\n",
      "        posonlyargs=[],\n",
      "        args=[],\n",
      "        kwonlyargs=[],\n",
      "        kw_defaults=[],\n",
      "        defaults=[]),\n",
      "    body=[\n",
      "        Return(\n",
      "            value=Call(\n",
      "                func=Name(id='pipeline', ctx=Load()),\n",
      "                args=[\n",
      "                    Call(\n",
      "                        func=Name(id='node', ctx=Load()),\n",
      "                        args=[\n",
      "                            Name(id='my_spark_etl_func', ctx=Load())],\n",
      "                        keywords=[\n",
      "                            keyword(\n",
      "                                arg='inputs',\n",
      "                                value=List(\n",
      "                                    elts=[\n",
      "                                        Constant(value='dataset_1'),\n",
      "                                        Constant(value='dataset_2')],\n",
      "                                    ctx=Load())),\n",
      "                            keyword(\n",
      "                                arg='outputs',\n",
      "                                value=List(\n",
      "                                    elts=[\n",
      "                                        Constant(value='output_dataset_1')],\n",
      "                                    ctx=Load()))])],\n",
      "                keywords=[]))],\n",
      "    decorator_list=[])\n"
     ]
    }
   ],
   "source": [
    "print(ast.dump(create_pipeline_def, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Build Kedro Pipeline object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KedroPipelineBuilder(ast.NodeVisitor):\n",
    "    def __init__(self, pipeline_def: list):\n",
    "        self.pipeline_def = pipeline_def\n",
    "\n",
    "    def build(self, node):\n",
    "        self.generic_visit(node)\n",
    "        return ...\n",
    "\n",
    "    def visit_Call(self, node):\n",
    "        \"\"\"Assume it is return from a create_pipeline\n",
    "        def create_pipeline():\n",
    "           return pipeline(node(...), node(...), node(...)) or\n",
    "\n",
    "         pipeline object that is imported from other module won't be captured.\n",
    "       \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ast.FunctionDef at 0x132fb3b20>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_pipeline_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ast.Call at 0x132fb3bb0>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_pipeline_def.body[0].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "call = create_pipeline_def.body[0].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<ast.Call object at 0x132fb24a0>]\n"
     ]
    }
   ],
   "source": [
    "print(call.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_args = call.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_arg = call_args[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_name = call_arg.args[0].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ast.Name at 0x132fb2a70>"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_arg.args[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def create_pipeline():\\n    return pipeline(node(my_spark_etl_func, inputs=['dataset_1', 'dataset_2'], outputs=['output_dataset_1']))\""
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.unparse(create_pipeline_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call(\n",
      "   func=Name(id='node', ctx=Load()),\n",
      "   args=[\n",
      "      Name(id='my_spark_etl_func', ctx=Load())],\n",
      "   keywords=[\n",
      "      keyword(\n",
      "         arg='inputs',\n",
      "         value=List(\n",
      "            elts=[\n",
      "               Constant(value='dataset_1'),\n",
      "               Constant(value='dataset_2')],\n",
      "            ctx=Load())),\n",
      "      keyword(\n",
      "         arg='outputs',\n",
      "         value=List(\n",
      "            elts=[\n",
      "               Constant(value='output_dataset_1')],\n",
      "            ctx=Load()))])\n"
     ]
    }
   ],
   "source": [
    "print(ast.dump(call_arg, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = call_arg.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FunctionDef(\n",
      "  name='create_pipeline',\n",
      "  args=arguments(\n",
      "    posonlyargs=[],\n",
      "    args=[],\n",
      "    kwonlyargs=[],\n",
      "    kw_defaults=[],\n",
      "    defaults=[]),\n",
      "  body=[\n",
      "    Return(\n",
      "      value=Call(\n",
      "        func=Name(id='pipeline', ctx=Load()),\n",
      "        args=[\n",
      "          Call(\n",
      "            func=Name(id='node', ctx=Load()),\n",
      "            args=[\n",
      "              Name(id='my_spark_etl_func', ctx=Load())],\n",
      "            keywords=[\n",
      "              keyword(\n",
      "                arg='inputs',\n",
      "                value=List(\n",
      "                  elts=[\n",
      "                    Constant(value='dataset_1'),\n",
      "                    Constant(value='dataset_2')],\n",
      "                  ctx=Load())),\n",
      "              keyword(\n",
      "                arg='outputs',\n",
      "                value=List(\n",
      "                  elts=[\n",
      "                    Constant(value='output_dataset_1')],\n",
      "                  ctx=Load()))])],\n",
      "        keywords=[]))],\n",
      "  decorator_list=[])\n"
     ]
    }
   ],
   "source": [
    "print(ast.dump(create_pipeline_def, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
